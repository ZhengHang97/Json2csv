comments,DECISION
"This paper improves significantly upon the original NPI work, showing that the model generalizes far better when trained on traces in recursive form. The authors show better sample complexity and generalization results for addition and bubblesort programs, and add two new and more interesting tasks - topological sort and quicksort (added based on reviewer discussion). Furthermore, they actually *prove* that the algorithms learned by the model generalize perfectly, which to my knowledge is the first time this has been done in neural program induction.",1
"This paper improves significantly upon the original NPI work, showing that the model generalizes far better when trained on traces in recursive form. The authors show better sample complexity and generalization results for addition and bubblesort programs, and add two new and more interesting tasks - topological sort and quicksort (added based on reviewer discussion). Furthermore, they actually *prove* that the algorithms learned by the model generalize perfectly, which to my knowledge is the first time this has been done in neural program induction.",1
"This nicely written paper presents an end-to-end learning method for image compression. By optimizing for rate-distortion performance and a clever relaxation the method is able to learn an efficient image compression method by optimizing over a database of natural images.

As the method is interesting, results are interesting and analysis is quite thorough it's easy for me to recommend acceptance.",1
"This nicely written paper presents an end-to-end learning method for image compression. By optimizing for rate-distortion performance and a clever relaxation the method is able to learn an efficient image compression method by optimizing over a database of natural images.

As the method is interesting, results are interesting and analysis is quite thorough it's easy for me to recommend acceptance.",1
"In light of the authors' responsiveness and the updates to the manuscript -- in particular to clarify the meta-learning task -- I am updating my score to an 8.

-----

This manuscript proposes to tackle few-shot learning with neural networks by leveraging meta-learning, a classic idea that has seen a renaissance in the last 12 months. The authors formulate few-shot learning as a sequential meta-learning problem: each ""example"" includes a sequence of batches of ""training"" pairs, followed by a final ""test"" batch. The inputs at each ""step"" include the outputs of a ""base learner"" (e.g., training loss and gradients), as well as the base learner's current state (parameters). The paper applies an LSTM to this meta-learning problem, using the inner memory cells in the *second* layer to directly model the updated parameters of the base learner. In doing this, they note similarities between the respective update rules of LSTM memory cells and gradient descent. Updates to the LSTM meta-learner are computed based on the base learner's prediction loss for the final ""test"" batch. The authors make several simplifying assumptions, such as sharing weights across all second layer cells (analogous to using the same learning rate for all parameters). The paper recreates the Mini-ImageNet data set proposed in Vinyals et al 2016, and shows that the meta-learner LSTM is competitive with the current state-of-the-art (Matchin Networks, Vinyals 2016) on 1- and 5-shot learning.

Strengths:
- It is intriguing -- and in hindsight, natural -- to cast the few-shot learning problem as a sequential (meta-)learning problem. While the authors did not originate the general idea of persisting learning across a series of learning problems, I think it is fair to say that they have advanced the state of the art, though I cannot confidently assert its novelty as I am not deeply familiar with recent work on meta-learning.
- The proposed approach is competitive with and outperforms Vinyals 2016 in 1-shot and 5-shot Mini-ImageNet experiments.
- The base learner in this setting (simple ConvNet classifier) is quite different from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals 2016. It is always exciting when state-of-the-art results can be reported using very different approaches, rather than incremental follow-up work.
- As far as I know, the insight about the relationship between the memory cell and gradient descent updates is novel here. It is interesting regardless.
- The paper offers several practical insights about how to design and train an LSTM meta-learner, which should make it easier for others to replicate this work and apply these ideas to new problems. These include proper initialization, weight sharing across coordinates, and the importance of normalizing/rescaling the loss, gradient, and parameter inputs. Some of the insights have been previously described (the importance of simulating test conditions during meta-training; assuming independence between meta-learner and base learner parameters when taking gradients with respect to the meta-learner parameters), but the discussion here is useful nonetheless.

Weaknesses:
- The writing is at times quite opaque. While it describes very interesting work, I would not call the paper an enjoyable read. It took me multiple passes (as well as consulting related work) to understand the general learning problem. The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area. The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks. This would definitely make it accessible to a wider audience.
- Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me. Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed through meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)? Many other details like these are unclear (see question).
- The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them.

This is an interesting paper with convincing results. It seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved. I will definitely raise my score if the writing is improved.",1
"In light of the authors' responsiveness and the updates to the manuscript -- in particular to clarify the meta-learning task -- I am updating my score to an 8.

-----

This manuscript proposes to tackle few-shot learning with neural networks by leveraging meta-learning, a classic idea that has seen a renaissance in the last 12 months. The authors formulate few-shot learning as a sequential meta-learning problem: each ""example"" includes a sequence of batches of ""training"" pairs, followed by a final ""test"" batch. The inputs at each ""step"" include the outputs of a ""base learner"" (e.g., training loss and gradients), as well as the base learner's current state (parameters). The paper applies an LSTM to this meta-learning problem, using the inner memory cells in the *second* layer to directly model the updated parameters of the base learner. In doing this, they note similarities between the respective update rules of LSTM memory cells and gradient descent. Updates to the LSTM meta-learner are computed based on the base learner's prediction loss for the final ""test"" batch. The authors make several simplifying assumptions, such as sharing weights across all second layer cells (analogous to using the same learning rate for all parameters). The paper recreates the Mini-ImageNet data set proposed in Vinyals et al 2016, and shows that the meta-learner LSTM is competitive with the current state-of-the-art (Matchin Networks, Vinyals 2016) on 1- and 5-shot learning.

Strengths:
- It is intriguing -- and in hindsight, natural -- to cast the few-shot learning problem as a sequential (meta-)learning problem. While the authors did not originate the general idea of persisting learning across a series of learning problems, I think it is fair to say that they have advanced the state of the art, though I cannot confidently assert its novelty as I am not deeply familiar with recent work on meta-learning.
- The proposed approach is competitive with and outperforms Vinyals 2016 in 1-shot and 5-shot Mini-ImageNet experiments.
- The base learner in this setting (simple ConvNet classifier) is quite different from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals 2016. It is always exciting when state-of-the-art results can be reported using very different approaches, rather than incremental follow-up work.
- As far as I know, the insight about the relationship between the memory cell and gradient descent updates is novel here. It is interesting regardless.
- The paper offers several practical insights about how to design and train an LSTM meta-learner, which should make it easier for others to replicate this work and apply these ideas to new problems. These include proper initialization, weight sharing across coordinates, and the importance of normalizing/rescaling the loss, gradient, and parameter inputs. Some of the insights have been previously described (the importance of simulating test conditions during meta-training; assuming independence between meta-learner and base learner parameters when taking gradients with respect to the meta-learner parameters), but the discussion here is useful nonetheless.

Weaknesses:
- The writing is at times quite opaque. While it describes very interesting work, I would not call the paper an enjoyable read. It took me multiple passes (as well as consulting related work) to understand the general learning problem. The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area. The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks. This would definitely make it accessible to a wider audience.
- Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me. Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed through meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)? Many other details like these are unclear (see question).
- The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them.

This is an interesting paper with convincing results. It seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved. I will definitely raise my score if the writing is improved.",1
"This paper makes a valuable contribution to provide a more clear understanding of generative adversarial network (GAN) training procedure. 

With the new insight of the training dynamics of GAN, as well as its variant, the authors reveal the reason that why the gradient is either vanishing in original GAN or unstable in its variant. More importantly, they also provide a way to avoid such difficulties by introducing perturbation. I believe this paper will inspire more principled research in this direction. 

I am very interested in the perturbation trick to avoid the gradient instability and vanishment. In fact, this is quite related to dropout trick in where the perturbation can be viewed as Bernoulli distribution. It will be great if the connection can be discussed.  Besides the theoretical analysis, is there any empirical study to justify this trick? Could you please add some experiments like Fig 2 and 3 for the perturbated GAN for comparison?",1
"This paper makes a valuable contribution to provide a more clear understanding of generative adversarial network (GAN) training procedure. 

With the new insight of the training dynamics of GAN, as well as its variant, the authors reveal the reason that why the gradient is either vanishing in original GAN or unstable in its variant. More importantly, they also provide a way to avoid such difficulties by introducing perturbation. I believe this paper will inspire more principled research in this direction. 

I am very interested in the perturbation trick to avoid the gradient instability and vanishment. In fact, this is quite related to dropout trick in where the perturbation can be viewed as Bernoulli distribution. It will be great if the connection can be discussed.  Besides the theoretical analysis, is there any empirical study to justify this trick? Could you please add some experiments like Fig 2 and 3 for the perturbated GAN for comparison?",1
"This work proposes to train RL agents to also perform auxiliary tasks, positing that doing so will help models learn stronger features.
They propose two pseudo-control tasks, control the change in pixel intensity, and control the activation of latent features. They also propose a supervised regression task, predict immediate reward following a sequence of events. The latter is learned offline via a skewed sampling of an experience replay buffer in order to balance seeing reward or not to 1/2 chance.
Such agents perform significantly well on discrete-action-continuous-space RL tasks, and reach baseline performance in 10x less iterations. 

This work contrasts with traditional ""passive"" unsupervised or model-based learning. Instead of forcing the model to learn a potentially useless representation of the input, or to learn the possibly impossible (due to partial observability) task-modelling objective, learning to control local and internal features of the environment complements learning the optimal control policy.

To me the approach is novel and proposes a very interesting alternative to unsupervised learning that takes advantage of the ""possibility"" of control that an agent has over the environment.
The proposed tasks are explained at a rather high level, which is convenient to understand intuition, but I think some lower level of detail might be useful. For example L_PC should be explicitly mentioned, before reaching the appendix. Otherwise this work is clear and easily understandable by readers familiar with Deep RL.
The methodology is sound, on one hand hand the distribution of best hyperparameters might be different for A3C and UNREAL, but also measuring top-3 ensures that, presuming that the both best hyperparameters for A3C and for UNREAL are within the explored intervals, the per-method best hyperparameters are found.
I think one weakness of the paper (or rather, considering the number of things that can fit in a paper, crucially needed future work) is that there is very little experimental analysis of the effect of the auxiliary tasks appart from their (very strong) effect on performance. In the same vein, pixel/feature control seems to have the most impact, in Labyrinth just A3C+PC beats anything else (except UNREAL), I think it would have been worth looking at this, either in isolation or in more depth, measuring more than just performance on RL tasks.",1
"This work proposes to train RL agents to also perform auxiliary tasks, positing that doing so will help models learn stronger features.
They propose two pseudo-control tasks, control the change in pixel intensity, and control the activation of latent features. They also propose a supervised regression task, predict immediate reward following a sequence of events. The latter is learned offline via a skewed sampling of an experience replay buffer in order to balance seeing reward or not to 1/2 chance.
Such agents perform significantly well on discrete-action-continuous-space RL tasks, and reach baseline performance in 10x less iterations. 

This work contrasts with traditional ""passive"" unsupervised or model-based learning. Instead of forcing the model to learn a potentially useless representation of the input, or to learn the possibly impossible (due to partial observability) task-modelling objective, learning to control local and internal features of the environment complements learning the optimal control policy.

To me the approach is novel and proposes a very interesting alternative to unsupervised learning that takes advantage of the ""possibility"" of control that an agent has over the environment.
The proposed tasks are explained at a rather high level, which is convenient to understand intuition, but I think some lower level of detail might be useful. For example L_PC should be explicitly mentioned, before reaching the appendix. Otherwise this work is clear and easily understandable by readers familiar with Deep RL.
The methodology is sound, on one hand hand the distribution of best hyperparameters might be different for A3C and UNREAL, but also measuring top-3 ensures that, presuming that the both best hyperparameters for A3C and for UNREAL are within the explored intervals, the per-method best hyperparameters are found.
I think one weakness of the paper (or rather, considering the number of things that can fit in a paper, crucially needed future work) is that there is very little experimental analysis of the effect of the auxiliary tasks appart from their (very strong) effect on performance. In the same vein, pixel/feature control seems to have the most impact, in Labyrinth just A3C+PC beats anything else (except UNREAL), I think it would have been worth looking at this, either in isolation or in more depth, measuring more than just performance on RL tasks.",1
"This paper proposed to use RL and RNN to design the architecture of networks for specific tasks. The idea of the paper is quite promising and the experimental results on two datasets show that method is solid.
The pros of the paper are:
1. The idea of using RNN to produce the description of the network and using RL to train the RNN is interesting and promising.
2. The generated architecture looks similar to what human designed, which shows that the human expertise and the generated network architectures are compatible.

The cons of the paper are:
1. The training time of the network is long, even with a lot of computing resources. 
2. The experiments did not provide the generality of the generated architectures. It would be nice to see the performances of the generated architecture on other similar but different datasets, especially the generated sequential models.

Overall, I believe this is a nice paper. But it need more experiments to show its potential advantage over the human designed models.",1
"This paper proposed to use RL and RNN to design the architecture of networks for specific tasks. The idea of the paper is quite promising and the experimental results on two datasets show that method is solid.
The pros of the paper are:
1. The idea of using RNN to produce the description of the network and using RL to train the RNN is interesting and promising.
2. The generated architecture looks similar to what human designed, which shows that the human expertise and the generated network architectures are compatible.

The cons of the paper are:
1. The training time of the network is long, even with a lot of computing resources. 
2. The experiments did not provide the generality of the generated architectures. It would be nice to see the performances of the generated architecture on other similar but different datasets, especially the generated sequential models.

Overall, I believe this is a nice paper. But it need more experiments to show its potential advantage over the human designed models.",1
"The paper presents an on-policy method to predict future intrinsic measurements. All the experiments are performed in the game of Doom (vizDoom to be exact), and instead of just predicting win/loss or the number of frags (score), the authors trained their model to predict (a sequence of) triplets of (health, ammunition, frags), weighted by (a sequence of) ""goal"" triplets that they provided as input. Changing the weights of the goal triplet is a way to perform/guide exploration. At test time, one can act by maximizing the long term goal only.

The results are impressive, as this model won the 2016 vizDoom competition. The experimental section of the paper seems sound:
 - There are comparisons of DFP with A3C, DQN, and an attempt to compare with DSR (a recent similar approach from Kulkarni et al., 2016). DFP outperforms other approaches (or equal them when they reach a ceiling / optimum, as for A3C in scenario D1).
 - There is an ablation study that supports the thesis that all the ""added complexity"" of the paper's model is useful.

Predicting intrinsic motivation (Singh et al. 2004), auxiliary variables, and forward modelling, are well-studied domains of reinforcement learning. The version that I read (December 4th revision) adequately references prior work, even if it is not completely exhaustive. 

A few comments (nitpicks) on the form:
 - Doom is described as a 3D environment, whereas it is actually a 2D environment (the height is not a discriminative/actionable dimension) presented in (fake) 3D.
 - The use of ""P"" in (2) (and subsequently) may be misleading as it stands for prediction but not probability (as is normally the case for P).
 - The double use of ""j"" (admittedly, with different fonts) in (6) may be misleading.
 - Results tables could repeat the units of the measurements (in particular as they are heterogenous in Table 1).

I think that this paper is a clear accept. One could argue that experiments could be conducted on different environments or that the novelty is limited, but I feel that ""correct"" (no-nonsense, experimentally sound on Doom, appendix providing details for reproducibility) and ""milestone"" (vizDoom winner) papers should get published.",1
"The paper presents an on-policy method to predict future intrinsic measurements. All the experiments are performed in the game of Doom (vizDoom to be exact), and instead of just predicting win/loss or the number of frags (score), the authors trained their model to predict (a sequence of) triplets of (health, ammunition, frags), weighted by (a sequence of) ""goal"" triplets that they provided as input. Changing the weights of the goal triplet is a way to perform/guide exploration. At test time, one can act by maximizing the long term goal only.

The results are impressive, as this model won the 2016 vizDoom competition. The experimental section of the paper seems sound:
 - There are comparisons of DFP with A3C, DQN, and an attempt to compare with DSR (a recent similar approach from Kulkarni et al., 2016). DFP outperforms other approaches (or equal them when they reach a ceiling / optimum, as for A3C in scenario D1).
 - There is an ablation study that supports the thesis that all the ""added complexity"" of the paper's model is useful.

Predicting intrinsic motivation (Singh et al. 2004), auxiliary variables, and forward modelling, are well-studied domains of reinforcement learning. The version that I read (December 4th revision) adequately references prior work, even if it is not completely exhaustive. 

A few comments (nitpicks) on the form:
 - Doom is described as a 3D environment, whereas it is actually a 2D environment (the height is not a discriminative/actionable dimension) presented in (fake) 3D.
 - The use of ""P"" in (2) (and subsequently) may be misleading as it stands for prediction but not probability (as is normally the case for P).
 - The double use of ""j"" (admittedly, with different fonts) in (6) may be misleading.
 - Results tables could repeat the units of the measurements (in particular as they are heterogenous in Table 1).

I think that this paper is a clear accept. One could argue that experiments could be conducted on different environments or that the novelty is limited, but I feel that ""correct"" (no-nonsense, experimentally sound on Doom, appendix providing details for reproducibility) and ""milestone"" (vizDoom winner) papers should get published.",1
"I think that the paper is quite interesting and useful. 
It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime.",1
"I think that the paper is quite interesting and useful. 
It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime.",1
"Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.

One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.

Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.

Finally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.

Other comments:

Discussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as “teacher-learner”). The only time the private labeled data is used is when learning the “primary ensemble.”  A ""secondary ensemble"" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.

G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.

Section C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. 

The paper is extremely well-written, for the most part. Some places needing clarification include:
- Last paragraph of 3.1. “all teachers….get the same training data….” This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.
- 4.1: The authors state: “The number n of teachers is limited by a trade-off between the classification task’s complexity and the available data.” However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.
- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.",1
"Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.

One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.

Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.

Finally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.

Other comments:

Discussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as “teacher-learner”). The only time the private labeled data is used is when learning the “primary ensemble.”  A ""secondary ensemble"" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.

G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.

Section C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. 

The paper is extremely well-written, for the most part. Some places needing clarification include:
- Last paragraph of 3.1. “all teachers….get the same training data….” This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.
- 4.1: The authors state: “The number n of teachers is limited by a trade-off between the classification task’s complexity and the available data.” However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.
- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.",1
"The paper presents an amortised MAP estimation method for SR problems. By learning a neural network which learns to project to an affine subspace of SR solutions which are consistent with the LR method the method enables finding propoer solutions with by using a variety of methods: GANs, noise assisted and density assisted optimisation.
Results are nicely demonstrated on several datasets.

I like the paper all in all, though I feel the writing can be polished by quite a bit and presentation should be made clearer. It was hard to follow at times and considering the subject matter is quite complicated making it clearer would help. Also, I would love to see some more analysis of the resulting the networks - what kind of features to they learn?",1
"The paper presents an amortised MAP estimation method for SR problems. By learning a neural network which learns to project to an affine subspace of SR solutions which are consistent with the LR method the method enables finding propoer solutions with by using a variety of methods: GANs, noise assisted and density assisted optimisation.
Results are nicely demonstrated on several datasets.

I like the paper all in all, though I feel the writing can be polished by quite a bit and presentation should be made clearer. It was hard to follow at times and considering the subject matter is quite complicated making it clearer would help. Also, I would love to see some more analysis of the resulting the networks - what kind of features to they learn?",1
"I like the setting presented in this paper but I have several criticism/questions:

(1) What are the failure model of this work? As richness of behaviors get complex, I expected this approach to have issues with the diversity of skills that could be discovered.

(2) Looking at Sec 5.3 -- "" let X be a random variable denoting the grid in which the agent is currently situated"" -- is the space discretized? And if so why and what happens if it isn't. 

(3) Expanding on the first point, does the approach work with more complicated embodiment? Say a 5-link swimmer instead of 2? I think this is important to assess the generality of this approach

(4) Authors claim that ""Recently, Heess et al. (2016) have independently proposed to learn a range of skills in a pre-training environment that will be useful for the downstream tasks, which is similar to our framework. However, their pre-training setup requires a set of goals to be specified. In comparison, we use intrinsic rewards as the only signal to the agent during the pre-training phase, the construction of which only requires very minimal domain knowledge.""

I don't entirely agree with this. The rewards that this paper proposes are also quite hand-crafted and specific to a seemingly limited set of control tasks.",1
"I like the setting presented in this paper but I have several criticism/questions:

(1) What are the failure model of this work? As richness of behaviors get complex, I expected this approach to have issues with the diversity of skills that could be discovered.

(2) Looking at Sec 5.3 -- "" let X be a random variable denoting the grid in which the agent is currently situated"" -- is the space discretized? And if so why and what happens if it isn't. 

(3) Expanding on the first point, does the approach work with more complicated embodiment? Say a 5-link swimmer instead of 2? I think this is important to assess the generality of this approach

(4) Authors claim that ""Recently, Heess et al. (2016) have independently proposed to learn a range of skills in a pre-training environment that will be useful for the downstream tasks, which is similar to our framework. However, their pre-training setup requires a set of goals to be specified. In comparison, we use intrinsic rewards as the only signal to the agent during the pre-training phase, the construction of which only requires very minimal domain knowledge.""

I don't entirely agree with this. The rewards that this paper proposes are also quite hand-crafted and specific to a seemingly limited set of control tasks.",1
"Summary:
This paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.

Review:
The proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.

I appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: “Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.” “Such observation models can easily achieve much higher log-likelihood scores, […].”)

Comparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.

Minor:
– I am missing citations for “ordered visible dimension sampling”
– Typos and frequent incorrect use of \citet and \citep",1
"Summary:
This paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.

Review:
The proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.

I appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: “Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.” “Such observation models can easily achieve much higher log-likelihood scores, […].”)

Comparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.

Minor:
– I am missing citations for “ordered visible dimension sampling”
– Typos and frequent incorrect use of \citet and \citep",1
"This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models.  A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images.  The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models.

This work is an important step in understanding the nonlinear response properties of visual neurons.  Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models.  So this presents an important challenge to the field.  If we even had *a* model that works, it would be a starting point.  So this work should be seen in that light.  The challenge now of course is to tease apart what the rnn is doing.  Perhaps it could now be pruned and simplified to see what parts are critical to performance.  It would have been nice to see such an analysis.   Nevertheless this result is a good first start and I think important for people to know about.

I am a bit confused about what is being called a ""movie.""  My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each.  But then it is stated that the ""frame rate"" is 1/8.33 ms.  I think this must refer to the refresh rate of the monitor, right?   

I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies.  Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.",1
"This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models.  A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images.  The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models.

This work is an important step in understanding the nonlinear response properties of visual neurons.  Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models.  So this presents an important challenge to the field.  If we even had *a* model that works, it would be a starting point.  So this work should be seen in that light.  The challenge now of course is to tease apart what the rnn is doing.  Perhaps it could now be pruned and simplified to see what parts are critical to performance.  It would have been nice to see such an analysis.   Nevertheless this result is a good first start and I think important for people to know about.

I am a bit confused about what is being called a ""movie.""  My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each.  But then it is stated that the ""frame rate"" is 1/8.33 ms.  I think this must refer to the refresh rate of the monitor, right?   

I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies.  Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.",1
"The authors present a way to complement the Gerative Adversarial Network traning procedure with an additional term based on denoising autoencoders. The use of denoising autoencoders is motivated by the observation that they implicitly capture the distribution of the data they were trained on. While sampling methods based denoising autoencoders alone don't amount to very interesting generative models (at least no-one could demonstrate otherwise), this paper shows that it can be combined successfully with generative adversarial networks.

My overall assessment of this paper is that it is well written, well reasoned, and presents a good idea motivated from first principles. I feel that the idea presented here is not revolutionary or a very radical departure from what has been done before, I would have liked a slightly more structured experiments section which focusses on and provides insights into the relative merits of different choices one could make (see pre-review questions for details), rather than focussing just on demonstrating that a chosen variant works.

In addition to this general review, I have already posted specific questions and criticism in the pre-review questions - thanks for the authors' responses. Based on those responses the area I am most uncomfortable about is whether the (Alain & Bengio, 2014) intuition about the denoising autoencoders is valid if it all happens in a nonlinear featurespace. If the denoiser function's behaviour ends up depending on the Jacobian of the nonlinear transformation Phi, another question is whether this dependence is exploitable by the optimization scheme.",1
"The authors present a way to complement the Gerative Adversarial Network traning procedure with an additional term based on denoising autoencoders. The use of denoising autoencoders is motivated by the observation that they implicitly capture the distribution of the data they were trained on. While sampling methods based denoising autoencoders alone don't amount to very interesting generative models (at least no-one could demonstrate otherwise), this paper shows that it can be combined successfully with generative adversarial networks.

My overall assessment of this paper is that it is well written, well reasoned, and presents a good idea motivated from first principles. I feel that the idea presented here is not revolutionary or a very radical departure from what has been done before, I would have liked a slightly more structured experiments section which focusses on and provides insights into the relative merits of different choices one could make (see pre-review questions for details), rather than focussing just on demonstrating that a chosen variant works.

In addition to this general review, I have already posted specific questions and criticism in the pre-review questions - thanks for the authors' responses. Based on those responses the area I am most uncomfortable about is whether the (Alain & Bengio, 2014) intuition about the denoising autoencoders is valid if it all happens in a nonlinear featurespace. If the denoiser function's behaviour ends up depending on the Jacobian of the nonlinear transformation Phi, another question is whether this dependence is exploitable by the optimization scheme.",1
"The paper considers the problem of transferring skills between robots with different morphologies, in the context of agents that have to perform several tasks.  A core component of the proposed approach is to use a task-invariant future space, which can be shared between tasks & between agents.

Compared to previous work (Ammar et al. 2015), it seems the main contribution here is to “assume that good correspondences in episodic tasks can be extracted through time alignment” (Sec. 2).  This is an interesting hypothesis. There is also similarity to work by Raimalwala et al (2016), but the authors argue their method is better equipped to handle non-linear dynamics. These are two interesting hypotheses, however I don’t see that they have been verified in the presented empirical results.  In particular, the question of the pairing correspondence seems crucial. What happens when the time alignment is not suitable. Is it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.

In general, more comparison to other transfer methods, including those listed in Sec.2, would be very valuable.  The addition of Sec.5.1 is definitely a right step in this direction, but represents a small portion of the recent work on transfer learning.  I appreciate that other methods transfer other pieces of information (e.g. the policy), but still if the end goal is better performance, what is worth transferring (in addition to how to do the transfer) should be a reasonable question to explore.

Overall, the paper tackles an important problem, but this is a very active area of research, and further comparison to other methods would be worthwhile.  The method proposed of transferring the representation is well motivated, cleanly described, and conceptually sound.  The assumption that time alignment can be used for the state pairing seems problematic, and should be further validated.",1
"The paper considers the problem of transferring skills between robots with different morphologies, in the context of agents that have to perform several tasks.  A core component of the proposed approach is to use a task-invariant future space, which can be shared between tasks & between agents.

Compared to previous work (Ammar et al. 2015), it seems the main contribution here is to “assume that good correspondences in episodic tasks can be extracted through time alignment” (Sec. 2).  This is an interesting hypothesis. There is also similarity to work by Raimalwala et al (2016), but the authors argue their method is better equipped to handle non-linear dynamics. These are two interesting hypotheses, however I don’t see that they have been verified in the presented empirical results.  In particular, the question of the pairing correspondence seems crucial. What happens when the time alignment is not suitable. Is it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.

In general, more comparison to other transfer methods, including those listed in Sec.2, would be very valuable.  The addition of Sec.5.1 is definitely a right step in this direction, but represents a small portion of the recent work on transfer learning.  I appreciate that other methods transfer other pieces of information (e.g. the policy), but still if the end goal is better performance, what is worth transferring (in addition to how to do the transfer) should be a reasonable question to explore.

Overall, the paper tackles an important problem, but this is a very active area of research, and further comparison to other methods would be worthwhile.  The method proposed of transferring the representation is well motivated, cleanly described, and conceptually sound.  The assumption that time alignment can be used for the state pairing seems problematic, and should be further validated.",1
"On one hand this paper is fairly standard in that it uses deep metric learning with a Siamese architecture. On the other, the connections to human perception involving persistence is quite interesting. I'm not an expert in human vision, but the comparison in general and the induced hierarchical groupings in particular seem like something that should interest people in this community. The experimental suite is ok but I was disappointed that it is 100% synthetic. The authors could have used a minimally viable real dataset such as ALOI",1
"On one hand this paper is fairly standard in that it uses deep metric learning with a Siamese architecture. On the other, the connections to human perception involving persistence is quite interesting. I'm not an expert in human vision, but the comparison in general and the induced hierarchical groupings in particular seem like something that should interest people in this community. The experimental suite is ok but I was disappointed that it is 100% synthetic. The authors could have used a minimally viable real dataset such as ALOI",1
"The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.

Figure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.

The main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.

The authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.

Overall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.",1
"The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.

Figure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.

The main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.

The authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.

Overall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.",1
"This paper proposed a neural attention model which has a learnable and differentiable sampling lattice. The work is well motivated as few previous work focus on learning the sampling lattice but with a fixed lattice. This work is quite similar to Spatial Transformer Networks (Jaderberg 2015), but the sampling lattice is learned by the model. The experiments showed that the model can learn a meaning lattice to the visual search task where the sampling lattice looks similar to human being's. 

The main concern of the paper is that experiments are not sufficient. The paper only reports the results on a modified clustered MNIST dataset. It would be more interesting if the authors could conduct  the model on real datasets, such as Toronto Face dataset, CUB bird dataset and SVHN. For example, for the Face dataset, it would be nice if the model can learn to attend different parts of the face for expression recognition, or attend different part of birds for fine-grained classification. Since the authors replied in the pre-review question that the model can learn meaningful lattice on MSCOCO dataset, I think it would be better to add that results into the paper.

Another drawback of the model is that the paper only compare with different variants of itselves. I suggest that this paper should compare with  Spatial Transformer Networks, DRAW, etc., on the same dataset to show the advantage of the learned sampling lattice.",1
"This paper proposed a neural attention model which has a learnable and differentiable sampling lattice. The work is well motivated as few previous work focus on learning the sampling lattice but with a fixed lattice. This work is quite similar to Spatial Transformer Networks (Jaderberg 2015), but the sampling lattice is learned by the model. The experiments showed that the model can learn a meaning lattice to the visual search task where the sampling lattice looks similar to human being's. 

The main concern of the paper is that experiments are not sufficient. The paper only reports the results on a modified clustered MNIST dataset. It would be more interesting if the authors could conduct  the model on real datasets, such as Toronto Face dataset, CUB bird dataset and SVHN. For example, for the Face dataset, it would be nice if the model can learn to attend different parts of the face for expression recognition, or attend different part of birds for fine-grained classification. Since the authors replied in the pre-review question that the model can learn meaningful lattice on MSCOCO dataset, I think it would be better to add that results into the paper.

Another drawback of the model is that the paper only compare with different variants of itselves. I suggest that this paper should compare with  Spatial Transformer Networks, DRAW, etc., on the same dataset to show the advantage of the learned sampling lattice.",1
"This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise.  The original Bell & Sejnowski infomax framework only considered the no noise case.  Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods.

This seems like an interesting and potentially more general approach to unsupervised learning.  However the paper is quite long and it was difficult for me to follow all the twists and turns.  For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going.  'Hierarchical' is probably not the right terminology here because it's not like a deep net hierarchy, it's just decomposing the tuning curve function into different parts.  I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document.

Also, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related.  They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges.",1
"This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise.  The original Bell & Sejnowski infomax framework only considered the no noise case.  Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods.

This seems like an interesting and potentially more general approach to unsupervised learning.  However the paper is quite long and it was difficult for me to follow all the twists and turns.  For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going.  'Hierarchical' is probably not the right terminology here because it's not like a deep net hierarchy, it's just decomposing the tuning curve function into different parts.  I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document.

Also, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related.  They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges.",1
"Summary:
This paper on autoregressive generative models explores various extensions of PixelCNNs. The proposed changes are to replace the softmax function with a logistic mixture model, to use dropout for regularization, to use downsampling to increase receptive field size, and the introduction of particular skip connections. The authors find that this allows the PixelCNN to outperform a PixelRNN on CIFAR-10, the previous state-of-the-art model. The authors further explore the performance of PixelCNNs with smaller receptive field sizes.

Review:
This is a useful contribution towards better tractable image models. In particular, autoregressive models can be quite slow at test time, and the more efficient architectures described here should help with that.

My main criticism regards the severe neglect of related work. Mixture models have been used a lot in autoregressive image modeling, including for multivariate conditional densities and including downsampling to increase receptive field size, albeit in a different manner: Domke (2008), Hosseini et al. (2010), Theis et al. (2012), Uria et al. (2013), Theis et al. (2015). Note that the logistic distribution is a special case of the Gaussian scale mixture (West, 1978).

The main difference seems to be the integration of the density to model integers. While this is clearly a good idea and the right way forward, the authors claim but do not support that not doing this has “proved to be a problem for earlier models based on continuous distributions”. Please elaborate, add a reference, or ideally report the performance achieved by PixelCNN++ without integration (and instead adding uniform noise to make the variables continuous).

60,000 images are not a lot in a high-dimensional space. While I can see the usefulness of regularization for specialized content – and this can serve as a good example to demonstrate the usefulness of dropout – why not use “80 million tiny images” (superset of CIFAR-10) for natural images? Semi-supervised learning should be fairly trivial here (because the model’s likelihood is tractable), so this data could even be used in the class-conditional case.

It would be interesting to know how fast the different models are at test time (i.e., when generating images).",1
"Summary:
This paper on autoregressive generative models explores various extensions of PixelCNNs. The proposed changes are to replace the softmax function with a logistic mixture model, to use dropout for regularization, to use downsampling to increase receptive field size, and the introduction of particular skip connections. The authors find that this allows the PixelCNN to outperform a PixelRNN on CIFAR-10, the previous state-of-the-art model. The authors further explore the performance of PixelCNNs with smaller receptive field sizes.

Review:
This is a useful contribution towards better tractable image models. In particular, autoregressive models can be quite slow at test time, and the more efficient architectures described here should help with that.

My main criticism regards the severe neglect of related work. Mixture models have been used a lot in autoregressive image modeling, including for multivariate conditional densities and including downsampling to increase receptive field size, albeit in a different manner: Domke (2008), Hosseini et al. (2010), Theis et al. (2012), Uria et al. (2013), Theis et al. (2015). Note that the logistic distribution is a special case of the Gaussian scale mixture (West, 1978).

The main difference seems to be the integration of the density to model integers. While this is clearly a good idea and the right way forward, the authors claim but do not support that not doing this has “proved to be a problem for earlier models based on continuous distributions”. Please elaborate, add a reference, or ideally report the performance achieved by PixelCNN++ without integration (and instead adding uniform noise to make the variables continuous).

60,000 images are not a lot in a high-dimensional space. While I can see the usefulness of regularization for specialized content – and this can serve as a good example to demonstrate the usefulness of dropout – why not use “80 million tiny images” (superset of CIFAR-10) for natural images? Semi-supervised learning should be fairly trivial here (because the model’s likelihood is tractable), so this data could even be used in the class-conditional case.

It would be interesting to know how fast the different models are at test time (i.e., when generating images).",1
"This paper provides a new perspective to understanding the ResNet and Highway net. The new perspective assumes that the blocks inside the networks with residual or skip-connection are groups of successive layers with the same hidden size, which performs to iteratively refine their estimates of the same feature instead of generate new representations. Under this perspective, some contradictories with the traditional representation view induced by ResNet and Highway network and other paper can be well explained.

The pros of the paper are:
1. A novel perspective to understand the recent progress of neural network is proposed.
2. The paper provides a quantitatively experimentals to compare ResNet and Highway net, and shows contradict results with several claims from previous work. The authors also give discussions and explanations about the contradictories, which provides a good insight of the disadvantages and advantages between these two kind of networks.

The main cons of the paper is that the experiments are not sufficient. For example, since the main contribution of the paper is to propose the “unrolled iterative estimation"" and the stage 4 of Figure 3 seems not follow the assumption of ""unrolled iterative estimation"" and the authors says: ""We note that stage four (with three blocks) appears to be underestimating the representation values, indicating a probable weak link in the architecture."". Thus, it would be much better to do experiments to show that under some condition, the performance of stage 4 can follow the assumption. 

Moreover, the paper should provide more experiments to show the evidence of ""unrolled iterative estimation"", not comparing ResNet with Highway Net. The lack of experiments on this point is the main concern from myself.",1
"This paper provides a new perspective to understanding the ResNet and Highway net. The new perspective assumes that the blocks inside the networks with residual or skip-connection are groups of successive layers with the same hidden size, which performs to iteratively refine their estimates of the same feature instead of generate new representations. Under this perspective, some contradictories with the traditional representation view induced by ResNet and Highway network and other paper can be well explained.

The pros of the paper are:
1. A novel perspective to understand the recent progress of neural network is proposed.
2. The paper provides a quantitatively experimentals to compare ResNet and Highway net, and shows contradict results with several claims from previous work. The authors also give discussions and explanations about the contradictories, which provides a good insight of the disadvantages and advantages between these two kind of networks.

The main cons of the paper is that the experiments are not sufficient. For example, since the main contribution of the paper is to propose the “unrolled iterative estimation"" and the stage 4 of Figure 3 seems not follow the assumption of ""unrolled iterative estimation"" and the authors says: ""We note that stage four (with three blocks) appears to be underestimating the representation values, indicating a probable weak link in the architecture."". Thus, it would be much better to do experiments to show that under some condition, the performance of stage 4 can follow the assumption. 

Moreover, the paper should provide more experiments to show the evidence of ""unrolled iterative estimation"", not comparing ResNet with Highway Net. The lack of experiments on this point is the main concern from myself.",1
"This paper not only shows that a cache model on top of a pre-trained RNN can improve language modeling, but also illustrates a shortcoming of standard RNN models in that they are unable to capture this information themselves. Regardless of whether this is due to the small BPTT window (35 is standard) or an issue with the capability of the RNN itself, this is a useful insight. This technique is an interesting variation of memory augmented neural networks with a number of advantages to many of the standard memory augmented architectures.

They illustrate the neural cache model on not just the Penn Treebank but also WikiText-2 and WikiText-103, two datasets specifically tailored to illustrating long term dependencies with a more realistic vocabulary size. I have not seen the ability to refer up to 2000 words back previously.
I recommend this paper be accepted. There is additionally extensive analysis of the hyperparameters on these datasets, providing further insight.

I recommend this interesting and well analyzed paper be accepted.",1
"This paper not only shows that a cache model on top of a pre-trained RNN can improve language modeling, but also illustrates a shortcoming of standard RNN models in that they are unable to capture this information themselves. Regardless of whether this is due to the small BPTT window (35 is standard) or an issue with the capability of the RNN itself, this is a useful insight. This technique is an interesting variation of memory augmented neural networks with a number of advantages to many of the standard memory augmented architectures.

They illustrate the neural cache model on not just the Penn Treebank but also WikiText-2 and WikiText-103, two datasets specifically tailored to illustrating long term dependencies with a more realistic vocabulary size. I have not seen the ability to refer up to 2000 words back previously.
I recommend this paper be accepted. There is additionally extensive analysis of the hyperparameters on these datasets, providing further insight.

I recommend this interesting and well analyzed paper be accepted.",1
"Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.

This paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.

Pros:
1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.

2. The proposed method produces visually appealing results on several datasets

3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task

4. The paper is well-written and easy to read

Cons:
1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)

2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.

3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.

I would also like to point out that using super-resolved outputs as opposed to the actual model’s outputs can produce a false impression of the visual quality of the transferred samples. I’d suggest moving original outputs from the appendix into the main part.",1
"Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.

This paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.

Pros:
1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.

2. The proposed method produces visually appealing results on several datasets

3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task

4. The paper is well-written and easy to read

Cons:
1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)

2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.

3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.

I would also like to point out that using super-resolved outputs as opposed to the actual model’s outputs can produce a false impression of the visual quality of the transferred samples. I’d suggest moving original outputs from the appendix into the main part.",1
"The paper extends the imitation learning paradigm to the case where the demonstrator and learner have different points of view. This is an important contribution, with several good applications.  The main insight is to use adversarial training to learn a policy that is robust to this difference in perspective.  This problem formulation is quite novel compared to the standard imitation learning literature (usually first-order perspective), though has close links to the literature on transfer learning (as explained in Sec.2).

The basic approach is clearly explained, and follows quite readily from recent literature on imitation learning and adversarial training.

I would have expected to see comparison to the following methods added to Figure 3:
1)  Standard 1st person imitation learning using agent A data, and apply the policy on agent A.  This is an upper-bound on how well you can expect to do, since you have the correct perspective.
2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.  Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance.
3)  Reinforcement learning using agent A data, and apply the policy on agent A.  I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents). I understand this is how the expert data is collected for the demonstrator, but I don’t see the performance results from just using this procedure on the learner (to compare to Fig.3 results).

Including these results would in my view significantly enhance the impact of the paper.",1
"The paper extends the imitation learning paradigm to the case where the demonstrator and learner have different points of view. This is an important contribution, with several good applications.  The main insight is to use adversarial training to learn a policy that is robust to this difference in perspective.  This problem formulation is quite novel compared to the standard imitation learning literature (usually first-order perspective), though has close links to the literature on transfer learning (as explained in Sec.2).

The basic approach is clearly explained, and follows quite readily from recent literature on imitation learning and adversarial training.

I would have expected to see comparison to the following methods added to Figure 3:
1)  Standard 1st person imitation learning using agent A data, and apply the policy on agent A.  This is an upper-bound on how well you can expect to do, since you have the correct perspective.
2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.  Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance.
3)  Reinforcement learning using agent A data, and apply the policy on agent A.  I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents). I understand this is how the expert data is collected for the demonstrator, but I don’t see the performance results from just using this procedure on the learner (to compare to Fig.3 results).

Including these results would in my view significantly enhance the impact of the paper.",1
"This paper combines variational RNN (VRNN) and domain adversarial networks (DANN) for domain adaptation in the sequence modelling domain.  The VRNN is used to learn representations for sequential data, which is the hidden states of the last time step.  The DANN is used to make the representations domain invariant, therefore achieving cross domain adaptation.

Experiments are done on a number of data sets, and the proposed method (VRADA) outperforms baselines including DANN, VFAE and R-DANN on almost all of them.

I don't have questions about the proposed model, the model is quite clear and seems to be a simple combination of VRNN and DANN.  But a few questions came up during the pre-review question phase:

- As the authors have mentioned, DANN in general outperforms MMD based methods, however, the VFAE method which is based on MMD regularization on the representations seems to outperform DANN across the board.  That seems to indicate VRNN + MMD should also be a good combination.

- One baseline the authors showed in the experiments is R-DANN, which is an RNN version of DANN.  There are two differences between R-DANN and VRADA: (1) R-DANN uses deterministic RNN for representation learning, while VRADA uses variational RNN; (2) on target domain R-DANN only optimizes adversarial loss, while VRADA optimizes both adversarial loss and reconstruction loss for feature learning.  It would be good to analyze further where the performance gain comes from.",1
"This paper combines variational RNN (VRNN) and domain adversarial networks (DANN) for domain adaptation in the sequence modelling domain.  The VRNN is used to learn representations for sequential data, which is the hidden states of the last time step.  The DANN is used to make the representations domain invariant, therefore achieving cross domain adaptation.

Experiments are done on a number of data sets, and the proposed method (VRADA) outperforms baselines including DANN, VFAE and R-DANN on almost all of them.

I don't have questions about the proposed model, the model is quite clear and seems to be a simple combination of VRNN and DANN.  But a few questions came up during the pre-review question phase:

- As the authors have mentioned, DANN in general outperforms MMD based methods, however, the VFAE method which is based on MMD regularization on the representations seems to outperform DANN across the board.  That seems to indicate VRNN + MMD should also be a good combination.

- One baseline the authors showed in the experiments is R-DANN, which is an RNN version of DANN.  There are two differences between R-DANN and VRADA: (1) R-DANN uses deterministic RNN for representation learning, while VRADA uses variational RNN; (2) on target domain R-DANN only optimizes adversarial loss, while VRADA optimizes both adversarial loss and reconstruction loss for feature learning.  It would be good to analyze further where the performance gain comes from.",1
"This paper introduces a novel method for language modeling which is suitable for both modeling programming language as well as natural language. The approach uses a program synthesis algorithm to search over program space and uses count-based estimation of the weights of the program. This is a departure from neural network-based approaches which rely on gradient descent, and thus are extremely slow to estimate. Count-based method such as regular n-gram models suffer because of their simplicity, i.e. not being able to model large context, and scaling badly as context increases. The proposed approach synthesizes programs using MCMC which learn context-sensitive probabilities using count-based estimation, and thus is both fast and able to model long-range context.

Experiments on a programming language datasets, the linux kernel corpus, show that this method is vastly better than both LSTM and n-gram language models. Experiments on the Wikipedia corpus show that the method is competitive, but not better, to SOTA models. Both estimation and query time are significantly better than LSTM LMs, and competitive to n-gram LMs.

It's debatable whether this paper is suitable for ICLR, due to ICLR's focus on neural network-based approaches. However, in the interest of diversity and novelty, such ""outside"" papers should be accepted to ICLR. This paper is likely to inspire more research into fusion of program synthesis and machine learning methods, which was a popular theme at NIPS 2016.

*Pros*
1. Novel approach.
2. Good results.

*Cons*
1. Some significant algorithmic details are not included in the paper. They should at least be included in an appendix for comprehensiveness.

*Comments*
1. Please include n-gram results in the table for Wikipedia results.",1
"This paper introduces a novel method for language modeling which is suitable for both modeling programming language as well as natural language. The approach uses a program synthesis algorithm to search over program space and uses count-based estimation of the weights of the program. This is a departure from neural network-based approaches which rely on gradient descent, and thus are extremely slow to estimate. Count-based method such as regular n-gram models suffer because of their simplicity, i.e. not being able to model large context, and scaling badly as context increases. The proposed approach synthesizes programs using MCMC which learn context-sensitive probabilities using count-based estimation, and thus is both fast and able to model long-range context.

Experiments on a programming language datasets, the linux kernel corpus, show that this method is vastly better than both LSTM and n-gram language models. Experiments on the Wikipedia corpus show that the method is competitive, but not better, to SOTA models. Both estimation and query time are significantly better than LSTM LMs, and competitive to n-gram LMs.

It's debatable whether this paper is suitable for ICLR, due to ICLR's focus on neural network-based approaches. However, in the interest of diversity and novelty, such ""outside"" papers should be accepted to ICLR. This paper is likely to inspire more research into fusion of program synthesis and machine learning methods, which was a popular theme at NIPS 2016.

*Pros*
1. Novel approach.
2. Good results.

*Cons*
1. Some significant algorithmic details are not included in the paper. They should at least be included in an appendix for comprehensiveness.

*Comments*
1. Please include n-gram results in the table for Wikipedia results.",1
"This work introduces some StarCraft micro-management tasks (controlling individual units during a battle). These tasks are difficult for recent DeepRL methods due to high-dimensional, variable action spaces (the action space is the task of each unit, the number of units may vary). In such large action spaces, simple exploration strategies (such as epsilon-greedy) perform poorly.

They introduce a novel algorithm ZO to tackle this problem. This algorithm combines ideas from policy gradient, deep networks trained with backpropagation for state embedding and gradient free optimization. The algorithm is well explained and is compared to some existing baselines. Due to the gradient free optimization providing for much better structured exploration, it performs far better.

This is a well-written paper and a novel algorithm which is applied to a very relevant problem. After the success of DeepRL approaches at learning in large state spaces such as visual environment, there is significant interest in applying RL to more structured state and action spaces. The tasks introduced here are interesting environments for these sorts of problems.

It would be helpful if the authors were able to share the source code / specifications for their tasks, to allow other groups to compare against this work.

I found section 5 (the details of the raw inputs and feature encodings) somewhat difficult to understand. In addition to clarifying, the authors might wish to consider whether they could provide the source code to their algorithm or at least the encoder to allow careful comparisons by other work.

Although discussed, there is no baseline comparison with valued based approaches with attempt to do better exploration by modeling uncertainty (such as Bootstrapped DQN). It would useful to understand how such approaches, which also promise better exploration, compare.

It would also be interesting to discuss whether action embedding models such as energy-based approaches (e.g.",1
"This work introduces some StarCraft micro-management tasks (controlling individual units during a battle). These tasks are difficult for recent DeepRL methods due to high-dimensional, variable action spaces (the action space is the task of each unit, the number of units may vary). In such large action spaces, simple exploration strategies (such as epsilon-greedy) perform poorly.

They introduce a novel algorithm ZO to tackle this problem. This algorithm combines ideas from policy gradient, deep networks trained with backpropagation for state embedding and gradient free optimization. The algorithm is well explained and is compared to some existing baselines. Due to the gradient free optimization providing for much better structured exploration, it performs far better.

This is a well-written paper and a novel algorithm which is applied to a very relevant problem. After the success of DeepRL approaches at learning in large state spaces such as visual environment, there is significant interest in applying RL to more structured state and action spaces. The tasks introduced here are interesting environments for these sorts of problems.

It would be helpful if the authors were able to share the source code / specifications for their tasks, to allow other groups to compare against this work.

I found section 5 (the details of the raw inputs and feature encodings) somewhat difficult to understand. In addition to clarifying, the authors might wish to consider whether they could provide the source code to their algorithm or at least the encoder to allow careful comparisons by other work.

Although discussed, there is no baseline comparison with valued based approaches with attempt to do better exploration by modeling uncertainty (such as Bootstrapped DQN). It would useful to understand how such approaches, which also promise better exploration, compare.

It would also be interesting to discuss whether action embedding models such as energy-based approaches (e.g.",1
"This paper essentially presents a new inductive bias in the architecture of (convolutional) neural networks (CNN). The mathematical motivations/derivations of the proposed architecture are detailed and rigorous. The proposed architecture promises to produce equivariant representations with steerable features using fewer parameters than traditional CNNs, which is particularly useful in small data regimes. Interesting and novel connections are presented between steerable filters and so called “steerable fibers”. The architecture is strongly inspired by the author’s previous work, as well as that of “capsules” (Hinton, 2011). The proposed architecture is compared on CIFAR10 against state-of-the-art inspired architectures (ResNets), and is shown to be superior particularly in the small data regime. The lack of empirical comparison on large scale dataset, such as ImageNet or COCO makes this largely a theoretical contribution. I would have also liked to see more empirical evaluation of the equivariance properties. It is not intuitively clear exactly why this architecture performs better on CIFAR10 as it is not clear that capturing equivariances helps to classify different instances of object categories. Wouldn’t action-recognition in videos, for example, not be a better illustrative dataset?",1
"This paper essentially presents a new inductive bias in the architecture of (convolutional) neural networks (CNN). The mathematical motivations/derivations of the proposed architecture are detailed and rigorous. The proposed architecture promises to produce equivariant representations with steerable features using fewer parameters than traditional CNNs, which is particularly useful in small data regimes. Interesting and novel connections are presented between steerable filters and so called “steerable fibers”. The architecture is strongly inspired by the author’s previous work, as well as that of “capsules” (Hinton, 2011). The proposed architecture is compared on CIFAR10 against state-of-the-art inspired architectures (ResNets), and is shown to be superior particularly in the small data regime. The lack of empirical comparison on large scale dataset, such as ImageNet or COCO makes this largely a theoretical contribution. I would have also liked to see more empirical evaluation of the equivariance properties. It is not intuitively clear exactly why this architecture performs better on CIFAR10 as it is not clear that capturing equivariances helps to classify different instances of object categories. Wouldn’t action-recognition in videos, for example, not be a better illustrative dataset?",1
"Paper Summary
This paper proposes an unsupervised learning model in which the network
predicts what its state would look like at the next time step (at input layer
and potentially other layers).  When these states are observed, an error signal
is computed by comparing the predictions and the observations. This error
signal is fed back into the model. The authors show that this model is able to
make good predictions on a toy dataset of rotating 3D faces as well as on
natural videos. They also show that these features help perform supervised
tasks.

Strengths
- The model is an interesting embodiment of the idea of predictive coding
  implemented using a end-to-end backpropable recurrent neural network architecture.
- The idea of feeding forward an error signal is perhaps not used as widely as it could
  be, and this work shows a compelling example of using it. 
- Strong empirical results and relevant comparisons show that the model works well.
- The authors present a detailed ablative analysis of the proposed model.

Weaknesses
- The model (esp. in Fig 1) is presented as a generalized predictive model
  where next step predictions are made at each layer. However, as discovered by
running the experiments, only the predictions at the input layer are the ones
that actually matter and the optimal choice seems to be to turn off the error
signal from the higher layers. While the authors intend to address this in future
work, I think this point merits some more discussion in the current work, given
the way this model is presented.
- The network currently lacks stochasticity and does not model the future as a
  multimodal distribution (However, this is mentioned as potential future work).

Quality
The experiments are well-designed and a detailed analysis is provided
in the appendix.

Clarity
The paper is well-written and easy to follow.

Originality
Some deep models have previously been proposed that use predictive coding.
However, the proposed model is most probably novel in the way it feds back the
error signal and implements the entire model as a single differentiable
network.

Significance
This paper will be of wide interest to the growing set of researchers working
in unsupervised learning of time series. This helps draw attention to
predictive coding as an important learning paradigm.

Overall
Good paper with detailed and well-designed experiments. The idea of feeding
forward the error signal is not being used as much as it could be in our
community. This work helps to draw the community's attention to this idea.",1
"Paper Summary
This paper proposes an unsupervised learning model in which the network
predicts what its state would look like at the next time step (at input layer
and potentially other layers).  When these states are observed, an error signal
is computed by comparing the predictions and the observations. This error
signal is fed back into the model. The authors show that this model is able to
make good predictions on a toy dataset of rotating 3D faces as well as on
natural videos. They also show that these features help perform supervised
tasks.

Strengths
- The model is an interesting embodiment of the idea of predictive coding
  implemented using a end-to-end backpropable recurrent neural network architecture.
- The idea of feeding forward an error signal is perhaps not used as widely as it could
  be, and this work shows a compelling example of using it. 
- Strong empirical results and relevant comparisons show that the model works well.
- The authors present a detailed ablative analysis of the proposed model.

Weaknesses
- The model (esp. in Fig 1) is presented as a generalized predictive model
  where next step predictions are made at each layer. However, as discovered by
running the experiments, only the predictions at the input layer are the ones
that actually matter and the optimal choice seems to be to turn off the error
signal from the higher layers. While the authors intend to address this in future
work, I think this point merits some more discussion in the current work, given
the way this model is presented.
- The network currently lacks stochasticity and does not model the future as a
  multimodal distribution (However, this is mentioned as potential future work).

Quality
The experiments are well-designed and a detailed analysis is provided
in the appendix.

Clarity
The paper is well-written and easy to follow.

Originality
Some deep models have previously been proposed that use predictive coding.
However, the proposed model is most probably novel in the way it feds back the
error signal and implements the entire model as a single differentiable
network.

Significance
This paper will be of wide interest to the growing set of researchers working
in unsupervised learning of time series. This helps draw attention to
predictive coding as an important learning paradigm.

Overall
Good paper with detailed and well-designed experiments. The idea of feeding
forward the error signal is not being used as much as it could be in our
community. This work helps to draw the community's attention to this idea.",1
The problem addressed here is practically important (supervised learning with n<,1
The problem addressed here is practically important (supervised learning with n<,1
"UPDATE: The authors addressed all my concerns in the new version of the paper, so I raised my score and now recommend acceptance.
--------------
This paper combines the recent progress in variational autoencoder and autoregressive density modeling in the proposed PixelVAE model. The paper shows that it can match the NLL performance of a PixelCNN with a PixelVAE that has a much shallower PixelCNN decoder.
I think the idea of capturing the global structure with a VAE and modeling the local structure with a PixelCNN decoder makes a lot of sense and can prevent the blurry reconstruction/samples of VAE. I specially like the hierarchical image generation experiments.

I have the following suggestions/concerns about the paper:

1) Is there any experiment showing that using the PixelCNN as the decoder of VAE will result in better disentangling of high-level factors of variations in the hidden code? For example, the authors can train a PixelVAE and VAE on MNIST with 2D hidden code and visualize the 2D hidden code for test images and color code each hidden code based on the digit and show that the digits have a better separation in the PixelVAE representation. A semi-supervised classification comparison between VAE and the PixelVAE will also significantly improve the quality of the paper.

2) A similar idea is also presented in a concurrent ICLR submission ""Variational Lossy Autoencoder"". It would be interesting to have a discussion included in the paper and compare these works.

3) The answer to the pre-review questions made the architecture details of the paper much more clear, but I still ask the authors to include the exact architecture details of all the experiments in the paper and/or open source the code. The clarity of the presentation is not satisfying and the experiments are difficult to reproduce.

4) As pointed out in my pre-review question, it would be great to include two sets of MNIST samples maybe in an appendix section. One with PixelCNN and the other with PixelVAE with the same pixelcnn depth to illustrate the hidden code in PixelVAE actually captures the global structure.

I will gladly raise the score if the authors address my concerns.",1
"UPDATE: The authors addressed all my concerns in the new version of the paper, so I raised my score and now recommend acceptance.
--------------
This paper combines the recent progress in variational autoencoder and autoregressive density modeling in the proposed PixelVAE model. The paper shows that it can match the NLL performance of a PixelCNN with a PixelVAE that has a much shallower PixelCNN decoder.
I think the idea of capturing the global structure with a VAE and modeling the local structure with a PixelCNN decoder makes a lot of sense and can prevent the blurry reconstruction/samples of VAE. I specially like the hierarchical image generation experiments.

I have the following suggestions/concerns about the paper:

1) Is there any experiment showing that using the PixelCNN as the decoder of VAE will result in better disentangling of high-level factors of variations in the hidden code? For example, the authors can train a PixelVAE and VAE on MNIST with 2D hidden code and visualize the 2D hidden code for test images and color code each hidden code based on the digit and show that the digits have a better separation in the PixelVAE representation. A semi-supervised classification comparison between VAE and the PixelVAE will also significantly improve the quality of the paper.

2) A similar idea is also presented in a concurrent ICLR submission ""Variational Lossy Autoencoder"". It would be interesting to have a discussion included in the paper and compare these works.

3) The answer to the pre-review questions made the architecture details of the paper much more clear, but I still ask the authors to include the exact architecture details of all the experiments in the paper and/or open source the code. The clarity of the presentation is not satisfying and the experiments are difficult to reproduce.

4) As pointed out in my pre-review question, it would be great to include two sets of MNIST samples maybe in an appendix section. One with PixelCNN and the other with PixelVAE with the same pixelcnn depth to illustrate the hidden code in PixelVAE actually captures the global structure.

I will gladly raise the score if the authors address my concerns.",1
"The work presented in this paper proposes a method to get an ensemble of neural networks at no extra training cost (i.e., at the cost of training a single network), by saving snapshots of the network during training. Network is trained using a cyclic (cosine) learning rate schedule; the snapshots are obtained when the learning rate is at the lowest points of the cycles. Using these snapshot ensembles, they show gains in performance over a single network on the image classification task on a variety of datasets.


Positives:

1. The work should be easy to adopt and re-produce, given the simple techinque and the experimental details in the paper.
2. Well written paper, with clear description of the method and thorough experiments.


Suggestions for improvement / other comments:

1. While it is fair to compare against other techniques assuming a fixed computational budget, for a clear perspective, thorough comaprisons with ""true ensembles"" (i.e., ensembles of networks trained independently) should be provided.
Specificially, Table 4 should be augmented with results from ""true ensembles"".

2. Comparison with true ensembles is only provided for DenseNet-40 on CIFAR100 in Figure 4. The proposed snapshot ensemble achieves approximately 66% of the improvement of ""true ensemble"" over the single baseline model. This is not reflected accurately in the authors' claim in the abstract: ""[snapshot ensembles] **almost match[es]** the results of far more expensive independently trained [true ensembles].""

3. As mentioned before: to understand the diversity of snapshot ensembles, it would help to the diversity against different ensembling technique, e.g. (1) ""true ensembles"", (2) ensembles from dropout as described by Gal et. al, 2016 (Dropout as a Bayesian Approximation).",1
"The work presented in this paper proposes a method to get an ensemble of neural networks at no extra training cost (i.e., at the cost of training a single network), by saving snapshots of the network during training. Network is trained using a cyclic (cosine) learning rate schedule; the snapshots are obtained when the learning rate is at the lowest points of the cycles. Using these snapshot ensembles, they show gains in performance over a single network on the image classification task on a variety of datasets.


Positives:

1. The work should be easy to adopt and re-produce, given the simple techinque and the experimental details in the paper.
2. Well written paper, with clear description of the method and thorough experiments.


Suggestions for improvement / other comments:

1. While it is fair to compare against other techniques assuming a fixed computational budget, for a clear perspective, thorough comaprisons with ""true ensembles"" (i.e., ensembles of networks trained independently) should be provided.
Specificially, Table 4 should be augmented with results from ""true ensembles"".

2. Comparison with true ensembles is only provided for DenseNet-40 on CIFAR100 in Figure 4. The proposed snapshot ensemble achieves approximately 66% of the improvement of ""true ensemble"" over the single baseline model. This is not reflected accurately in the authors' claim in the abstract: ""[snapshot ensembles] **almost match[es]** the results of far more expensive independently trained [true ensembles].""

3. As mentioned before: to understand the diversity of snapshot ensembles, it would help to the diversity against different ensembling technique, e.g. (1) ""true ensembles"", (2) ensembles from dropout as described by Gal et. al, 2016 (Dropout as a Bayesian Approximation).",1
"This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive “extensions” of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings — one of the main contributions is the so-called “Recursive-Reverse-Recursive Neural Net” which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).

There are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it’s unclear why the authors did not simply train on longer programs…  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt…).  Overall however, I would certainly like to see this paper accepted at ICLR.

Other miscellaneous comments:
* Too many e’s in the expansion probability expression — might be better just to write “Softmax”.
* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).
* The authors claim that using hyperbolic tangent activation functions is important — I’d be interested in some more discussion on this and why something like ReLU would not be good.
* It’s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible.
* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?)
* There is a missing related work by Piech et al (Learning Program Embeddings…) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).",1
"This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive “extensions” of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings — one of the main contributions is the so-called “Recursive-Reverse-Recursive Neural Net” which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).

There are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it’s unclear why the authors did not simply train on longer programs…  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt…).  Overall however, I would certainly like to see this paper accepted at ICLR.

Other miscellaneous comments:
* Too many e’s in the expansion probability expression — might be better just to write “Softmax”.
* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).
* The authors claim that using hyperbolic tangent activation functions is important — I’d be interested in some more discussion on this and why something like ReLU would not be good.
* It’s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible.
* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?)
* There is a missing related work by Piech et al (Learning Program Embeddings…) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).",1
"1) Summary

This paper investigates the usefulness of decoupling appearance and motion information for the problem of future frame prediction in natural videos. The method introduces a novel two-stream encoder-decoder architecture, MCNet, consisting of two separate encoders -- a convnet on single frames and a convnet+LSTM on sequences of temporal differences -- followed by combination layers (stacking + convolutions) and a deconvolutional network decoder leveraging also residual connections from the two encoders. The architecture is trained end-to-end using the objective and adversarial training strategy of Mathieu et al.

2) Contributions

+ The architecture seems novel and is well motivated. It is also somewhat related to the two-stream networks of Simonyan & Zisserman, which are very effective for real-world action recognition.
+ The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections.

3) Suggestions for improvement

Static dataset bias:
In response to the pre-review concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame. On the one hand, the updated experiments on KTH confirm the good results of the method in these conditions. On the other hand, the fact that this baseline is better than all other methods (not just the authors's) on UCF101 casts some doubts on whether reporting average statistics on UCF101 is insightful enough. Although the authors provide some qualitative analysis pertaining to the quantity of motion, further quantitative analysis seems necessary to validate the performance of this and other methods on future frame prediction. At least, the results on UCF101 should be disambiguated with respect to the type of scene, for instance by measuring the overall quantity of motion (e.g., l2 norm of time differences) and reporting PSNR and SSIM per quartile / decile. Ideally, other realistic datasets than UCF101 should be considered in complement. For instance, the Hollywood 2 dataset of Marszalek et al would be a good candidate, as it focuses on movies and often contains complex actor, camera, and background motions that would make the ""pixel-copying"" baseline very poor. Experiments on video datasets beyond actions, like the KITTI tracking benchmark, would also greatly improve the paper.

Additional recognition experiments:
As mentioned in pre-review questions, further UCF-101 experiments on action recognition tasks by fine-tuning would also greatly improve the paper. Classifying videos indeed requires learning both appearance and motion features, and the two-stream encoder + combination layers of the MCNet+Res architecture seem particularly adapted, if they indeed allowed for unsupervised pre-trainining of content and motion representations, as postulated by the authors. These experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations.

4) Conclusion

Overall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors. If the authors make the aforementioned additional experiments and the results are convincing, then this paper would be clearly relevant for ICLR.

5) Post-rebuttal final decision

The authors did a significant amount of additional work, following the suggestions made by the reviewers, and providing additional compelling experimental evidence. This makes this one of the most experimentally thorough ones for this problem. I, therefore, increase my rating, and suggest to accept this paper. Good job!",1
"1) Summary

This paper investigates the usefulness of decoupling appearance and motion information for the problem of future frame prediction in natural videos. The method introduces a novel two-stream encoder-decoder architecture, MCNet, consisting of two separate encoders -- a convnet on single frames and a convnet+LSTM on sequences of temporal differences -- followed by combination layers (stacking + convolutions) and a deconvolutional network decoder leveraging also residual connections from the two encoders. The architecture is trained end-to-end using the objective and adversarial training strategy of Mathieu et al.

2) Contributions

+ The architecture seems novel and is well motivated. It is also somewhat related to the two-stream networks of Simonyan & Zisserman, which are very effective for real-world action recognition.
+ The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections.

3) Suggestions for improvement

Static dataset bias:
In response to the pre-review concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame. On the one hand, the updated experiments on KTH confirm the good results of the method in these conditions. On the other hand, the fact that this baseline is better than all other methods (not just the authors's) on UCF101 casts some doubts on whether reporting average statistics on UCF101 is insightful enough. Although the authors provide some qualitative analysis pertaining to the quantity of motion, further quantitative analysis seems necessary to validate the performance of this and other methods on future frame prediction. At least, the results on UCF101 should be disambiguated with respect to the type of scene, for instance by measuring the overall quantity of motion (e.g., l2 norm of time differences) and reporting PSNR and SSIM per quartile / decile. Ideally, other realistic datasets than UCF101 should be considered in complement. For instance, the Hollywood 2 dataset of Marszalek et al would be a good candidate, as it focuses on movies and often contains complex actor, camera, and background motions that would make the ""pixel-copying"" baseline very poor. Experiments on video datasets beyond actions, like the KITTI tracking benchmark, would also greatly improve the paper.

Additional recognition experiments:
As mentioned in pre-review questions, further UCF-101 experiments on action recognition tasks by fine-tuning would also greatly improve the paper. Classifying videos indeed requires learning both appearance and motion features, and the two-stream encoder + combination layers of the MCNet+Res architecture seem particularly adapted, if they indeed allowed for unsupervised pre-trainining of content and motion representations, as postulated by the authors. These experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations.

4) Conclusion

Overall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors. If the authors make the aforementioned additional experiments and the results are convincing, then this paper would be clearly relevant for ICLR.

5) Post-rebuttal final decision

The authors did a significant amount of additional work, following the suggestions made by the reviewers, and providing additional compelling experimental evidence. This makes this one of the most experimentally thorough ones for this problem. I, therefore, increase my rating, and suggest to accept this paper. Good job!",1
"The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. 

Comments:

- It's not clear to me why this should be called a ""statistician"". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like ""statistic network"" and stuck to the more accurate ""approximate posterior"".

- The experiments are nice, and I appreciate the response to my question regarding ""one shot generation"". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: 

(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? 

(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one ""proper"" way of computing the ""one shot generation"" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that.",1
"The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. 

Comments:

- It's not clear to me why this should be called a ""statistician"". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like ""statistic network"" and stuck to the more accurate ""approximate posterior"".

- The experiments are nice, and I appreciate the response to my question regarding ""one shot generation"". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: 

(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? 

(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one ""proper"" way of computing the ""one shot generation"" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that.",1
"The paper proposes to study the problem of semi-supervised RL where one has to distinguish between labelled MDPs that provide rewards, and unlabelled MDPs that are not associated with any reward signal. The underlying is very simple since it aims at simultaneously learning a policy based on the REINFORCE+entropy regularization technique, and also a model of the reward that will be used (as in inverse reinforcement learning) as a feedback over unlabelled MDPs. The experiments are made on different continous domains and show interesting results

The paper is well written, and easy to understand. It is based on a simple but efficient idea of simultaneously learning the policy and a model of the reward and the resulting algorithm exhibit interesting properties. The proposed idea is quite obvious, but the authors are the first ones to propose to test such a model. The experiments could be made stronger by mixing continuous and discrete problems but are convincing.",1
"The paper proposes to study the problem of semi-supervised RL where one has to distinguish between labelled MDPs that provide rewards, and unlabelled MDPs that are not associated with any reward signal. The underlying is very simple since it aims at simultaneously learning a policy based on the REINFORCE+entropy regularization technique, and also a model of the reward that will be used (as in inverse reinforcement learning) as a feedback over unlabelled MDPs. The experiments are made on different continous domains and show interesting results

The paper is well written, and easy to understand. It is based on a simple but efficient idea of simultaneously learning the policy and a model of the reward and the resulting algorithm exhibit interesting properties. The proposed idea is quite obvious, but the authors are the first ones to propose to test such a model. The experiments could be made stronger by mixing continuous and discrete problems but are convincing.",1
"This paper proposes a new Bayesian neural network architecture for predicting the values of learning curves during the training of machine learning models. This is an exploratory paper, in that the ultimate goal is to use this method in a Bayesian optimization system, but for now the experiments are limited to assessing the quality of the predictions. This builds on previous work in Domhan, 2015, however in this work the model incorporates information from all tested hyperparameter settings rather than just extrapolating from a single learning curve. This paper also explores two MCMC methods for inference: SGLD and SGHMC, but I couldn’t tell if either of these were tested in Domhan, 2015 as well.

The performance seems overall positive, particularly in the initial phase of each curve where there is very little information. In this case, as expected, sharing knowledge across curves helps. One regime which did not seem to be tested, but might be very informative, is when some curves in the training set have been mostly, or fully observed. This might be a case where sharing information really helps.

Something that concerns me about this approach is the timing. The authors stated that to train the network takes about 20-60 seconds. In the worst case, with 100 epochs, this results in a little over 1.5 hours spent training the Bayesian network. This is a non-trivial fraction of the several hours it takes to train the model being tuned.

The Bayesian network makes many separate predictions, as shown in Figure 2. It would be interesting to see how accurate some of these individual pieces are. For example, did you bound the asymptotic value of the learning curve, since you mostly predicted accuracy? If not, did the value tend to lie in [0,1]?

Below are some minor questions/comments.

Figure 1 axes should read “validation accuracy”
Figure 6 can you describe LastSeenValue (although it seems self-explanatory, it’s good to be explicit) in the bottom left figure, and why isn’t it used anywhere else as a baseline?
Figure 7 and Table 1 are you predicting just the final value of the curves? Or every value along each curve, conditioned on the previous values?
Why do you only use 5 basis functions? Does this sufficiently capture all of the flexibility of these learning curves? Would more basis functions help or hurt?",1
"This paper proposes a new Bayesian neural network architecture for predicting the values of learning curves during the training of machine learning models. This is an exploratory paper, in that the ultimate goal is to use this method in a Bayesian optimization system, but for now the experiments are limited to assessing the quality of the predictions. This builds on previous work in Domhan, 2015, however in this work the model incorporates information from all tested hyperparameter settings rather than just extrapolating from a single learning curve. This paper also explores two MCMC methods for inference: SGLD and SGHMC, but I couldn’t tell if either of these were tested in Domhan, 2015 as well.

The performance seems overall positive, particularly in the initial phase of each curve where there is very little information. In this case, as expected, sharing knowledge across curves helps. One regime which did not seem to be tested, but might be very informative, is when some curves in the training set have been mostly, or fully observed. This might be a case where sharing information really helps.

Something that concerns me about this approach is the timing. The authors stated that to train the network takes about 20-60 seconds. In the worst case, with 100 epochs, this results in a little over 1.5 hours spent training the Bayesian network. This is a non-trivial fraction of the several hours it takes to train the model being tuned.

The Bayesian network makes many separate predictions, as shown in Figure 2. It would be interesting to see how accurate some of these individual pieces are. For example, did you bound the asymptotic value of the learning curve, since you mostly predicted accuracy? If not, did the value tend to lie in [0,1]?

Below are some minor questions/comments.

Figure 1 axes should read “validation accuracy”
Figure 6 can you describe LastSeenValue (although it seems self-explanatory, it’s good to be explicit) in the bottom left figure, and why isn’t it used anywhere else as a baseline?
Figure 7 and Table 1 are you predicting just the final value of the curves? Or every value along each curve, conditioned on the previous values?
Why do you only use 5 basis functions? Does this sufficiently capture all of the flexibility of these learning curves? Would more basis functions help or hurt?",1
"The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.

This work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance.

Detail: 
- The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections.
- If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).
- Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?
- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.
- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.",1
"The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.

This work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance.

Detail: 
- The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections.
- If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).
- Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?
- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.
- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.",1
"The paper shows promising results but it is difficult to read and follow. It presents different things closely related and it is difficult to asses the performance of each one. Diversity, sparsity, regularization term, tying weights. Anyway results are good.",1
"The paper shows promising results but it is difficult to read and follow. It presents different things closely related and it is difficult to asses the performance of each one. Diversity, sparsity, regularization term, tying weights. Anyway results are good.",1
"This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called “amortized inference” can be much faster than normal inference where inference must be run iteratively for every document. Some comments:
Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?
The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?
The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof:",1
"This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called “amortized inference” can be much faster than normal inference where inference must be run iteratively for every document. Some comments:
Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?
The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?
The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof:",1
"The paper propose to find an optimal decoder for binary data using a min-max decoder on the binary hypercube given a linear constraint on the correlation between the encoder and the  data. 
The paper gives finally that the optimal decoder as logistic of the lagragian W multiplying the encoding e.
 
Given the weights of the ‘min-max’decoder W the paper finds the best encoding for the data distribution considered, by minimizing that error as a function of the encoding.

The paper then alternates that optimization between the encoding and the min-max decoding, starting from random weights W.


clarity:

-The paper would be easier to follow if the real data (x in section 3 ) is differentiated from the worst case data played by the model (x in section 2). 


significance

Overall I like the paper, however I have some doubts on what the alternating optimization optimum ends up being.  The paper ends up implementing a single layer network. The correlation constraints while convenient in the derivation, is  a bit intriguing. Since linear relation between the encoding and the data  seems to be weak modeling constraint and might be not different from what PCA would implement.

- what is the performance of PCA on those tasks? one could you use a simple sign function to decode. This is related to one bit compressive sensing.

- what happens if you initialize W in algorithm one with PCA weights? or weighted pca weights?

- Have you tried on more complex datasets such as cifar?",1
"The paper propose to find an optimal decoder for binary data using a min-max decoder on the binary hypercube given a linear constraint on the correlation between the encoder and the  data. 
The paper gives finally that the optimal decoder as logistic of the lagragian W multiplying the encoding e.
 
Given the weights of the ‘min-max’decoder W the paper finds the best encoding for the data distribution considered, by minimizing that error as a function of the encoding.

The paper then alternates that optimization between the encoding and the min-max decoding, starting from random weights W.


clarity:

-The paper would be easier to follow if the real data (x in section 3 ) is differentiated from the worst case data played by the model (x in section 2). 


significance

Overall I like the paper, however I have some doubts on what the alternating optimization optimum ends up being.  The paper ends up implementing a single layer network. The correlation constraints while convenient in the derivation, is  a bit intriguing. Since linear relation between the encoding and the data  seems to be weak modeling constraint and might be not different from what PCA would implement.

- what is the performance of PCA on those tasks? one could you use a simple sign function to decode. This is related to one bit compressive sensing.

- what happens if you initialize W in algorithm one with PCA weights? or weighted pca weights?

- Have you tried on more complex datasets such as cifar?",1
"The paper describes a method to evaluate generative models such as VAE, GAN and GMMN. This is very much needed in our community where we still eyeball generated images to judge the quality of a model. However, the technical increment over the NIPS 16 paper: “Measuring the reliability of MCMC inference with bidirectional Monte Carlo” is very small, or nonexistent (but please correct me if I am wrong!).  (Grosse et al). The relative contribution of this paper is the application of this method to generative models. 
In section 2.3 the authors seem to make a mistake. They write E[p’(x)] <= p(x) but I think they mean: E[log p’(x)] <= log E[p’(x)] = log p(x). Also,  for what value of x? If p(x) is normalized it can’t be true for all values of x. Anyways, I think there are typos here and there and the equations could be more precise.
On page 5 top of the page it is said that the AIS procedure can be initialized with q(z|x) instead of p(z). However, it is unclear what value of x is then picked? Is it perhaps Ep(x)[q(z|x)] ?
I am confused with the use of the term overfitting (p8 bottom). Does a model A overfit relative to a another model B if the test accuracy of A is higher than that of B even though the gap between train and test accuracy is also higher for B than for A. I think not. Perhaps the last sentence on page 8 should say that VAE-50 underfits less than GMMN-50?
The experimental results are interesting in that it exposes the fact that GANs and GMMNs seem to have much lover test accuracy than VAE despite the fact that their samples look great.",1
"The paper describes a method to evaluate generative models such as VAE, GAN and GMMN. This is very much needed in our community where we still eyeball generated images to judge the quality of a model. However, the technical increment over the NIPS 16 paper: “Measuring the reliability of MCMC inference with bidirectional Monte Carlo” is very small, or nonexistent (but please correct me if I am wrong!).  (Grosse et al). The relative contribution of this paper is the application of this method to generative models. 
In section 2.3 the authors seem to make a mistake. They write E[p’(x)] <= p(x) but I think they mean: E[log p’(x)] <= log E[p’(x)] = log p(x). Also,  for what value of x? If p(x) is normalized it can’t be true for all values of x. Anyways, I think there are typos here and there and the equations could be more precise.
On page 5 top of the page it is said that the AIS procedure can be initialized with q(z|x) instead of p(z). However, it is unclear what value of x is then picked? Is it perhaps Ep(x)[q(z|x)] ?
I am confused with the use of the term overfitting (p8 bottom). Does a model A overfit relative to a another model B if the test accuracy of A is higher than that of B even though the gap between train and test accuracy is also higher for B than for A. I think not. Perhaps the last sentence on page 8 should say that VAE-50 underfits less than GMMN-50?
The experimental results are interesting in that it exposes the fact that GANs and GMMNs seem to have much lover test accuracy than VAE despite the fact that their samples look great.",1
"- summary

The paper proposes a differntiable Neural Physics Engine (NPE). The NPE consists of an encoder and a decoder function. The NPE takes as input the state of pairs of objects (within a neighbourhood of a focus object) at two previous time-steps in a scene. The encoder function summarizes the interaction of each pair of objects. The decoder then outputs the change in velocity of the focus object at the next time step. The NPE is evaluated on various environments containing bouncing balls.

- novelty

The differentiable NPE is a novel concept. However, concurrently Battaglia et al. (NIPS 2016) proposes a very similar model. Just as this work, Battaglia et al. (NIPS 2016) consider a model which consists of a encoder function (relation-centric) which encodes the interaction among a focus object and other objects in the scene and a decoder (relation-centric) function which considers the cumulative (encoded) effect of object interactions on the focus object and predicts effect of the interactions.  Aspects like only considering objects interactions within a neighbourhood (versus the complete object interaction graph in Battaglia et al.) based on euclideian distance  are novel to this work. However, the advantages (if any) of NPE versus the model of Battaglia et al. are not clear. Moreover, it is not clear how this neighbourhood thresholding scene would preform in case of n-ball systems, where gravitational forces of massive objects can be felt over large distances.

- citations 

This work includes all relevant citations.

- clarity

The article is well written and easy to understand.

- experiments 

Battaglia et al. evaluates on wider variety senerios compared to this work (e.g. n-bodies under gravitation, falling strings). Such experiments demonstrate the ability of the models to generalize. However, this work does include more in-depth experiments in case of bouncing balls compared to Battaglia et al. (e.g. mass estimation and varying world configurations with obstacles in the bouncing balls senerio). 

Moreover, an extensive comparison to Fragkiadaki et al. (2015) (in the bouncing balls senerios) is missing. The authors (referring to answer to question 4) do point out to comaprable numbers in both works, but the experimental settings are different.  Comparison in a billiard table senerio like that Fragkiadaki et al. (2015) where a initial force is applied to a ball, would have been enlightening. 

The authors only evaluate the error in velocity in the bouncing balls senerios. We understand that this model predicts only the velocity (refer to answer of question 2). Error analysis also with respect to ground truth ball position would be more enlightening. As small errors in velocity can quickly lead to entirely different scene configuration.

- conclusion / recommendation

The main issue with this work is the unclear novelty with respect to work of Battaglia et al. at NIPS'16. A quantitative and qualitative comparison with Battaglia et al. is lacking.  But the authors state that their work was developed independently.

Differentiable physics engines like NPE or that of Battaglia et al. (NIPS 2016) requires generation of an extensive amount of synthetic data to learn about the physics of a certain senerio. Moreover, extensive retraining is required to adapt to new sceneries (e.g. bouncing balls to n-body systems). Any practical advantage versus generating new code for a physics engine is not clear. Other ""bottom-up"" approaches like that of  Fragkiadaki et al. (2015) couple vision along with learning dynamics. However, they require very few input parameters (position, mass, current velocity, world configuration), as approximate parameter estimation can be done from the visual component.  Such approaches could be potentially more useful of a robot in ""common-sense"" everyday tasks (e.g. manipulation). Thus, overall potential applications of a differentiable physics engine like NPE is unclear.",1
"- summary

The paper proposes a differntiable Neural Physics Engine (NPE). The NPE consists of an encoder and a decoder function. The NPE takes as input the state of pairs of objects (within a neighbourhood of a focus object) at two previous time-steps in a scene. The encoder function summarizes the interaction of each pair of objects. The decoder then outputs the change in velocity of the focus object at the next time step. The NPE is evaluated on various environments containing bouncing balls.

- novelty

The differentiable NPE is a novel concept. However, concurrently Battaglia et al. (NIPS 2016) proposes a very similar model. Just as this work, Battaglia et al. (NIPS 2016) consider a model which consists of a encoder function (relation-centric) which encodes the interaction among a focus object and other objects in the scene and a decoder (relation-centric) function which considers the cumulative (encoded) effect of object interactions on the focus object and predicts effect of the interactions.  Aspects like only considering objects interactions within a neighbourhood (versus the complete object interaction graph in Battaglia et al.) based on euclideian distance  are novel to this work. However, the advantages (if any) of NPE versus the model of Battaglia et al. are not clear. Moreover, it is not clear how this neighbourhood thresholding scene would preform in case of n-ball systems, where gravitational forces of massive objects can be felt over large distances.

- citations 

This work includes all relevant citations.

- clarity

The article is well written and easy to understand.

- experiments 

Battaglia et al. evaluates on wider variety senerios compared to this work (e.g. n-bodies under gravitation, falling strings). Such experiments demonstrate the ability of the models to generalize. However, this work does include more in-depth experiments in case of bouncing balls compared to Battaglia et al. (e.g. mass estimation and varying world configurations with obstacles in the bouncing balls senerio). 

Moreover, an extensive comparison to Fragkiadaki et al. (2015) (in the bouncing balls senerios) is missing. The authors (referring to answer to question 4) do point out to comaprable numbers in both works, but the experimental settings are different.  Comparison in a billiard table senerio like that Fragkiadaki et al. (2015) where a initial force is applied to a ball, would have been enlightening. 

The authors only evaluate the error in velocity in the bouncing balls senerios. We understand that this model predicts only the velocity (refer to answer of question 2). Error analysis also with respect to ground truth ball position would be more enlightening. As small errors in velocity can quickly lead to entirely different scene configuration.

- conclusion / recommendation

The main issue with this work is the unclear novelty with respect to work of Battaglia et al. at NIPS'16. A quantitative and qualitative comparison with Battaglia et al. is lacking.  But the authors state that their work was developed independently.

Differentiable physics engines like NPE or that of Battaglia et al. (NIPS 2016) requires generation of an extensive amount of synthetic data to learn about the physics of a certain senerio. Moreover, extensive retraining is required to adapt to new sceneries (e.g. bouncing balls to n-body systems). Any practical advantage versus generating new code for a physics engine is not clear. Other ""bottom-up"" approaches like that of  Fragkiadaki et al. (2015) couple vision along with learning dynamics. However, they require very few input parameters (position, mass, current velocity, world configuration), as approximate parameter estimation can be done from the visual component.  Such approaches could be potentially more useful of a robot in ""common-sense"" everyday tasks (e.g. manipulation). Thus, overall potential applications of a differentiable physics engine like NPE is unclear.",1
"The paper proposes a new memory module to be used as an addition to existing neural network models.

Pros:
* Clearly written and original idea.
* Useful memory module, shows nice improvements.
* Tested on some big tasks.

Cons:
* No comparisons to other memory modules such as associative LSTMs etc.",1
"The paper proposes a new memory module to be used as an addition to existing neural network models.

Pros:
* Clearly written and original idea.
* Useful memory module, shows nice improvements.
* Tested on some big tasks.

Cons:
* No comparisons to other memory modules such as associative LSTMs etc.",1
"This paper presents a clear hierarchical taxonomy of transfer learning methods as applicable to sequence tagging problems. This contextualizes and unifies previous work on specific instances of this taxonomy. Moreover, the paper shows that previously unexplored places in this taxonomy are competitive with or superior to the state of the art in key benchmark problems.

It'd be nice to see this explored further, such as highlighting what is the loss as you move from the more restrictive to the less restrictive transfer learning approaches, but I believe this paper is interesting and acceptable as-is.",1
"This paper presents a clear hierarchical taxonomy of transfer learning methods as applicable to sequence tagging problems. This contextualizes and unifies previous work on specific instances of this taxonomy. Moreover, the paper shows that previously unexplored places in this taxonomy are competitive with or superior to the state of the art in key benchmark problems.

It'd be nice to see this explored further, such as highlighting what is the loss as you move from the more restrictive to the less restrictive transfer learning approaches, but I believe this paper is interesting and acceptable as-is.",1
"This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too.

Overall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?",1
"This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too.

Overall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?",1
"The authors investigate a variety of existing and two new RNN architectures to obtain more insight about the effectiveness at which these models can store task information in their parameters and activations.

The experimental setups look sound. To generalize comparisons between different architectures it’s necessary to consider multiple tasks and control for the effect of the hyperparameters. This work uses multiple tasks of varying complexities, principled hyperparameter tuning methodology and a number of tuning iterations that can currently only be achieved by the computational resources of some of the larger industrial research groups. 

The descriptions of the models and the objective where very clear to me. The descriptions of the experiments and presentation of the results were not always clear to me at times, even with the additional details in the appendix available. Most of these issues can easily be resolved by editing the text. For example, in the memory task the scaling of the inputs (and hence also outputs) is not provided so it’s hard to interpret the squared error scores in Figure 2c. It’s not clear to me what the term ‘unrollings’ refers to in Figure 2b. Is this a time lag with additional hidden state updates between the presentation of the input sequence and the generation of the output? Since the perceptron capacity task is somewhat central to the paper, I think a slightly more precise description of how and when the predictions are computed would be helpful. Due to the large number of graphs, it can be somewhat hard to find the most relevant results. Perhaps some of the more obvious findings (like Figure 1(b-d) given Figure 1a) could move to the appendix to make space for more detailed task descriptions.

Novelty is not really the aim of this paper since it mostly investigates existing architectures. To use the mutual information to obtain bits per parameter scores in highly non-linear parameterized functions is new to me. The paper also proposed to new architectures that seem to have practical value. The paper adds to the currently still somewhat neglected research effort to employ the larger computational resources we currently have towards a better understanding of architectures which were designed when such resources were not yet available. I’d argue that the paper is original enough for that reason alone.

The paper provides some interesting new insights into the properties of RNNs. While observed before, it is interesting to see the importance of gated units for maintaining trainability of networks with many layers. It is also interesting to see a potential new use for vanilla RNNs for simpler tasks where a high capacity per parameter may be required due to hardware constraints. The proposed +RNN may turn out to have practical value as well and the hyperparameter robustness results shed some light on the popularity of certain architectures when limited time for HP tuning is available. The large body of results and hyperparameter analysis should be useful to many researchers who want to use RNNs in the future. All in all, I think this paper would make a valuable addition to the ICLR conference but would benefit from some improvements to the text.

Pros:
* Thorough analysis.
* Seemingly proper experiments.
* The way of quantifying capacity in neural networks adds to the novelty of the paper.
* The results have some practical value and suggest similar analysis of other architectures.
* The results provide useful insights into the relative merits of different RNN architectures.

Cons:
* It’s hard to isolate the most important findings (some plots seem redundant).
* Some relevant experimental details are missing.",1
"The authors investigate a variety of existing and two new RNN architectures to obtain more insight about the effectiveness at which these models can store task information in their parameters and activations.

The experimental setups look sound. To generalize comparisons between different architectures it’s necessary to consider multiple tasks and control for the effect of the hyperparameters. This work uses multiple tasks of varying complexities, principled hyperparameter tuning methodology and a number of tuning iterations that can currently only be achieved by the computational resources of some of the larger industrial research groups. 

The descriptions of the models and the objective where very clear to me. The descriptions of the experiments and presentation of the results were not always clear to me at times, even with the additional details in the appendix available. Most of these issues can easily be resolved by editing the text. For example, in the memory task the scaling of the inputs (and hence also outputs) is not provided so it’s hard to interpret the squared error scores in Figure 2c. It’s not clear to me what the term ‘unrollings’ refers to in Figure 2b. Is this a time lag with additional hidden state updates between the presentation of the input sequence and the generation of the output? Since the perceptron capacity task is somewhat central to the paper, I think a slightly more precise description of how and when the predictions are computed would be helpful. Due to the large number of graphs, it can be somewhat hard to find the most relevant results. Perhaps some of the more obvious findings (like Figure 1(b-d) given Figure 1a) could move to the appendix to make space for more detailed task descriptions.

Novelty is not really the aim of this paper since it mostly investigates existing architectures. To use the mutual information to obtain bits per parameter scores in highly non-linear parameterized functions is new to me. The paper also proposed to new architectures that seem to have practical value. The paper adds to the currently still somewhat neglected research effort to employ the larger computational resources we currently have towards a better understanding of architectures which were designed when such resources were not yet available. I’d argue that the paper is original enough for that reason alone.

The paper provides some interesting new insights into the properties of RNNs. While observed before, it is interesting to see the importance of gated units for maintaining trainability of networks with many layers. It is also interesting to see a potential new use for vanilla RNNs for simpler tasks where a high capacity per parameter may be required due to hardware constraints. The proposed +RNN may turn out to have practical value as well and the hyperparameter robustness results shed some light on the popularity of certain architectures when limited time for HP tuning is available. The large body of results and hyperparameter analysis should be useful to many researchers who want to use RNNs in the future. All in all, I think this paper would make a valuable addition to the ICLR conference but would benefit from some improvements to the text.

Pros:
* Thorough analysis.
* Seemingly proper experiments.
* The way of quantifying capacity in neural networks adds to the novelty of the paper.
* The results have some practical value and suggest similar analysis of other architectures.
* The results provide useful insights into the relative merits of different RNN architectures.

Cons:
* It’s hard to isolate the most important findings (some plots seem redundant).
* Some relevant experimental details are missing.",1
"The paper describes a novel technique to improve the efficiency of computation graphs in deep learning frameworks. An impressive speedup can be observed in their implementation within TensorFlow. The content is presented with sufficient clarity, although some more graphical illustrations could be useful. This work is relevant in order to achieve highest performance in neural network training.


Pros:

- significant speed improvements through dynamic batching
- source code provided


Cons:

- the effect on a large real-world (ASR, SMT) would allow the reader to put the improvements better into context
- presentation/vizualisation can be improved",1
"The paper describes a novel technique to improve the efficiency of computation graphs in deep learning frameworks. An impressive speedup can be observed in their implementation within TensorFlow. The content is presented with sufficient clarity, although some more graphical illustrations could be useful. This work is relevant in order to achieve highest performance in neural network training.


Pros:

- significant speed improvements through dynamic batching
- source code provided


Cons:

- the effect on a large real-world (ASR, SMT) would allow the reader to put the improvements better into context
- presentation/vizualisation can be improved",1
"The submission explores several alternatives to provide the generator function in generative adversarial training with additional gradient information. The exposition starts by describing a general formulation about how this additional gradient information (termed K(p_gen) could be added to the generative adversarial training objective function (Equation 1). Next, the authors prove that the shape of the optimal discriminator does indeed depend on the added gradient information (Proposition 3.1), which is unsurprising. Finally, the authors propose three particular alternatives to construct K(p_gen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function (which resembles the EBGAN objective of Zhao et al, 2016).

The exposition moves then to an experimental evaluation of the method, which sets K(p_gen) to be the approximate entropy of the generator distribution. At this point, my intuition is that the objective function under study is the vanilla GAN objective, plus a regularization term that encourages diversity (high entropy) in the generator distribution. The hope of the authors is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution.

The experimental evaluation proceeds by 1) showing the contour plots of the obtained generator distribution for a 2D problem, 2) studying the generation diversity in MNIST digits, and 3) showing some samples for CIFAR-10 and CelebA. The 2D problem results are convincing, since one can clearly observe that the discriminator scores translate into unnormalized values of the density function. The MNIST results offer good intuition also: the more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, and the less prototypical digits are assigned smaller scores. The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison.

To this end, I would recommend the authors to clarify three aspects. First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution. But, how does this regularization reshape the generator function? It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible. Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the art? Third, what are the *shortcomings* of this method versus vanilla GAN? Too much computational overhead? What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript?

Overall, a clearly written paper. I vote for acceptance.

As an open question to the authors: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?",1
"The submission explores several alternatives to provide the generator function in generative adversarial training with additional gradient information. The exposition starts by describing a general formulation about how this additional gradient information (termed K(p_gen) could be added to the generative adversarial training objective function (Equation 1). Next, the authors prove that the shape of the optimal discriminator does indeed depend on the added gradient information (Proposition 3.1), which is unsurprising. Finally, the authors propose three particular alternatives to construct K(p_gen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function (which resembles the EBGAN objective of Zhao et al, 2016).

The exposition moves then to an experimental evaluation of the method, which sets K(p_gen) to be the approximate entropy of the generator distribution. At this point, my intuition is that the objective function under study is the vanilla GAN objective, plus a regularization term that encourages diversity (high entropy) in the generator distribution. The hope of the authors is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution.

The experimental evaluation proceeds by 1) showing the contour plots of the obtained generator distribution for a 2D problem, 2) studying the generation diversity in MNIST digits, and 3) showing some samples for CIFAR-10 and CelebA. The 2D problem results are convincing, since one can clearly observe that the discriminator scores translate into unnormalized values of the density function. The MNIST results offer good intuition also: the more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, and the less prototypical digits are assigned smaller scores. The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison.

To this end, I would recommend the authors to clarify three aspects. First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution. But, how does this regularization reshape the generator function? It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible. Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the art? Third, what are the *shortcomings* of this method versus vanilla GAN? Too much computational overhead? What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript?

Overall, a clearly written paper. I vote for acceptance.

As an open question to the authors: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?",1
Authors propose a neural pruning technique starting from trained models using an approximation of change in the cost function and outperform other criteria. Authors obtain solid speedups while maintaining reasonable accuracy thanks to finetuning after pruning. Comparisons to existing methods is weak as GFLOPS graphs only show a couple simple baselines and no prior work baselines. I would be more convinced of the superiority of the approach with such comparison.,1
Authors propose a neural pruning technique starting from trained models using an approximation of change in the cost function and outperform other criteria. Authors obtain solid speedups while maintaining reasonable accuracy thanks to finetuning after pruning. Comparisons to existing methods is weak as GFLOPS graphs only show a couple simple baselines and no prior work baselines. I would be more convinced of the superiority of the approach with such comparison.,1
"SUMMARY.
This paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage.
The proposed model combines two well-know neural network architectures match-lstm and pointer nets.
First the passage and the questions are encoded with a unidirectional LSTM.
Then the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question.
For each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm.
The same process is done in the opposite direction with a backward lstm.
The final representation is a concatenation of the two lstms.
As a decoded a pointer network is used.
The authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer.

The proposed model is tested on the Stanford Question Answering Dataset.
An ensemble of the proposed model achieves performance close to state-of-the-art models.


----------

OVERALL JUDGMENT

I think the model is interesting mainly because of the use of pointer networks as a decoder.
One thing that the authors could have tried is a multi-hop approach. It has been shown in many works to be extremely beneficial in the joint encoding of passage and query. The authors can think of it as a deep match-lstm.
The analysis of the model is interesting and insightful.
The sharing of the code is good.",1
"SUMMARY.
This paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage.
The proposed model combines two well-know neural network architectures match-lstm and pointer nets.
First the passage and the questions are encoded with a unidirectional LSTM.
Then the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question.
For each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm.
The same process is done in the opposite direction with a backward lstm.
The final representation is a concatenation of the two lstms.
As a decoded a pointer network is used.
The authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer.

The proposed model is tested on the Stanford Question Answering Dataset.
An ensemble of the proposed model achieves performance close to state-of-the-art models.


----------

OVERALL JUDGMENT

I think the model is interesting mainly because of the use of pointer networks as a decoder.
One thing that the authors could have tried is a multi-hop approach. It has been shown in many works to be extremely beneficial in the joint encoding of passage and query. The authors can think of it as a deep match-lstm.
The analysis of the model is interesting and insightful.
The sharing of the code is good.",1
"The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al. that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps. 
The paper claims that this 
a) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method
b) improves performance on texture inpainting tasks compared to the Gatys et al. method
c) improves results in season transfer when combined with the style transfer method by Gatys et al. 
Furthermore the paper shows that
d) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved.

I agree with claim a). However, the generated textures still have some issues such as greyish regions so the problem is not solved. Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower. For example, in comparison, the concurrent work by Liu et al. (",1
"The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al. that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps. 
The paper claims that this 
a) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method
b) improves performance on texture inpainting tasks compared to the Gatys et al. method
c) improves results in season transfer when combined with the style transfer method by Gatys et al. 
Furthermore the paper shows that
d) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved.

I agree with claim a). However, the generated textures still have some issues such as greyish regions so the problem is not solved. Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower. For example, in comparison, the concurrent work by Liu et al. (",1
"This paper proposed a dynamic coattention network for the question answering task with long contextual documents. 
The model is able to encode co-dependent representations of the question and the document, and a dynamic decoder iteratively pointing the potential answer spans to locate the final answer. 

Overall, this is a well-written paper. 
Although the model is a bit complicated (coattention encoder, iterative dynamic pointering decoder and highway maxout network), the intuitions behind and the details of the model are clearly presented. 
Also the performance on the SQuAD dataset is good. 
I would recommend this paper to be accepted.",1
"This paper proposed a dynamic coattention network for the question answering task with long contextual documents. 
The model is able to encode co-dependent representations of the question and the document, and a dynamic decoder iteratively pointing the potential answer spans to locate the final answer. 

Overall, this is a well-written paper. 
Although the model is a bit complicated (coattention encoder, iterative dynamic pointering decoder and highway maxout network), the intuitions behind and the details of the model are clearly presented. 
Also the performance on the SQuAD dataset is good. 
I would recommend this paper to be accepted.",1
"The paper introduces SampleRNN, a hierarchical recurrent neural network model of raw audio. The model is trained end-to-end and evaluated using log-likelihood and by human judgement of unconditional samples, on three different datasets covering speech and music. This evaluation shows the proposed model to compare favourably to the baselines.

It is shown that the subsequence length used for truncated BPTT affects performance significantly, but interestingly, a subsequence length of 512 samples (~32 ms) is sufficient to get good results, even though the features of the data that are modelled span much longer timescales. This is an interesting and somewhat unintuitive result that I think warrants a bit more discussion.

The authors have attempted to reimplement WaveNet, an alternative model of raw audio that is fully convolutional. They were unable to reproduce the exact model architecture from the original paper, but have attempted to build an instance of the model with a receptive field of about 250ms that could be trained in a reasonable time using their computational resources, which is commendable.

The architecture of the Wavenet model is described in detail, but it found it challenging to find the same details for the proposed SampleRNN architecture (e.g. which value of ""r"" is used for the different tiers, how many units per layer, ...). I think a comparison in terms of computational cost, training time and number of parameters would also be very informative.

Surprisingly, Table 1 shows a vanilla RNN (LSTM) substantially outperforming this model in terms of likelihood, which is quite suspicious as LSTMs tend to have effective receptive fields of a few hundred timesteps at best. One would expect the much larger receptive field of the Wavenet model to be reflected in the likelihood scores to some extent. Similarly, Figure 3 shows the vanilla RNN outperforming the Wavenet reimplementation in human evaluation on the Blizzard dataset. This raises questions about the implementation of the latter. Some discussion about this result and whether the authors expected it or not would be very welcome.

Table 1 and Figure 4 also show the 2-tier SampleRNN outperforming the 3-tier model in terms of likelihood and human rating respectively, which is very counterintuitive as one would expect longer-range temporal correlations to be even more relevant for music than for speech. This is not discussed at all, I think it would be useful to comment on why this could be happening.

Overall, this an interesting attempt to tackle modelling very long sequences with long-range temporal correlations and the results are quite convincing, even if the same can't always be said of the comparison with the baselines. It would be interesting to see how the model performs for conditional generation, seeing as it can be more easily be objectively compared to models like Wavenet in that domain.



Other remarks:

- upsampling the output of the models is done with r separate linear projections. This choice of upsampling method is not motivated. Why not just use linear interpolation or nearest neighbour upsampling? What is the advantage of learning this operation? Don't the r linear projections end up learning largely the same thing, give or take some noise?

- The third paragraph of Section 2.1.1 indicates that 8-bit linear PCM was used. This is in contrast to Wavenet, for which an 8-bit mu-law encoding was used, and this supposedly improves the audio fidelity of the samples. Did you try this as well?

- Section 2.1 mentions the discretisation of the input and the use of a softmax to model this discretised input, without any reference to prior work that made the same observation. A reference is given in 2.1.1, but it should probably be moved up a bit to avoid giving the impression that this is a novel observation.",1
"The paper introduces SampleRNN, a hierarchical recurrent neural network model of raw audio. The model is trained end-to-end and evaluated using log-likelihood and by human judgement of unconditional samples, on three different datasets covering speech and music. This evaluation shows the proposed model to compare favourably to the baselines.

It is shown that the subsequence length used for truncated BPTT affects performance significantly, but interestingly, a subsequence length of 512 samples (~32 ms) is sufficient to get good results, even though the features of the data that are modelled span much longer timescales. This is an interesting and somewhat unintuitive result that I think warrants a bit more discussion.

The authors have attempted to reimplement WaveNet, an alternative model of raw audio that is fully convolutional. They were unable to reproduce the exact model architecture from the original paper, but have attempted to build an instance of the model with a receptive field of about 250ms that could be trained in a reasonable time using their computational resources, which is commendable.

The architecture of the Wavenet model is described in detail, but it found it challenging to find the same details for the proposed SampleRNN architecture (e.g. which value of ""r"" is used for the different tiers, how many units per layer, ...). I think a comparison in terms of computational cost, training time and number of parameters would also be very informative.

Surprisingly, Table 1 shows a vanilla RNN (LSTM) substantially outperforming this model in terms of likelihood, which is quite suspicious as LSTMs tend to have effective receptive fields of a few hundred timesteps at best. One would expect the much larger receptive field of the Wavenet model to be reflected in the likelihood scores to some extent. Similarly, Figure 3 shows the vanilla RNN outperforming the Wavenet reimplementation in human evaluation on the Blizzard dataset. This raises questions about the implementation of the latter. Some discussion about this result and whether the authors expected it or not would be very welcome.

Table 1 and Figure 4 also show the 2-tier SampleRNN outperforming the 3-tier model in terms of likelihood and human rating respectively, which is very counterintuitive as one would expect longer-range temporal correlations to be even more relevant for music than for speech. This is not discussed at all, I think it would be useful to comment on why this could be happening.

Overall, this an interesting attempt to tackle modelling very long sequences with long-range temporal correlations and the results are quite convincing, even if the same can't always be said of the comparison with the baselines. It would be interesting to see how the model performs for conditional generation, seeing as it can be more easily be objectively compared to models like Wavenet in that domain.



Other remarks:

- upsampling the output of the models is done with r separate linear projections. This choice of upsampling method is not motivated. Why not just use linear interpolation or nearest neighbour upsampling? What is the advantage of learning this operation? Don't the r linear projections end up learning largely the same thing, give or take some noise?

- The third paragraph of Section 2.1.1 indicates that 8-bit linear PCM was used. This is in contrast to Wavenet, for which an 8-bit mu-law encoding was used, and this supposedly improves the audio fidelity of the samples. Did you try this as well?

- Section 2.1 mentions the discretisation of the input and the use of a softmax to model this discretised input, without any reference to prior work that made the same observation. A reference is given in 2.1.1, but it should probably be moved up a bit to avoid giving the impression that this is a novel observation.",1
"Thank you for an interesting read on an approach to choose computational models based on kind of examples given.

Pros
- As an idea, using a meta controller to decide the computational model and the number of steps to reach the conclusion is keeping in line with solving an important practical issue of increased computational times of a simple example. 

- The approach seems similar to an ensemble learning construct. But instead of random experts and a fixed computational complexity during testing time the architecture is designed to estimate hyper-parameters like number of ponder steps which gives this approach a distinct advantage.


Cons
- Even though the metacontroller is designed to choose the best amongst the given experts, its complete capability has not been explored yet. It would be interesting to see the architecture handle more than 2 experts.",1
"Thank you for an interesting read on an approach to choose computational models based on kind of examples given.

Pros
- As an idea, using a meta controller to decide the computational model and the number of steps to reach the conclusion is keeping in line with solving an important practical issue of increased computational times of a simple example. 

- The approach seems similar to an ensemble learning construct. But instead of random experts and a fixed computational complexity during testing time the architecture is designed to estimate hyper-parameters like number of ponder steps which gives this approach a distinct advantage.


Cons
- Even though the metacontroller is designed to choose the best amongst the given experts, its complete capability has not been explored yet. It would be interesting to see the architecture handle more than 2 experts.",1
"The paper proposes a neural approach to learning an image compression-decompression scheme as an auto-encoder. While the idea is certainly interesting and well-motivated, in practice, it turns out to achieve effectively identical rates to JPEG-2000.

Now, as the authors argue, there is some value to the fact that this scheme was learned automatically rather than by expert design---which means it has benefits beyond the compression of natural images (e.g., it could be used to automatically learning a compression scheme for signals for which we don't have as much domain knowledge). However, I still believe that this makes the paper unsuitable for publication in its current form because of the following reasons---

1. Firstly, the fact that the learned encoder is competitive---and not clearly better---than JPEG 2000 means that the focus of the paper should more be about the aspects in which the encoder is similar to, and the aspects in which it differs, from JPEG 2000. Is it learning similar filters or completely different ones ? For what kinds of textures does it do better and for what kinds does it do worse (the paper could show the best and worst 10 patches at different bit-rates) ?

2. Secondly, I think it's crucial that the paper demonstrate that the benefits come from a better coding scheme (as opposed to just a better decoder), as suggested in my initial pre-review question. How would a decoder trained on JPEG-2000 codes (and perhaps also on encoded random projections) do worse or better ?

3. Finally, I think the fact that it does as well/worse than JPEG-2000 significantly diminishes the case for using a 'deep' auto-encoder. JPEG-2000 essentially uses a wavelet transform, which is a basis that past studies have shown could be recovered using a simple sparse dictionary algorithm like K-SVD. This is why I feel that the method needs to clearly outperform JPEG-2000, or show comparisons to (or atleast discuss) a well-crafted traditional/generative model-based baseline.",1
"The paper proposes a neural approach to learning an image compression-decompression scheme as an auto-encoder. While the idea is certainly interesting and well-motivated, in practice, it turns out to achieve effectively identical rates to JPEG-2000.

Now, as the authors argue, there is some value to the fact that this scheme was learned automatically rather than by expert design---which means it has benefits beyond the compression of natural images (e.g., it could be used to automatically learning a compression scheme for signals for which we don't have as much domain knowledge). However, I still believe that this makes the paper unsuitable for publication in its current form because of the following reasons---

1. Firstly, the fact that the learned encoder is competitive---and not clearly better---than JPEG 2000 means that the focus of the paper should more be about the aspects in which the encoder is similar to, and the aspects in which it differs, from JPEG 2000. Is it learning similar filters or completely different ones ? For what kinds of textures does it do better and for what kinds does it do worse (the paper could show the best and worst 10 patches at different bit-rates) ?

2. Secondly, I think it's crucial that the paper demonstrate that the benefits come from a better coding scheme (as opposed to just a better decoder), as suggested in my initial pre-review question. How would a decoder trained on JPEG-2000 codes (and perhaps also on encoded random projections) do worse or better ?

3. Finally, I think the fact that it does as well/worse than JPEG-2000 significantly diminishes the case for using a 'deep' auto-encoder. JPEG-2000 essentially uses a wavelet transform, which is a basis that past studies have shown could be recovered using a simple sparse dictionary algorithm like K-SVD. This is why I feel that the method needs to clearly outperform JPEG-2000, or show comparisons to (or atleast discuss) a well-crafted traditional/generative model-based baseline.",1
"The authors propose to extend the “standard” attention mechanism, by extending it to consider a distribution over latent structures (e.g., alignments, syntactic parse trees, etc.). These latent variables are modeled as a graphical model with potentials derived from a neural network.

The paper is well-written and clear to understand. The proposed methods are evaluated on various problems, and in each case the “structured attention” models outperform baseline models (either one without attention, or using simple attention). For the two real-world tasks, the improvements obtained from the proposed approach are relatively small compared to the “simple” attention models, but the techniques are nonetheless interesting.

Main comments:
1. In the Japanese-English Machine Translation example, the relative difference in performance between the Sigmoid attention model, and the Structured attention model appears to be relatively small. In this case, I’m curious if the authors analyzed the attention alignments to determine whether the structured models resulted in better alignments. In other words, if ground-truth alignments are available for the dataset, or if they can be human-annotated for some test examples, it would be interesting to measure the quality of the alignments in addition to the BLEU metric.
2. In the final experiment on natural language inference, I thought it was a bit surprising that using pretrained syntactic attention layers did not appear to improve model performance, but instead appear to degrade performance. I was curious if the authors have any hypotheses for why this is the case?

Minor comments:
1. Typographical error: Equation 1: “p(z | x, q” → “p(z | x, q)”
2. Section 3.3: “Past work has demonstrated that the techniques necessary for this approach, … ” →  “Past work has demonstrated the techniques necessary for this approach, … ”",1
"The authors propose to extend the “standard” attention mechanism, by extending it to consider a distribution over latent structures (e.g., alignments, syntactic parse trees, etc.). These latent variables are modeled as a graphical model with potentials derived from a neural network.

The paper is well-written and clear to understand. The proposed methods are evaluated on various problems, and in each case the “structured attention” models outperform baseline models (either one without attention, or using simple attention). For the two real-world tasks, the improvements obtained from the proposed approach are relatively small compared to the “simple” attention models, but the techniques are nonetheless interesting.

Main comments:
1. In the Japanese-English Machine Translation example, the relative difference in performance between the Sigmoid attention model, and the Structured attention model appears to be relatively small. In this case, I’m curious if the authors analyzed the attention alignments to determine whether the structured models resulted in better alignments. In other words, if ground-truth alignments are available for the dataset, or if they can be human-annotated for some test examples, it would be interesting to measure the quality of the alignments in addition to the BLEU metric.
2. In the final experiment on natural language inference, I thought it was a bit surprising that using pretrained syntactic attention layers did not appear to improve model performance, but instead appear to degrade performance. I was curious if the authors have any hypotheses for why this is the case?

Minor comments:
1. Typographical error: Equation 1: “p(z | x, q” → “p(z | x, q)”
2. Section 3.3: “Past work has demonstrated that the techniques necessary for this approach, … ” →  “Past work has demonstrated the techniques necessary for this approach, … ”",1
"This paper tests zoneout against a variety of datasets - character level, word level, and pMNIST classification - showing applicability in a wide range of scenarios. While zoneout acts as a regularizer to prevent overfitting, it also has similarities to residual connections. The continued analysis of this aspect, including analyzing how the gradient flow improves the given tasks, is of great interest and helps show it as an inherent property of zoneout.

This is a well written paper with a variety of experiments that support the claims. I have also previously used this technique in a recurrent setting and am confident on the positive impact it can have upon tasks. This is likely to become a standard technique used within RNNs across various frameworks.",1
"This paper tests zoneout against a variety of datasets - character level, word level, and pMNIST classification - showing applicability in a wide range of scenarios. While zoneout acts as a regularizer to prevent overfitting, it also has similarities to residual connections. The continued analysis of this aspect, including analyzing how the gradient flow improves the given tasks, is of great interest and helps show it as an inherent property of zoneout.

This is a well written paper with a variety of experiments that support the claims. I have also previously used this technique in a recurrent setting and am confident on the positive impact it can have upon tasks. This is likely to become a standard technique used within RNNs across various frameworks.",1
"Thank you for an interesting read.

I found this paper very interesting. Since I don't think (deterministic) approximate inference is separated from the modelling procedure (cf. exact inference), it is important to allow the users to select the inference method to suit their needs and constraints. I'm not an expert of PPL, but to my knowledge this is the first package that I've seen which put more focus on compositional inference. Leveraging tensorflow is also a plus, which allows flexible computation graph design as well as parallel computation using GPUs.

The only question I have is about the design of flexible objective functions to learn hyper-parameters (or in the paper those variables associated with delta q distributions). It seems hyper-parameter learning is also specified as inference, which makes sense if using MAP. However the authors also demonstrated other objective functions such as Renyi divergences, does that mean the user need to define a new class of inference method whenever they want to test an alternative loss function?",1
"Thank you for an interesting read.

I found this paper very interesting. Since I don't think (deterministic) approximate inference is separated from the modelling procedure (cf. exact inference), it is important to allow the users to select the inference method to suit their needs and constraints. I'm not an expert of PPL, but to my knowledge this is the first package that I've seen which put more focus on compositional inference. Leveraging tensorflow is also a plus, which allows flexible computation graph design as well as parallel computation using GPUs.

The only question I have is about the design of flexible objective functions to learn hyper-parameters (or in the paper those variables associated with delta q distributions). It seems hyper-parameter learning is also specified as inference, which makes sense if using MAP. However the authors also demonstrated other objective functions such as Renyi divergences, does that mean the user need to define a new class of inference method whenever they want to test an alternative loss function?",1
"The authors propose a method that generates naturally looking images by first generating the background and then conditioned on the previous layer one or multiple foreground objects. Additionally they add a image transformer layer that allows the model to more easily model different appearances.

I would like to see some discussion about the choice of foreground+mask rather than just predicting foreground directly. For MNIST, for example the foreground seems completely irrelevant. For CUB and CIFAR of course the fg adds the texture and color while the masks ensures a crisp boundary. 
- Is the mask a binary mask or a alpha blending mask?
- I find the fact that the model learns to decompose images this nicely and learns to produce crisp foreground masks w/o too much spurious elements (though there are some in CIFAR) pretty fascinating.

The proposed evaluation metric makes sense and seems reasonable. However, AFAICT, theoretically it would be possible to get a high score even though the GAN produces images not recognizable to humans, but only to the classifier network that produces P_g. E.g. if the Generator encodes the class in some subtle way (though this shouldn't happen given the training with an adversarial network).

Fig 3 shows indeed nicely that the decomposition is much nicer when spatial transformers are used. However, it also seems to indicate that the foreground prediction and the foreground mask are largely redundant. For the final results the ""niceness"" of the decomposition appears to be largely irrelevant.

Furthermore, the transformation layer seems to have a small effect, judging from the transformed masked foreground objects. They are mainly scaled down.

- What is the 3rd & 6th column in Fig 9? It is not clear if the final composed images are really as bad as ""advertised"".

Regarding the eval experiment using AMT it is not clear why it is better to provide the users with L2 minimized NN matches rather than random pairs.

I assume that Tab 1 Adversarial Divergence for Real images was not actually evaluated? It would be interesting to see how close to 0 multiple differently initialized networks actually are. Also please mention how the confidences/std where generated, i.e. different training sets, initialisations, eval sets, and how many runs.",1
"The authors propose a method that generates naturally looking images by first generating the background and then conditioned on the previous layer one or multiple foreground objects. Additionally they add a image transformer layer that allows the model to more easily model different appearances.

I would like to see some discussion about the choice of foreground+mask rather than just predicting foreground directly. For MNIST, for example the foreground seems completely irrelevant. For CUB and CIFAR of course the fg adds the texture and color while the masks ensures a crisp boundary. 
- Is the mask a binary mask or a alpha blending mask?
- I find the fact that the model learns to decompose images this nicely and learns to produce crisp foreground masks w/o too much spurious elements (though there are some in CIFAR) pretty fascinating.

The proposed evaluation metric makes sense and seems reasonable. However, AFAICT, theoretically it would be possible to get a high score even though the GAN produces images not recognizable to humans, but only to the classifier network that produces P_g. E.g. if the Generator encodes the class in some subtle way (though this shouldn't happen given the training with an adversarial network).

Fig 3 shows indeed nicely that the decomposition is much nicer when spatial transformers are used. However, it also seems to indicate that the foreground prediction and the foreground mask are largely redundant. For the final results the ""niceness"" of the decomposition appears to be largely irrelevant.

Furthermore, the transformation layer seems to have a small effect, judging from the transformed masked foreground objects. They are mainly scaled down.

- What is the 3rd & 6th column in Fig 9? It is not clear if the final composed images are really as bad as ""advertised"".

Regarding the eval experiment using AMT it is not clear why it is better to provide the users with L2 minimized NN matches rather than random pairs.

I assume that Tab 1 Adversarial Divergence for Real images was not actually evaluated? It would be interesting to see how close to 0 multiple differently initialized networks actually are. Also please mention how the confidences/std where generated, i.e. different training sets, initialisations, eval sets, and how many runs.",1
"This paper motivates the combination of autoregressive models with Variational Auto-Encoders and how to control the amount the amount of information stored in the latent code. The authors provide state-of-the-art results on MNIST, OMNIGLOT and Caltech-101.
I find that the insights provided in the paper, e.g. with respect to the effect of having a more powerful decoder on learning the latent code, the bit-back coding, and the lossy decoding are well-written but are not novel.
The difference between an auto-regressive prior and the inverse auto-regressive posterior is new and interesting though.
The model presented combines the recent technique of PixelRNN/PixelCNN and Variational Auto-Encoders with Inverse Auto-Regressive Flows, which enables the authors to obtain state-of-the-art results on MNIST, OMNIGLOT and Caltech-101. Given the insights provided in the paper, the authors are also able to control the amount of information contained in the latent code to an extent.
This paper gather several insight on Variational Auto-Encoders scattered through several publications in a well-written way. From these, the authors are able to obtain state-of-the-art models on small complexity datasets. Larger scale experiments will be necessary.",1
"This paper motivates the combination of autoregressive models with Variational Auto-Encoders and how to control the amount the amount of information stored in the latent code. The authors provide state-of-the-art results on MNIST, OMNIGLOT and Caltech-101.
I find that the insights provided in the paper, e.g. with respect to the effect of having a more powerful decoder on learning the latent code, the bit-back coding, and the lossy decoding are well-written but are not novel.
The difference between an auto-regressive prior and the inverse auto-regressive posterior is new and interesting though.
The model presented combines the recent technique of PixelRNN/PixelCNN and Variational Auto-Encoders with Inverse Auto-Regressive Flows, which enables the authors to obtain state-of-the-art results on MNIST, OMNIGLOT and Caltech-101. Given the insights provided in the paper, the authors are also able to control the amount of information contained in the latent code to an extent.
This paper gather several insight on Variational Auto-Encoders scattered through several publications in a well-written way. From these, the authors are able to obtain state-of-the-art models on small complexity datasets. Larger scale experiments will be necessary.",1
"This paper poses an interesting idea: removing chaotic behavior or RNNs.
While many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box, this paper does a good job on presenting why its method may work well.

Although, the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model), the reviewer is not fully convinced by the main claim of this paper, the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable. In the paper, the LSTM shows a very sensitive behaviour, even when a very small amount of noise is added to the input. However, it still performs surprisingly well with this chaotic behaviour. 

Measuring the model complexity is a very difficult task, therefore, many papers manage to use either same number of hidden units or choose approximately close model sizes. In this paper, the experiments were carried by using the same amount of parameters for both the LSTM and CFN. However, I think the CFN may have much more simpler computational graph. Taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor?

It is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher. Could you provide more statistics on this? For example, what is the average relaxation time of the whole hidden units at each layer?

Batch normalization and layer normalization can be helpful to make the training of RNNs become more stable. How would the behaviour of batch normalized or perhaps layer normalized LSTM look like? Also, it is often not trivial to make batch normalization or layer normalization to work on a new architecture. I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN.

The quality of the work is good, explanation is clear enough along with nice analyses and proofs. Overall, the performance is not any better than LSTMs, but it is still interesting when thinking of simplicity of this model. I am a bit concerned if this model might not work that well in more harder task, e.g., translation. Figure 4 of this paper is very interesting, where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones.",1
"This paper poses an interesting idea: removing chaotic behavior or RNNs.
While many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box, this paper does a good job on presenting why its method may work well.

Although, the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model), the reviewer is not fully convinced by the main claim of this paper, the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable. In the paper, the LSTM shows a very sensitive behaviour, even when a very small amount of noise is added to the input. However, it still performs surprisingly well with this chaotic behaviour. 

Measuring the model complexity is a very difficult task, therefore, many papers manage to use either same number of hidden units or choose approximately close model sizes. In this paper, the experiments were carried by using the same amount of parameters for both the LSTM and CFN. However, I think the CFN may have much more simpler computational graph. Taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor?

It is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher. Could you provide more statistics on this? For example, what is the average relaxation time of the whole hidden units at each layer?

Batch normalization and layer normalization can be helpful to make the training of RNNs become more stable. How would the behaviour of batch normalized or perhaps layer normalized LSTM look like? Also, it is often not trivial to make batch normalization or layer normalization to work on a new architecture. I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN.

The quality of the work is good, explanation is clear enough along with nice analyses and proofs. Overall, the performance is not any better than LSTMs, but it is still interesting when thinking of simplicity of this model. I am a bit concerned if this model might not work that well in more harder task, e.g., translation. Figure 4 of this paper is very interesting, where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones.",1
"Paper Strengths: 
-- Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting  very large datasets in a computationally feasible manner

-- The effective batch size for training the MoE drastically increased also

-- Interesting experimental results on the effects of increasing the number of MoEs, which is expected.


Paper Weaknesses:

--- there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss  the use of MoE and other alternatives in terms of computational efficiency and other factors.",1
"Paper Strengths: 
-- Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting  very large datasets in a computationally feasible manner

-- The effective batch size for training the MoE drastically increased also

-- Interesting experimental results on the effects of increasing the number of MoEs, which is expected.


Paper Weaknesses:

--- there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss  the use of MoE and other alternatives in terms of computational efficiency and other factors.",1
"Authors' response well answered my questions. Thanks. 
Evaluation not changed.

###

This paper proposes a neural model for generating tree structure output from scratch. The model does 1) separate the recurrence between depths and siblings; 2) separate the topology and label generation, and outperforms previous methods on a benchmark IFTTT dataset. Compared to previous tree-decoding methods, the model avoids manually annotating subtrees with special tokens, and thus is a very good alternative to such problems. The paper does solid experiments on one synthetic dataset, and outperforms alternative methods on one real-world IFTTT dataset. 

There are couple of interesting results in the paper that I believe is worth further investigation. Firstly, on the synthetic dataset, the precision drops rapidly with the number of nodes. Is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences, such that the tree decoder can not do a good job? Or is it because that such tree decoder is not tolerant to the long sequence input, i.e., large tree structure? I believe that it is important to understand this before a better model can be developed. For example, if it is the fault of encoder, maybe an attention layer can be added, as in a seq-to-seq model, to preserve more information of the input sequence. 

Moreover, besides only showing how the precision changes with the number of nodes in the tree, it might be interesting to investigate how it goes with 1) number of depths; 2) number of widths; 3) symmetricity; etc. Moreover, as greedy search is used in decoding, it might be interesting to see how it helps, if it does, to use beam-search in tree decoding. 

On the IFTTT dataset, listing more statistics about this dataset might be helpful for better understanding the difficulty of this task. How deep are the trees? How large are the vocabularies on both language and program sides?

The paper is well written, except for minor typo as mentioned in my pre-review questions. 

In general, I believe this is a solid paper, and more can be explored in this direction. So I tend to accept it.",1
"Authors' response well answered my questions. Thanks. 
Evaluation not changed.

###

This paper proposes a neural model for generating tree structure output from scratch. The model does 1) separate the recurrence between depths and siblings; 2) separate the topology and label generation, and outperforms previous methods on a benchmark IFTTT dataset. Compared to previous tree-decoding methods, the model avoids manually annotating subtrees with special tokens, and thus is a very good alternative to such problems. The paper does solid experiments on one synthetic dataset, and outperforms alternative methods on one real-world IFTTT dataset. 

There are couple of interesting results in the paper that I believe is worth further investigation. Firstly, on the synthetic dataset, the precision drops rapidly with the number of nodes. Is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences, such that the tree decoder can not do a good job? Or is it because that such tree decoder is not tolerant to the long sequence input, i.e., large tree structure? I believe that it is important to understand this before a better model can be developed. For example, if it is the fault of encoder, maybe an attention layer can be added, as in a seq-to-seq model, to preserve more information of the input sequence. 

Moreover, besides only showing how the precision changes with the number of nodes in the tree, it might be interesting to investigate how it goes with 1) number of depths; 2) number of widths; 3) symmetricity; etc. Moreover, as greedy search is used in decoding, it might be interesting to see how it helps, if it does, to use beam-search in tree decoding. 

On the IFTTT dataset, listing more statistics about this dataset might be helpful for better understanding the difficulty of this task. How deep are the trees? How large are the vocabularies on both language and program sides?

The paper is well written, except for minor typo as mentioned in my pre-review questions. 

In general, I believe this is a solid paper, and more can be explored in this direction. So I tend to accept it.",1
"EDIT: Updated score. See additional comment.

I quite like the main idea of the paper, which is based on the observation in Sec. 3.0 - that the authors find many predictable patterns in the independent evolution of weights during neural network training. It is very encouraging that a simple neural network can be used to speed up training by directly predicting weights.

However the technical quality of the current paper leaves much to be desired, and I encourage the authors to do more rigorous analysis of the approach. Here are some concrete suggestions:

- The findings in Section 3.0 which motivate the approach, should be clearly presented in the paper. Presently they are stated as anecdotes.

- A central issue with the paper is that the training of the Introspection network I is completely glossed over. How well did the training work, in terms of training, validation/test losses? How well does it need to work in order to be useful for speeding up training? These are important questions for anyone interested in this approach.

- An additional important issue is that of baselines. Would a simple linear/quadratic model also work instead of a neural network? What about a simple heuristic rule to increase/decrease weights? I think it's important to compare to such baselines to understand the complexity of the weight evolution learned by the neural network.

- I do not think that default tensorflow example hyperparameters should be used, as mentioned by authors on OpenReview. There is no scientific basis for using them. Instead, first hyperparameters which produce good results in a reasonable time should be selected as the baseline, and then added the benefit of the introspection network to speed up training (and reaching a similar result) should be shown.

- The authors state in the discussion on OpenReview that they also tried RNNs as the introspection network but it didn't work with small state size. What does ""didn't work"" mean in this context? Did it underfit? I find it hard to imagine that a large state size would be required for this task. Even if it is, that doesn't rule out evaluation due to memory issues because the RNN can be run on the weights in 'mini-batch' mode. In general, I think other baselines are more important than RNN.

- A question about jump points: 
The I is trained on SGD trajectories. While using I to speed up training at several jump points, if the input weights cross previous jump points, then I gets input data from a weight evolution which is not from SGD (it has been altered by I). This seems problematic but doesn't seem to affect your experiments. I feel that this again highlights the importance of the baselines. Perhaps I is doing something extremely simple that is not affected by this issue.

Since the main idea is very interesting, I will be happy to update my score if the above concerns are addressed.",1
"EDIT: Updated score. See additional comment.

I quite like the main idea of the paper, which is based on the observation in Sec. 3.0 - that the authors find many predictable patterns in the independent evolution of weights during neural network training. It is very encouraging that a simple neural network can be used to speed up training by directly predicting weights.

However the technical quality of the current paper leaves much to be desired, and I encourage the authors to do more rigorous analysis of the approach. Here are some concrete suggestions:

- The findings in Section 3.0 which motivate the approach, should be clearly presented in the paper. Presently they are stated as anecdotes.

- A central issue with the paper is that the training of the Introspection network I is completely glossed over. How well did the training work, in terms of training, validation/test losses? How well does it need to work in order to be useful for speeding up training? These are important questions for anyone interested in this approach.

- An additional important issue is that of baselines. Would a simple linear/quadratic model also work instead of a neural network? What about a simple heuristic rule to increase/decrease weights? I think it's important to compare to such baselines to understand the complexity of the weight evolution learned by the neural network.

- I do not think that default tensorflow example hyperparameters should be used, as mentioned by authors on OpenReview. There is no scientific basis for using them. Instead, first hyperparameters which produce good results in a reasonable time should be selected as the baseline, and then added the benefit of the introspection network to speed up training (and reaching a similar result) should be shown.

- The authors state in the discussion on OpenReview that they also tried RNNs as the introspection network but it didn't work with small state size. What does ""didn't work"" mean in this context? Did it underfit? I find it hard to imagine that a large state size would be required for this task. Even if it is, that doesn't rule out evaluation due to memory issues because the RNN can be run on the weights in 'mini-batch' mode. In general, I think other baselines are more important than RNN.

- A question about jump points: 
The I is trained on SGD trajectories. While using I to speed up training at several jump points, if the input weights cross previous jump points, then I gets input data from a weight evolution which is not from SGD (it has been altered by I). This seems problematic but doesn't seem to affect your experiments. I feel that this again highlights the importance of the baselines. Perhaps I is doing something extremely simple that is not affected by this issue.

Since the main idea is very interesting, I will be happy to update my score if the above concerns are addressed.",1
"This paper discusses Hyperband, an extension of successive halving by Jamieson & Talwalkar (AISTATS 2016). Successive halving is a very nice algorithm that starts evaluating many configurations and repeatedly cuts off the current worst half to explore many configuration for a limited budget.

Having read the paper for the question period and just rereading it again, I am now not entirely sure what its contribution is meant to be: the only improvement of Hyperband vs. successive halving is in the theoretical worst case bounds (not more than 5x worse than random search), but you can (a) trivially obtain that bound by using a fifth of your time for running random configurations to completion and (b) the theoretical analysis to show this is said to be beyond the scope of the paper. That makes me wonder whether the theoretical results are the contribution of this paper, or whether they are the subject of a different paper and the current paper is mostly an empirical study of the method?
I hope to get a response by the authors and see this made clearer in an updated version of the paper.

In terms of experiments, the paper fails to show a case where Hyperband actually performs better than the authors' previous algorithm successive halving with its most agressive setting of bracket b=4. Literally, in every figure, bracket b=4 is at least as good (and sometimes substantially better) than Hyperband. That makes me think that in practice I would prefer successive halving with b=4 over Hyperband. (And if I really want Hyperband's guarantee of not being more than 5x worse than random search I can run random search on a fifth of my machines.) 
The experiments also compare to some Bayesian optimization methods, but not to the most relevant very closely related Multi-Task Bayesian Optimization methods that have been dominating effective methods for deep learning in that area in the last 3 years: ""Multi-Task Bayesian Optimization"" by Swersky, Snoek, and Adams (2013) already showed 5x speedups for deep learning by starting with smaller datasets, and there have been several follow-up papers showing even larger speedups. 

Given that this prominent work on multitask Bayesian optimization exists, I also think the introduction, which sells Hyperband as a very new approach to hyperparameter optimization is misleading. I would've much preferred a more down-to-earth pitch that says ""configuration evaluation"" has been becoming a very important feature in hyperparameter optimization, including Bayesian optimization, that sometimes yields very large speedups (this can be quantified by examples from existing papers) and this paper adds some much-needed theoretical understanding to this and demonstrates how important configuration evaluation is even in the simplest case of being used with random search. I think this could be done easily and locally by adding a paragraph to the intro.

As another point regarding novelty, I think the authors should make clear that approaches for adaptively deciding how many resources to use for which evaluation have been studied for (at least) 23 years in the ML community -- see Maron & Moore, NIPS 1993: ""Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation"" (",1
"This paper discusses Hyperband, an extension of successive halving by Jamieson & Talwalkar (AISTATS 2016). Successive halving is a very nice algorithm that starts evaluating many configurations and repeatedly cuts off the current worst half to explore many configuration for a limited budget.

Having read the paper for the question period and just rereading it again, I am now not entirely sure what its contribution is meant to be: the only improvement of Hyperband vs. successive halving is in the theoretical worst case bounds (not more than 5x worse than random search), but you can (a) trivially obtain that bound by using a fifth of your time for running random configurations to completion and (b) the theoretical analysis to show this is said to be beyond the scope of the paper. That makes me wonder whether the theoretical results are the contribution of this paper, or whether they are the subject of a different paper and the current paper is mostly an empirical study of the method?
I hope to get a response by the authors and see this made clearer in an updated version of the paper.

In terms of experiments, the paper fails to show a case where Hyperband actually performs better than the authors' previous algorithm successive halving with its most agressive setting of bracket b=4. Literally, in every figure, bracket b=4 is at least as good (and sometimes substantially better) than Hyperband. That makes me think that in practice I would prefer successive halving with b=4 over Hyperband. (And if I really want Hyperband's guarantee of not being more than 5x worse than random search I can run random search on a fifth of my machines.) 
The experiments also compare to some Bayesian optimization methods, but not to the most relevant very closely related Multi-Task Bayesian Optimization methods that have been dominating effective methods for deep learning in that area in the last 3 years: ""Multi-Task Bayesian Optimization"" by Swersky, Snoek, and Adams (2013) already showed 5x speedups for deep learning by starting with smaller datasets, and there have been several follow-up papers showing even larger speedups. 

Given that this prominent work on multitask Bayesian optimization exists, I also think the introduction, which sells Hyperband as a very new approach to hyperparameter optimization is misleading. I would've much preferred a more down-to-earth pitch that says ""configuration evaluation"" has been becoming a very important feature in hyperparameter optimization, including Bayesian optimization, that sometimes yields very large speedups (this can be quantified by examples from existing papers) and this paper adds some much-needed theoretical understanding to this and demonstrates how important configuration evaluation is even in the simplest case of being used with random search. I think this could be done easily and locally by adding a paragraph to the intro.

As another point regarding novelty, I think the authors should make clear that approaches for adaptively deciding how many resources to use for which evaluation have been studied for (at least) 23 years in the ML community -- see Maron & Moore, NIPS 1993: ""Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation"" (",1
"*** Paper Summary ***

This paper formalizes the properties required for addressing (indexing) memory augmented neural networks as well as how to pair the addressing with read/write operation. It then proposes a framework in which any Lie group as the addressing space. Experiments on algorithmic tasks are reported.

*** Review Summary ***

This paper brings unity and formalism in the requirement for memory addressing while maintaining differentiable memories. Its proposal provide a generic scheme to build addressing mechanisms. When comparing the proposed approach with key-value networks, the unbounded number of memory cells and the lack of incentive to reuse indexes might reveal impractical. 

*** Detailed Review ***

The paper reads well, has appropriate relevance to related work. The unified presentation of memory augmented networks is clear and brings unity to the field. The proposed approach is introduced clearly, is powerful and gives a tool that can be reused after reading the article. I do not appreciate that the growing memory is not mentioned as a drawback. It should be stressed and a discussion on the impact it has on efficiency/scalability is needed.",1
"*** Paper Summary ***

This paper formalizes the properties required for addressing (indexing) memory augmented neural networks as well as how to pair the addressing with read/write operation. It then proposes a framework in which any Lie group as the addressing space. Experiments on algorithmic tasks are reported.

*** Review Summary ***

This paper brings unity and formalism in the requirement for memory addressing while maintaining differentiable memories. Its proposal provide a generic scheme to build addressing mechanisms. When comparing the proposed approach with key-value networks, the unbounded number of memory cells and the lack of incentive to reuse indexes might reveal impractical. 

*** Detailed Review ***

The paper reads well, has appropriate relevance to related work. The unified presentation of memory augmented networks is clear and brings unity to the field. The proposed approach is introduced clearly, is powerful and gives a tool that can be reused after reading the article. I do not appreciate that the growing memory is not mentioned as a drawback. It should be stressed and a discussion on the impact it has on efficiency/scalability is needed.",1
"The authors propose a recurrent neural network architecture that is able to output more accurate long-term predictions of several game environments than the current state-of-the-art.
The original network architecture was inspired by inability of previous methods to accurately predict many time-steps into the future,
and their inability to jump directly to a future prediction without iterating through all intermediate states.
The authors have provided an extensive experimental evaluation on several benchmarks with promising results.
In general the paper is well written and quite clear in its explanations.
Demonstrating that this kind of future state prediction is useful for 3D maze exploration is a plus.

# Minor comments:
`jumpy predictions have been developed in low-dimensional observation spaces' - cite relevant work in the paper.

# Typos
Section 3.1 - `this configuration is all experiments'",1
"The authors propose a recurrent neural network architecture that is able to output more accurate long-term predictions of several game environments than the current state-of-the-art.
The original network architecture was inspired by inability of previous methods to accurately predict many time-steps into the future,
and their inability to jump directly to a future prediction without iterating through all intermediate states.
The authors have provided an extensive experimental evaluation on several benchmarks with promising results.
In general the paper is well written and quite clear in its explanations.
Demonstrating that this kind of future state prediction is useful for 3D maze exploration is a plus.

# Minor comments:
`jumpy predictions have been developed in low-dimensional observation spaces' - cite relevant work in the paper.

# Typos
Section 3.1 - `this configuration is all experiments'",1
"The paper looks at the problem of transferring a policy learned in a simulator to  a target real-world system.  The proposed approach considers using an ensemble of simulated source domains, along with adversarial training, to learn a robust policy that is able to generalize to several target domains.

Overall, the paper tackles an interesting problem, and provides a reasonable solution.  The notion of adversarial training used here does not seem the same as other recent literature (e.g. on GANs).  It would be useful to add more details on a few components, as discussed in the question/response round.  I also encourage including the results with alternative policy gradient subroutines, even if they don’t perform well (e.g. Reinforce), as well as results with and without the baseline on the value function. Such results are very useful to other researchers.",1
"The paper looks at the problem of transferring a policy learned in a simulator to  a target real-world system.  The proposed approach considers using an ensemble of simulated source domains, along with adversarial training, to learn a robust policy that is able to generalize to several target domains.

Overall, the paper tackles an interesting problem, and provides a reasonable solution.  The notion of adversarial training used here does not seem the same as other recent literature (e.g. on GANs).  It would be useful to add more details on a few components, as discussed in the question/response round.  I also encourage including the results with alternative policy gradient subroutines, even if they don’t perform well (e.g. Reinforce), as well as results with and without the baseline on the value function. Such results are very useful to other researchers.",1
"In this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably.
One possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates.


Pros:
The paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work.
The experiments are good proofs of concept, but do not go beyond that i.m.h.o. 
Even so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning).

Cons:
As the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell.
The transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016).
Since the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task.
Finally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an  average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data.",1
"In this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably.
One possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates.


Pros:
The paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work.
The experiments are good proofs of concept, but do not go beyond that i.m.h.o. 
Even so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning).

Cons:
As the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell.
The transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016).
Since the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task.
Finally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an  average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data.",1
"this proposes a multi-view learning approach for learning representations for acoustic sequences. they investigate the use of bidirectional LSTM with contrastive losses. experiments show improvement over the previous work.

although I have no expertise in speech processing, I am in favor of accepting this paper because of following contributions:
- investigating the use of fairly known architecture on a new domain.
- providing novel objectives specific to the domain
- setting up new benchmarks designed for evaluating multi-view models

I hope authors open-source their implementation so that people can replicate results, compare their work, and improve on this work.",1
"this proposes a multi-view learning approach for learning representations for acoustic sequences. they investigate the use of bidirectional LSTM with contrastive losses. experiments show improvement over the previous work.

although I have no expertise in speech processing, I am in favor of accepting this paper because of following contributions:
- investigating the use of fairly known architecture on a new domain.
- providing novel objectives specific to the domain
- setting up new benchmarks designed for evaluating multi-view models

I hope authors open-source their implementation so that people can replicate results, compare their work, and improve on this work.",1
"The paper introduces a new dataset called MusicNet (presumably analogous to ImageNet), featuring dense ground truth labels for 30+ hours of classical music, which is provided as raw audio. Such a dataset is extremely valuable for music information retrieval (MIR) research and a dataset of this size has never before been publicly available. It has the potential to dramatically increase the impact of modern machine learning techniques (e.g. deep learning) in this field, whose adoption has previously been hampered by a lack of available datasets that are large enough. The paper is clear and well-written.

The paper also features some ""example"" experiments using the dataset, which I am somewhat less excited about. The authors decided to focus on one single task that is not particularly challenging: identifying pitches in isolated segments of audio. Pitch information is a fairly low-level characteristic of music. Considering that isolated fragments are used as input, this is a relatively simple problem that probably doesn't even require machine learning to solve adequately, e.g. peak picking on a spectral representation could already get you pretty far. It's not clear what value the machine learning component in the proposed approach actually adds, if any. I could be wrong about this as I haven't done the comparison myself, but I think the burden is on the authors to demonstrate that using ML here is actually useful.

I would argue that one of the strenghts of the dataset is the variety of label information it provides, so a much more convincing setup would have been to demonstrate many different prediction tasks for both low-level (e.g. pitch, onsets) and high-level (e.g. composer) characteristics, perhaps with fewer and simpler models -- maybe even sticking to spectrogram input and forgoing raw audio input for the time being, as this comparison seems orthogonal to the introduction of the dataset itself. As it stands, I feel that the fact that the experiments are relatively uninteresting detracts from the main point of the paper, which is to introduce a new public dataset that is truly unique in terms of its scale and scope.

That said, the experiments seem to have been conducted in a rigorous fashion and the evaluation and analysis of the resulting models is properly executed.

Re: Section 4.5, it is rather unsurprising to me that a pitch detector would learn filters that resemble pitches (i.e. sinusoids), although the observation that this requires a relatively large amount of data is interesting. However, it would be more interesting to demonstrate that this is also the case for higher-level tasks. The authors favourably compare the features learnt by their model with prior work on end-to-end learning from raw audio, but neglect that the tasks considered in this work were much more high-level.

Some might also question whether ICLR is the appropriate venue to introduce a new dataset, but personally I think it's a great idea to submit it here, seeing as it will reach the right people. I suppose this is up to the organisers and the program committee, but I thought it important to mention this, because I don't think this paper merits acceptance based on its experimental results alone.",1
"The paper introduces a new dataset called MusicNet (presumably analogous to ImageNet), featuring dense ground truth labels for 30+ hours of classical music, which is provided as raw audio. Such a dataset is extremely valuable for music information retrieval (MIR) research and a dataset of this size has never before been publicly available. It has the potential to dramatically increase the impact of modern machine learning techniques (e.g. deep learning) in this field, whose adoption has previously been hampered by a lack of available datasets that are large enough. The paper is clear and well-written.

The paper also features some ""example"" experiments using the dataset, which I am somewhat less excited about. The authors decided to focus on one single task that is not particularly challenging: identifying pitches in isolated segments of audio. Pitch information is a fairly low-level characteristic of music. Considering that isolated fragments are used as input, this is a relatively simple problem that probably doesn't even require machine learning to solve adequately, e.g. peak picking on a spectral representation could already get you pretty far. It's not clear what value the machine learning component in the proposed approach actually adds, if any. I could be wrong about this as I haven't done the comparison myself, but I think the burden is on the authors to demonstrate that using ML here is actually useful.

I would argue that one of the strenghts of the dataset is the variety of label information it provides, so a much more convincing setup would have been to demonstrate many different prediction tasks for both low-level (e.g. pitch, onsets) and high-level (e.g. composer) characteristics, perhaps with fewer and simpler models -- maybe even sticking to spectrogram input and forgoing raw audio input for the time being, as this comparison seems orthogonal to the introduction of the dataset itself. As it stands, I feel that the fact that the experiments are relatively uninteresting detracts from the main point of the paper, which is to introduce a new public dataset that is truly unique in terms of its scale and scope.

That said, the experiments seem to have been conducted in a rigorous fashion and the evaluation and analysis of the resulting models is properly executed.

Re: Section 4.5, it is rather unsurprising to me that a pitch detector would learn filters that resemble pitches (i.e. sinusoids), although the observation that this requires a relatively large amount of data is interesting. However, it would be more interesting to demonstrate that this is also the case for higher-level tasks. The authors favourably compare the features learnt by their model with prior work on end-to-end learning from raw audio, but neglect that the tasks considered in this work were much more high-level.

Some might also question whether ICLR is the appropriate venue to introduce a new dataset, but personally I think it's a great idea to submit it here, seeing as it will reach the right people. I suppose this is up to the organisers and the program committee, but I thought it important to mention this, because I don't think this paper merits acceptance based on its experimental results alone.",1
"The authors present results on a number of different tasks where the goal is to determine whether a given test example is out-of-domain or likely to be mis-classified. This is accomplished by examining statistics for the softmax probability for the most likely class; although the score by itself is not a particularly good measure of confidence, the statistics for out-of-domain examples are different enough from in-domain examples to allow these to be identified with some certainty. 

My comments appear below:
1. As the authors point out, the AUROC/AUPR criterion is threshold independent. As a result, it is not obvious whether the thresholds that would correspond to a certain operating point (say a true positive rate of 10%) would be similar across different data sets. In other words, it would be interesting to know how sensitive the thresholds are to different test sets (or different splits of the test set). This is important if we want to use the thresholds determined on a given held-out set during evaluation on unseen data (where we would need to select a threshold).

2. Performance is reported in terms of AUROC/AUPR and models are compared against a random baseline. I think it’s a little hard to look at the differences in AUC/AUPR to get a sense for how much better the proposed classifier is than the random baseline. It would be useful, for example, if the authors could also report how strongly statistically significant some of these differences are (although admittedly they look to be pretty large in most cases).

3. In the experiments on speech recognition presented in Section 3.3, I was not entirely clear on how the model was evaluated. In Table 9, for example, is an “example” the entire utterance or just a single (stacked?) speech frame. Assuming that each “example” is an utterance, are the softmax probabilities the probability of the entire phone sequence (obtained by multiplying the local probability estimates from a Viterbi decoding?)

4. I’m curious about the decision to ignore the blank symbol’s logit in Section 3.3. Why is this required?

5. As I mentioned in the pre-review question, at least in the speech recognition case, it would have been interesting to compare performance obtained using a simple generative baseline (e.g., GMM-HMM). I think that would serve as a good indication of the ability of the proposed model to detect out-of-domain examples over the baseline.",1
"The authors present results on a number of different tasks where the goal is to determine whether a given test example is out-of-domain or likely to be mis-classified. This is accomplished by examining statistics for the softmax probability for the most likely class; although the score by itself is not a particularly good measure of confidence, the statistics for out-of-domain examples are different enough from in-domain examples to allow these to be identified with some certainty. 

My comments appear below:
1. As the authors point out, the AUROC/AUPR criterion is threshold independent. As a result, it is not obvious whether the thresholds that would correspond to a certain operating point (say a true positive rate of 10%) would be similar across different data sets. In other words, it would be interesting to know how sensitive the thresholds are to different test sets (or different splits of the test set). This is important if we want to use the thresholds determined on a given held-out set during evaluation on unseen data (where we would need to select a threshold).

2. Performance is reported in terms of AUROC/AUPR and models are compared against a random baseline. I think it’s a little hard to look at the differences in AUC/AUPR to get a sense for how much better the proposed classifier is than the random baseline. It would be useful, for example, if the authors could also report how strongly statistically significant some of these differences are (although admittedly they look to be pretty large in most cases).

3. In the experiments on speech recognition presented in Section 3.3, I was not entirely clear on how the model was evaluated. In Table 9, for example, is an “example” the entire utterance or just a single (stacked?) speech frame. Assuming that each “example” is an utterance, are the softmax probabilities the probability of the entire phone sequence (obtained by multiplying the local probability estimates from a Viterbi decoding?)

4. I’m curious about the decision to ignore the blank symbol’s logit in Section 3.3. Why is this required?

5. As I mentioned in the pre-review question, at least in the speech recognition case, it would have been interesting to compare performance obtained using a simple generative baseline (e.g., GMM-HMM). I think that would serve as a good indication of the ability of the proposed model to detect out-of-domain examples over the baseline.",1
"Two things I really liked about this paper:
1. The whole idea of having a data-dependent proposal distribution for MCMC. I wasn't familiar with this, although it apparently was previously published. I went back: the (Zhu, 2000) paper was unreadable. The (Jampani, 2014) paper on informed sampling was good. So, perhaps this isn't a good reason for accepting to ICLR.

2. The results are quite impressive. The rough rule-of-thumb is that optimization can help you speed up code by 10%. The standard MCMC results presented on the paper on randomly-generated programs roughly matches this (15%). The fact that the proposed algorithm get ~33% speedup is quite surprising, and worth publishing.

The argument against accepting this paper is that it doesn't match the goals of ICLR. I don't go to ICLR to hear about generic machine learning papers (we have NIPS and ICML for that). Instead, I go to learn about how to automatically represent data and models. Now, maybe this paper talks about how to represent (generated) programs, so it tangentially lives under the umbrella of ICLR. But it will compete against more relevant papers in the conference -- it may just be a poster. Sending this to a programming language conference may have more eventual impact.

Nonetheless, I give this paper an ""accept"", because I learned something valuable and the results are very good.",1
"Two things I really liked about this paper:
1. The whole idea of having a data-dependent proposal distribution for MCMC. I wasn't familiar with this, although it apparently was previously published. I went back: the (Zhu, 2000) paper was unreadable. The (Jampani, 2014) paper on informed sampling was good. So, perhaps this isn't a good reason for accepting to ICLR.

2. The results are quite impressive. The rough rule-of-thumb is that optimization can help you speed up code by 10%. The standard MCMC results presented on the paper on randomly-generated programs roughly matches this (15%). The fact that the proposed algorithm get ~33% speedup is quite surprising, and worth publishing.

The argument against accepting this paper is that it doesn't match the goals of ICLR. I don't go to ICLR to hear about generic machine learning papers (we have NIPS and ICML for that). Instead, I go to learn about how to automatically represent data and models. Now, maybe this paper talks about how to represent (generated) programs, so it tangentially lives under the umbrella of ICLR. But it will compete against more relevant papers in the conference -- it may just be a poster. Sending this to a programming language conference may have more eventual impact.

Nonetheless, I give this paper an ""accept"", because I learned something valuable and the results are very good.",1
"A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.

Summary:
———
I think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.

Quality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.
Clarity: Some of the derivations and intuitions could be explained in more detail.
Originality: The suggested idea is reasonable albeit heuristics are required.
Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.

Details:
————
1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.

2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (",1
"A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.

Summary:
———
I think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.

Quality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.
Clarity: Some of the derivations and intuitions could be explained in more detail.
Originality: The suggested idea is reasonable albeit heuristics are required.
Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.

Details:
————
1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.

2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (",1
"This is an interesting paper about quantized networks that work on temporal difference inputs.  The basic idea is that when a network has only to process differences then this is computational much more efficient specifically with natural video data since large parts of an image would be fairly constant so that the network only has to process the informative sections of the image (video stream). This is of course how the human visual system works, and it is hence of interest even beyond the core machine learning community. 

As an aside, there is a strong community interested in event-based vision such as the group of Tobi Delbrück, and it might be interesting to connect to this community. This might even provide a reference for your comments on page 1.

I guess the biggest novel contribution is that a rounding network can be replaced by a sigma-delta network, but that the order of discretization and summation doe make some difference in the actual processing load. I think I followed the steps and 
Most of my questions have already been answers in the pre-review period. My only question remaining is on page 3, “It should be noted that when we refer to “temporal differences”, we refer not to the change in the signal over time, but in the change between two inputs presented sequentially. The output of our network only depends on the value and order of inputs, not on the temporal spacing between them.”

This does not make sense to me. As I understand you just take the difference between two frames regardless if you call this temporal or not it is a change in one frame. So this statement rather confuses me and maybe should be dropped unless I do miss something here, in which case some more explanation would be necessary.

Figure 1 should be made bigger.

An improvement of the paper that I could think about is a better discussion of the relevance of the findings. Yes, you do show that your sigma-delta network save some operation compared to threshold, but is this difference essential for a specific task, or does your solution has relevance for neuroscience?",1
"This is an interesting paper about quantized networks that work on temporal difference inputs.  The basic idea is that when a network has only to process differences then this is computational much more efficient specifically with natural video data since large parts of an image would be fairly constant so that the network only has to process the informative sections of the image (video stream). This is of course how the human visual system works, and it is hence of interest even beyond the core machine learning community. 

As an aside, there is a strong community interested in event-based vision such as the group of Tobi Delbrück, and it might be interesting to connect to this community. This might even provide a reference for your comments on page 1.

I guess the biggest novel contribution is that a rounding network can be replaced by a sigma-delta network, but that the order of discretization and summation doe make some difference in the actual processing load. I think I followed the steps and 
Most of my questions have already been answers in the pre-review period. My only question remaining is on page 3, “It should be noted that when we refer to “temporal differences”, we refer not to the change in the signal over time, but in the change between two inputs presented sequentially. The output of our network only depends on the value and order of inputs, not on the temporal spacing between them.”

This does not make sense to me. As I understand you just take the difference between two frames regardless if you call this temporal or not it is a change in one frame. So this statement rather confuses me and maybe should be dropped unless I do miss something here, in which case some more explanation would be necessary.

Figure 1 should be made bigger.

An improvement of the paper that I could think about is a better discussion of the relevance of the findings. Yes, you do show that your sigma-delta network save some operation compared to threshold, but is this difference essential for a specific task, or does your solution has relevance for neuroscience?",1
"Encouraging orthogonality in weight features has been reported useful for deep networks in many previous works. The authors present a explicit regularization cost to achieve de-correlation among weight features in a layer and encourage orthogonality. Further, they also show why and how negative correlations can and should be avoided for better de-correlation. 

Orthogonal weight features achieve better generalization in case of large number of trainable parameters and less training data, which usually results in over-fitting. As also mentioned by the authors biases help in de-correlation of feature responses even in the presence of correlated features (weights). Regularization techniques like OrthoReg can be more helpful in training deeper and leaner networks, where the representational capacity of each layer is low, and also generalize better.

Although the improvement in performances is not significant the direction of research and the observations made are promising.",1
"Encouraging orthogonality in weight features has been reported useful for deep networks in many previous works. The authors present a explicit regularization cost to achieve de-correlation among weight features in a layer and encourage orthogonality. Further, they also show why and how negative correlations can and should be avoided for better de-correlation. 

Orthogonal weight features achieve better generalization in case of large number of trainable parameters and less training data, which usually results in over-fitting. As also mentioned by the authors biases help in de-correlation of feature responses even in the presence of correlated features (weights). Regularization techniques like OrthoReg can be more helpful in training deeper and leaner networks, where the representational capacity of each layer is low, and also generalize better.

Although the improvement in performances is not significant the direction of research and the observations made are promising.",1
"This paper applies the idea of normalizing flows (NFs), which allows us to build complex densities with tractable likelihoods, to maximum entropy constrained optimization.

The paper is clearly written and is easy to follow.

Novelty is a weak factor in this paper. The main contributions come from (1) applying previous work on NFs to the problem of MaxEnt estimation and (2) addressing some of the optimization issues resulting from stochastic approximations to E[||T||] in combination with the annealing of Lagrange multipliers.
Applying the NFs to MaxEnt is in itself not very novel as a framework. For instance, one could obtain a loss equivalent to the main loss in eq. (6) by minimizing the KLD between KL[p_{\phi};f], where f is the unormalized likelihood f \propto exp \sum_k( - \lambda_k T - c_k ||T_k||^2  ). This type of derivation is typical in all previous works using NFs for variational inference.
A few experiments on more complex data would strengthen the paper's results. The two experiments provided show good results but both of them are toy problems.


Minor point:

Although intuitive, it would be good to have a short discussion of step 8 of algorithm 1 as well.",1
"This paper applies the idea of normalizing flows (NFs), which allows us to build complex densities with tractable likelihoods, to maximum entropy constrained optimization.

The paper is clearly written and is easy to follow.

Novelty is a weak factor in this paper. The main contributions come from (1) applying previous work on NFs to the problem of MaxEnt estimation and (2) addressing some of the optimization issues resulting from stochastic approximations to E[||T||] in combination with the annealing of Lagrange multipliers.
Applying the NFs to MaxEnt is in itself not very novel as a framework. For instance, one could obtain a loss equivalent to the main loss in eq. (6) by minimizing the KLD between KL[p_{\phi};f], where f is the unormalized likelihood f \propto exp \sum_k( - \lambda_k T - c_k ||T_k||^2  ). This type of derivation is typical in all previous works using NFs for variational inference.
A few experiments on more complex data would strengthen the paper's results. The two experiments provided show good results but both of them are toy problems.


Minor point:

Although intuitive, it would be good to have a short discussion of step 8 of algorithm 1 as well.",1
"The paper presents an approach for tackling the instability problem that is present in generative adversarial networks. The general idea is to allow the generator to ""peek ahead"" at how the discriminator will evolve its decision boundary over-time with the premise that this information should prevent the generator from collapsing to produce only samples from a single mode of the data distribution.

This is a very well written paper that clearly motivates its attack on an important open issue. The experiments are well carried out and strongly support the presented idea. The pursued approach is substantially more elegant than current existing ""hacks"" that are commonly used to make GANs work in practice. I however have three main issues that let me partly doubt the success of the method. If these can be resolved this paper is a clear candidate for acceptance.

1) I am not entirely convinced that the same effect cannot be obtained by the following procedure: simply train the discriminator for an extended number of K steps when updating the generator (say a number equivalent to the unrolling steps used in the current experiments) then, after the generator was updated undo the K updates to the discriminator and do 1 new update step instead. I only briefly glanced at your response to Reviewer2 which seems to imply you now tried something similar to this setup by stopping gradient flow at an appropriate point (although I think this is not exactly equivalent).
2) I tried to reproduce the simple MNIST example but using a fully connected network instead of an RNN generator without much success. Even when unrolling the discriminator for 30-40 steps the generator still engages in mode seeking behavior or does not train at all. This could either be because of a bug in my implementation or because of some peculiarities of the RNN generator or because I did not use batch normalization anywhere. If it is one of the latter two this would entail a dependence of the proposed approach on specific forms of the discriminator and generator and should be discussed. My code can be found here",1
"The paper presents an approach for tackling the instability problem that is present in generative adversarial networks. The general idea is to allow the generator to ""peek ahead"" at how the discriminator will evolve its decision boundary over-time with the premise that this information should prevent the generator from collapsing to produce only samples from a single mode of the data distribution.

This is a very well written paper that clearly motivates its attack on an important open issue. The experiments are well carried out and strongly support the presented idea. The pursued approach is substantially more elegant than current existing ""hacks"" that are commonly used to make GANs work in practice. I however have three main issues that let me partly doubt the success of the method. If these can be resolved this paper is a clear candidate for acceptance.

1) I am not entirely convinced that the same effect cannot be obtained by the following procedure: simply train the discriminator for an extended number of K steps when updating the generator (say a number equivalent to the unrolling steps used in the current experiments) then, after the generator was updated undo the K updates to the discriminator and do 1 new update step instead. I only briefly glanced at your response to Reviewer2 which seems to imply you now tried something similar to this setup by stopping gradient flow at an appropriate point (although I think this is not exactly equivalent).
2) I tried to reproduce the simple MNIST example but using a fully connected network instead of an RNN generator without much success. Even when unrolling the discriminator for 30-40 steps the generator still engages in mode seeking behavior or does not train at all. This could either be because of a bug in my implementation or because of some peculiarities of the RNN generator or because I did not use batch normalization anywhere. If it is one of the latter two this would entail a dependence of the proposed approach on specific forms of the discriminator and generator and should be discussed. My code can be found here",1
"This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.

The paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement.

Finally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).

Some questions:
How important is the stop word modelling? What do the results look like if l_t = 0.5 for all t?

It seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work?

It is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments?

Does factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets.

Minor comments:
Below figure 2: GHz -> GB
\Gamma is not defined.",1
"This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.

The paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement.

Finally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).

Some questions:
How important is the stop word modelling? What do the results look like if l_t = 0.5 for all t?

It seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work?

It is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments?

Does factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets.

Minor comments:
Below figure 2: GHz -> GB
\Gamma is not defined.",1
"The paper presents an investigation of various neural language models designed to query context information from their recent history using an attention mechanism. The authors propose to separate the attended vectors into key, value and prediction parts. The results suggest that this helps performance. The authors also found that a simple model which which concatenates recent activation vectors performs at a similar level as the more complicated attention-based models.

The experimental methodology seems sound in general. I do have some issues with the way the dimensionality of the vectors involved in the attention-mechanism is chosen. While it’s good that the hidden layer sizes are adapted to ensure similar numbers of trainable parameters for all the models, this doesn’t control for the fact that key/value/prediction vectors of a higher dimensionality may simply work better regardless of whether their dimensions are dedicated to one particular task or used together. This separation clearly saves parameters but there could also be benefits of having some overlap of information assuming that vectors that lead to similar predictions may also be required in similar contexts for example. Some tasks may also require more dimensions than others and the explicit separation prevents the model from discovering and exploiting this. 

While memory augmented RNNs and RNNs with attention mechanisms are not new, some of these architectures had not yet been applied to language modeling. Similarly (and as acknowledged by the authors), the strategy of separating key and value functionality has been proposed before, but not in the context of natural language modeling. I’m not sure about the novelty of the proposed n-gram RNN because I recall seeing similar architectures before but I understand that novelty was not the point of that architecture as it mainly serves as a proof of the lack of ability of the more complicated architectures to do better. In that sense I do consider it an inventive baseline that could be used in future work to test the ability of other models that claim to exploit long-term dependencies. 

The exact computation of the representation h_t was initially not that clear to me (the terms hidden and output can be ambiguous at times) but besides this, the paper is quite clear and generally well-written.

The results in this paper are important because they show that learning long-term dependencies is not a solved problem by any means. The authors provide a very nice comparison to prior results and the fact that their n-gram RNN is often at least competitive with far more complicated approaches is a clear indication that some of those methods may not capture as much context information as previously thought. The success of the separation of key/value/prediction functionality in attention-based system is also noteworthy although I think this is something that needs to be investigated more thoroughly (i.e., with more control for hyperparameter choices). 


Pros:
Impressive and also interesting results.
Good comparison with earlier work.
The n-gram RNN is an interesting baseline.


Cons:
The relation between the attention-mechanism type and the number of hidden units weakens the claim that the key/value/prediction separation is the reason for the increase in performance somewhat.
The model descriptions are not entirely clear.
I would have liked to have seen what happens when the attention is applied to a much larger context size.",1
"The paper presents an investigation of various neural language models designed to query context information from their recent history using an attention mechanism. The authors propose to separate the attended vectors into key, value and prediction parts. The results suggest that this helps performance. The authors also found that a simple model which which concatenates recent activation vectors performs at a similar level as the more complicated attention-based models.

The experimental methodology seems sound in general. I do have some issues with the way the dimensionality of the vectors involved in the attention-mechanism is chosen. While it’s good that the hidden layer sizes are adapted to ensure similar numbers of trainable parameters for all the models, this doesn’t control for the fact that key/value/prediction vectors of a higher dimensionality may simply work better regardless of whether their dimensions are dedicated to one particular task or used together. This separation clearly saves parameters but there could also be benefits of having some overlap of information assuming that vectors that lead to similar predictions may also be required in similar contexts for example. Some tasks may also require more dimensions than others and the explicit separation prevents the model from discovering and exploiting this. 

While memory augmented RNNs and RNNs with attention mechanisms are not new, some of these architectures had not yet been applied to language modeling. Similarly (and as acknowledged by the authors), the strategy of separating key and value functionality has been proposed before, but not in the context of natural language modeling. I’m not sure about the novelty of the proposed n-gram RNN because I recall seeing similar architectures before but I understand that novelty was not the point of that architecture as it mainly serves as a proof of the lack of ability of the more complicated architectures to do better. In that sense I do consider it an inventive baseline that could be used in future work to test the ability of other models that claim to exploit long-term dependencies. 

The exact computation of the representation h_t was initially not that clear to me (the terms hidden and output can be ambiguous at times) but besides this, the paper is quite clear and generally well-written.

The results in this paper are important because they show that learning long-term dependencies is not a solved problem by any means. The authors provide a very nice comparison to prior results and the fact that their n-gram RNN is often at least competitive with far more complicated approaches is a clear indication that some of those methods may not capture as much context information as previously thought. The success of the separation of key/value/prediction functionality in attention-based system is also noteworthy although I think this is something that needs to be investigated more thoroughly (i.e., with more control for hyperparameter choices). 


Pros:
Impressive and also interesting results.
Good comparison with earlier work.
The n-gram RNN is an interesting baseline.


Cons:
The relation between the attention-mechanism type and the number of hidden units weakens the claim that the key/value/prediction separation is the reason for the increase in performance somewhat.
The model descriptions are not entirely clear.
I would have liked to have seen what happens when the attention is applied to a much larger context size.",1
"This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed method is better able to learn meaningful representations of sequence data.

The proposed DVBF is well motivated, and for the most part the presentation is clear. The experiments show interesting results on illustrative toy examples. I think the contribution is interesting and potentially useful, so I’d recommend acceptance.

The SVAE method of Johnson et al. (2016) deserves more discussion than the two sentences devoted to it, since the method seems pretty closely related. Like the DVBF, the SVAE imposes a Markovianity assumption, and it is able to handle similar kinds of problems. From what I understand, the most important algorithmic difference is that the SVAE q network predicts potentials, whereas the DVBF q network predicts innovations. What are the tradeoffs between the two?  Section 2.2 says they do the latter in the interest of solving control-related tasks, but I’m not clear why this follows. 

Is there a reason SVAEs don’t meet all the desiderata mentioned at the end of the Introduction?

Since the SVAE code is publicly available, one could probably compare against it in the experiments. 

I’m a bit confused about the role of uncertainty about v. In principle, one could estimate the transition parameters by maximum likelihood (i.e. fitting a point estimate of v), but this isn’t what’s done. Instead, v is integrated out as part of the marginal likelihood, which I interpret as giving the flexibility to model different dynamics for different sequences. But if this is the case, then shouldn’t the q distribution for v depend on the data, rather than being data-independent as in Eqn. (9)?",1
"This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed method is better able to learn meaningful representations of sequence data.

The proposed DVBF is well motivated, and for the most part the presentation is clear. The experiments show interesting results on illustrative toy examples. I think the contribution is interesting and potentially useful, so I’d recommend acceptance.

The SVAE method of Johnson et al. (2016) deserves more discussion than the two sentences devoted to it, since the method seems pretty closely related. Like the DVBF, the SVAE imposes a Markovianity assumption, and it is able to handle similar kinds of problems. From what I understand, the most important algorithmic difference is that the SVAE q network predicts potentials, whereas the DVBF q network predicts innovations. What are the tradeoffs between the two?  Section 2.2 says they do the latter in the interest of solving control-related tasks, but I’m not clear why this follows. 

Is there a reason SVAEs don’t meet all the desiderata mentioned at the end of the Introduction?

Since the SVAE code is publicly available, one could probably compare against it in the experiments. 

I’m a bit confused about the role of uncertainty about v. In principle, one could estimate the transition parameters by maximum likelihood (i.e. fitting a point estimate of v), but this isn’t what’s done. Instead, v is integrated out as part of the marginal likelihood, which I interpret as giving the flexibility to model different dynamics for different sequences. But if this is the case, then shouldn’t the q distribution for v depend on the data, rather than being data-independent as in Eqn. (9)?",1
"In this interesting paper the authors explore the idea of using an ensemble of multiple discriminators in generative adversarial network training. This comes with a number of benefits, mainly being able to use less powerful discriminators which may provide better training signal to the generator early on in training when strong discriminators might overpower the generator.

My main comment is about the way the paper is presented. The caption of Figure 1. and Section 3.1 suggests using the best discriminator by taking the maximum over the performance of individual ensemble members. This does not appear to be the best thing to do because we are just bound to get a training signal that is stricter than any of the individual members of the ensemble. Then the rest of the paper explores relaxing the maximum and considers various averaging techniques to obtain a ’soft-discriminator’. To me, this idea is far more appealing, and the results seem to support this, too. Skimming the paper it seems as if the authors mainly advocated always using the strongest discriminator, evidenced by my premature pre-review question earlier.

Overall, I think this paper is a valuable contribution, and I think the idea of multiple discriminators is an interesting direction to pursue.",1
"In this interesting paper the authors explore the idea of using an ensemble of multiple discriminators in generative adversarial network training. This comes with a number of benefits, mainly being able to use less powerful discriminators which may provide better training signal to the generator early on in training when strong discriminators might overpower the generator.

My main comment is about the way the paper is presented. The caption of Figure 1. and Section 3.1 suggests using the best discriminator by taking the maximum over the performance of individual ensemble members. This does not appear to be the best thing to do because we are just bound to get a training signal that is stricter than any of the individual members of the ensemble. Then the rest of the paper explores relaxing the maximum and considers various averaging techniques to obtain a ’soft-discriminator’. To me, this idea is far more appealing, and the results seem to support this, too. Skimming the paper it seems as if the authors mainly advocated always using the strongest discriminator, evidenced by my premature pre-review question earlier.

Overall, I think this paper is a valuable contribution, and I think the idea of multiple discriminators is an interesting direction to pursue.",1
"The authors show that the idea of smoothing a highly non-convex loss function can make deep neural networks easier to train.

The paper is well-written, the idea is carefully analyzed, and the experiments are convincing, so we recommend acceptance. For a stronger recommendation, it would be valuable to perform more experiments. In particular, how does your smoothing technique compare to inserting probes in various layers of the network? Another interesting question would be how it performs on hard-to-optimize tasks such as algorithm learning. For example, in the ""Neural GPU Learns Algorithms"" paper the authors had to relax the weights of different layers of their RNN to make it optimize -- could this be avoided with your smoothing technique?",1
"The authors show that the idea of smoothing a highly non-convex loss function can make deep neural networks easier to train.

The paper is well-written, the idea is carefully analyzed, and the experiments are convincing, so we recommend acceptance. For a stronger recommendation, it would be valuable to perform more experiments. In particular, how does your smoothing technique compare to inserting probes in various layers of the network? Another interesting question would be how it performs on hard-to-optimize tasks such as algorithm learning. For example, in the ""Neural GPU Learns Algorithms"" paper the authors had to relax the weights of different layers of their RNN to make it optimize -- could this be avoided with your smoothing technique?",1
"This paper extends preceding works to create a mapping between the word embedding space of two languages. The word embeddings had been independently trained on monolingual data only, and various forms of bilingual information is used to learn the mapping. This mapping is then used to measure the precision of translations.

In this paper, the authors propose two changes: ""CCA"" and ""inverted softmax"".  Looking at Table 1, CCA is only better than Dina et al in 1 out of 6 cases (It/En @1).  Most of the improvements are in fact obtained by the introduction of the inverted softmax normalization.

Overall, I wonder which aspect of this paper is really new. You mention:
 - Faruqui & Dyer 2014 already used CCA and dimensionality reduction
 - Xing et al 2015 argued already that Mikolov's linear matrix should be orthogonal

Could you make clear in what aspect your work is different from Faruqui & Dyer 2014 (other the fact that you applied the method to measure translation precision) ?

Using cognates instead of a bilingual directory is a nice trick. Please explain how you obtained this list of cognates ? Obviously, this only works for languages with the same alphabet (for instance Greek and Russian are excluded)

Also, it seems to me that in linguistics the term ""cognate"" refers to words which have a common etymological origin - they don't necessarily have the same written form (e.g. night, nuit, noche, Nacht). Maybe, you should use a different term ? Those words are probably proper names in news texts.",1
"This paper extends preceding works to create a mapping between the word embedding space of two languages. The word embeddings had been independently trained on monolingual data only, and various forms of bilingual information is used to learn the mapping. This mapping is then used to measure the precision of translations.

In this paper, the authors propose two changes: ""CCA"" and ""inverted softmax"".  Looking at Table 1, CCA is only better than Dina et al in 1 out of 6 cases (It/En @1).  Most of the improvements are in fact obtained by the introduction of the inverted softmax normalization.

Overall, I wonder which aspect of this paper is really new. You mention:
 - Faruqui & Dyer 2014 already used CCA and dimensionality reduction
 - Xing et al 2015 argued already that Mikolov's linear matrix should be orthogonal

Could you make clear in what aspect your work is different from Faruqui & Dyer 2014 (other the fact that you applied the method to measure translation precision) ?

Using cognates instead of a bilingual directory is a nice trick. Please explain how you obtained this list of cognates ? Obviously, this only works for languages with the same alphabet (for instance Greek and Russian are excluded)

Also, it seems to me that in linguistics the term ""cognate"" refers to words which have a common etymological origin - they don't necessarily have the same written form (e.g. night, nuit, noche, Nacht). Maybe, you should use a different term ? Those words are probably proper names in news texts.",1
"The authors of this work propose an interesting approach to visualizing the predictions made by a deep neural network. The manuscript is well written is provides good insight into the problem. I also appreciate the application to medical images, as simply illustrating the point on ImageNet isn't interesting enough. I do have some questions and comments.
1.  As the authors correctly point out in 3.1, approximating the conditional probability of a feature x_i by the marginal distribution p(x_i) is not realistic. They advocate for translation invariance, i.e. the position of the pixel in the image shouldn't affect the probability, and suggest that the pixels appearance depends on the small neighborhood around it. However, it is well known that global context makes an big impact on the semantics of pixels. In ""Objects in Contexts"", authors show that a given neighborhood of pixels can take different semantic meanings based on the global context in the image. In the context of deep neural networks, works such as ""ParseNet"" also illustrate the importance of global context on the spatial label distribution. This does not necessarily invalidate this approach, but is a significant limitation. It would be great if the authors provided a modification to (4) and empirically verified the change.

2. Figure 7 shows the distribution over top 3 predictions before and after softmax. It is expected that even fairly uniform distributions will transform toward delta functions after softmax normalization. Is there an additional insight here?

4. Finally, in 4.1, the authors state that it takes 30 minutes to analyze a single image with GooLeNet on a GPU? Why is this so computationally expensive? Such complexity seems to make the algorithm impractical and analyzing datasets of statistical relevance seems prohibitive.",1
"The authors of this work propose an interesting approach to visualizing the predictions made by a deep neural network. The manuscript is well written is provides good insight into the problem. I also appreciate the application to medical images, as simply illustrating the point on ImageNet isn't interesting enough. I do have some questions and comments.
1.  As the authors correctly point out in 3.1, approximating the conditional probability of a feature x_i by the marginal distribution p(x_i) is not realistic. They advocate for translation invariance, i.e. the position of the pixel in the image shouldn't affect the probability, and suggest that the pixels appearance depends on the small neighborhood around it. However, it is well known that global context makes an big impact on the semantics of pixels. In ""Objects in Contexts"", authors show that a given neighborhood of pixels can take different semantic meanings based on the global context in the image. In the context of deep neural networks, works such as ""ParseNet"" also illustrate the importance of global context on the spatial label distribution. This does not necessarily invalidate this approach, but is a significant limitation. It would be great if the authors provided a modification to (4) and empirically verified the change.

2. Figure 7 shows the distribution over top 3 predictions before and after softmax. It is expected that even fairly uniform distributions will transform toward delta functions after softmax normalization. Is there an additional insight here?

4. Finally, in 4.1, the authors state that it takes 30 minutes to analyze a single image with GooLeNet on a GPU? Why is this so computationally expensive? Such complexity seems to make the algorithm impractical and analyzing datasets of statistical relevance seems prohibitive.",1
"This submission proposes to learn the word decomposition, or word to sub-word sequence mapping jointly with the attention based sequence-to-sequence model. A particular feature of this approach is that the decomposition is not static, instead, it also conditions on the acoustic input, and the mapping is probabilistic, i.e., one word may map to multiple sub-word sequences. The authors argue that the dynamic decomposition approach can more naturally reflect the acoustic pattern. Interestingly, the motivation behind this approach is analogous to learning the pronunciation mixture model for HMM based speech recognition, where the probabilistic mapping from a word to its pronunciations also conditions on the acoustic input, e.g.,

I. McGraw, I. Badr, and J. Glass, ""Learning lexicons form speech using a pronunciation mixture model,"" in IEEE Transactions on Audio, Speech, and Language Processing, 2013

L. Lu, A. Ghoshal, S. Renals, ""Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition"", in Proc. ASRU 

R. Singh, B. Raj, and R. Stern, ""Automatic generation of subword units for speech recognition systems,""  in IEEE Transactions on Speech and Audio Processing, 2002

It would be interesting to put this work in the context by linking it to some previous works in the HMM framework.

Overall, the paper is well written, and it is theoretically convincing. The experimental study could be more solid, e.g., it is reasonable to have a word-level baseline, as the proposed approach lies in between the character-level and word-level systems. the vocabulary size of the WSJ si284 dataset is 20K at maximum, which is not very large for the softmax layer, and it is a closed vocabulary task. I guess the word-level system may be also competitive to the numbers reported in this paper. Furthermore, can you explain what is the computational bottleneck of the proposed approach? You downsampled the data by the factor of 4 using an RNN, and it still took around 5 days to converge. To me, it is a bit expensive, especially given that you only take one sample when computing the gradient. Table 2 is a little bit misleading, as CTC with language model and seq2seq with a language model model from Bahdanau et al. is much closer to the best number reported in this Table 2, while you may only get a very small improvement using a language model. Finally, ""O(5) days to converge"" sounds a bit odd to me.",1
"This submission proposes to learn the word decomposition, or word to sub-word sequence mapping jointly with the attention based sequence-to-sequence model. A particular feature of this approach is that the decomposition is not static, instead, it also conditions on the acoustic input, and the mapping is probabilistic, i.e., one word may map to multiple sub-word sequences. The authors argue that the dynamic decomposition approach can more naturally reflect the acoustic pattern. Interestingly, the motivation behind this approach is analogous to learning the pronunciation mixture model for HMM based speech recognition, where the probabilistic mapping from a word to its pronunciations also conditions on the acoustic input, e.g.,

I. McGraw, I. Badr, and J. Glass, ""Learning lexicons form speech using a pronunciation mixture model,"" in IEEE Transactions on Audio, Speech, and Language Processing, 2013

L. Lu, A. Ghoshal, S. Renals, ""Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition"", in Proc. ASRU 

R. Singh, B. Raj, and R. Stern, ""Automatic generation of subword units for speech recognition systems,""  in IEEE Transactions on Speech and Audio Processing, 2002

It would be interesting to put this work in the context by linking it to some previous works in the HMM framework.

Overall, the paper is well written, and it is theoretically convincing. The experimental study could be more solid, e.g., it is reasonable to have a word-level baseline, as the proposed approach lies in between the character-level and word-level systems. the vocabulary size of the WSJ si284 dataset is 20K at maximum, which is not very large for the softmax layer, and it is a closed vocabulary task. I guess the word-level system may be also competitive to the numbers reported in this paper. Furthermore, can you explain what is the computational bottleneck of the proposed approach? You downsampled the data by the factor of 4 using an RNN, and it still took around 5 days to converge. To me, it is a bit expensive, especially given that you only take one sample when computing the gradient. Table 2 is a little bit misleading, as CTC with language model and seq2seq with a language model model from Bahdanau et al. is much closer to the best number reported in this Table 2, while you may only get a very small improvement using a language model. Finally, ""O(5) days to converge"" sounds a bit odd to me.",1
"This paper is technically sound. It highlights well the strengths and weaknesses of the proposed simplified model.

In terms of impact, its novelty is limited, in the sense that the authors did seemingly the right thing and obtained the expected outcomes. The idea of modeling deep learning computation is not in itself particularly novel. As a companion paper to an open source release of the model, it would meet my bar of acceptance in the same vein as a paper describing a novel dataset, which might not provide groundbreaking insights, yet be generally useful to the community.

In the absence of released code, even if the authors promise to release it soon, I am more ambivalent, since that's where all the value lies. It would also be a different story if the authors had been able to use this framework to make novel architectural decisions that improved training scalability in some way, and incorporated such new insights in the paper.

UPDATED: code is now available. Revised review accordingly.",1
"This paper is technically sound. It highlights well the strengths and weaknesses of the proposed simplified model.

In terms of impact, its novelty is limited, in the sense that the authors did seemingly the right thing and obtained the expected outcomes. The idea of modeling deep learning computation is not in itself particularly novel. As a companion paper to an open source release of the model, it would meet my bar of acceptance in the same vein as a paper describing a novel dataset, which might not provide groundbreaking insights, yet be generally useful to the community.

In the absence of released code, even if the authors promise to release it soon, I am more ambivalent, since that's where all the value lies. It would also be a different story if the authors had been able to use this framework to make novel architectural decisions that improved training scalability in some way, and incorporated such new insights in the paper.

UPDATED: code is now available. Revised review accordingly.",1
"This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied.

I think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful.

That being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular:
- The notation \tilde{Q}^pi is introduced in a way that is not very clear, as ""an estimate of the Q-values"" while eq. 5 is an exact equality (no estimation)
- It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations.
- The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the ""variant of asynchronous deep Q-learning"" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods
- Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The ""mu"" distribution also comes somewhat out of nowhere.

I hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states).

Minor remarks:
- The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined)
- The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over ""a"" of a quantity that does not depend (clearly) on ""a""
- I believe 4.3 is for the tabular case but this is not clearly stated
- Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case.

Typos:
- ""we refer to the classic text Sutton & Barto (1998)"" => missing ""by""?
- ""Online policy gradient typically require an estimate of the action-values function"" => requires & value
- ""the agent generates experience from interacting the environment"" => with the environment
- in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi
- ""allowing us the spread the influence of a reward"" => to spread
- ""in the off-policy case tabular case"" => remove the first case
- ""The green line is Q-learning where at the step an update is performed"" => at each step
- In Fig. 2 it says A2C instead of A3C

NB: I did not have time to carefully read Appendix A",1
"This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied.

I think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful.

That being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular:
- The notation \tilde{Q}^pi is introduced in a way that is not very clear, as ""an estimate of the Q-values"" while eq. 5 is an exact equality (no estimation)
- It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations.
- The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the ""variant of asynchronous deep Q-learning"" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods
- Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The ""mu"" distribution also comes somewhat out of nowhere.

I hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states).

Minor remarks:
- The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined)
- The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over ""a"" of a quantity that does not depend (clearly) on ""a""
- I believe 4.3 is for the tabular case but this is not clearly stated
- Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case.

Typos:
- ""we refer to the classic text Sutton & Barto (1998)"" => missing ""by""?
- ""Online policy gradient typically require an estimate of the action-values function"" => requires & value
- ""the agent generates experience from interacting the environment"" => with the environment
- in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi
- ""allowing us the spread the influence of a reward"" => to spread
- ""in the off-policy case tabular case"" => remove the first case
- ""The green line is Q-learning where at the step an update is performed"" => at each step
- In Fig. 2 it says A2C instead of A3C

NB: I did not have time to carefully read Appendix A",1
"Building on earlier work on a model called NICE, this paper presents an approach to constructing deep feed-forward generative models. The model is evaluated on several datasets. While it does not achieve state-of-the-art performance, it advances an interesting class of models. The paper is mostly well written and clear.

Given that inference and generation are both efficient and exact, and given that this represents a main advantage over other models, it would be great if the authors could provide some motivating example applications where this is needed/would be useful.

The authors claim that “unlike both variational autoencoders and GANs, our technique is able to learn a semantically meaningful latent space which is as high dimensional as the input space.” Where is the evidence for this claim? I didn’t see any analysis of the semantic meaningfulness of the latent space learned by real NVP. Stronger evidence that the learned representations are actually useful for downstream tasks would be nice.

I still think the author’s intuitions around the “fixed reconstruction cost of L2” are very vague. The factorial Gaussian assumption itself does not limit the generative model, it merely smoothes an otherwise arbitrary distribution, and to a degree which can be arbitrarily small, p(x) = \int p(z) N(x | f(z), \sigma^2) dz. How a lose lower bound plays into this is not clear from the paper.",1
"Building on earlier work on a model called NICE, this paper presents an approach to constructing deep feed-forward generative models. The model is evaluated on several datasets. While it does not achieve state-of-the-art performance, it advances an interesting class of models. The paper is mostly well written and clear.

Given that inference and generation are both efficient and exact, and given that this represents a main advantage over other models, it would be great if the authors could provide some motivating example applications where this is needed/would be useful.

The authors claim that “unlike both variational autoencoders and GANs, our technique is able to learn a semantically meaningful latent space which is as high dimensional as the input space.” Where is the evidence for this claim? I didn’t see any analysis of the semantic meaningfulness of the latent space learned by real NVP. Stronger evidence that the learned representations are actually useful for downstream tasks would be nice.

I still think the author’s intuitions around the “fixed reconstruction cost of L2” are very vague. The factorial Gaussian assumption itself does not limit the generative model, it merely smoothes an otherwise arbitrary distribution, and to a degree which can be arbitrarily small, p(x) = \int p(z) N(x | f(z), \sigma^2) dz. How a lose lower bound plays into this is not clear from the paper.",1
"Contributions
The paper presents an adaptation of batch normalization for RNNs in the case of LSTMs, along the horizontal depth. Contrary to previous work from (Laurent 2015; Amodei 2016), the work demonstrates that batch-normalizing the hidden states of RNNs can improve optimization, and argues with quantitative experiments that the key factor to making this work is proper initialization of parameters, in particular gamma. Experiments show some gain in performance over vanilla LSTMs on Sequential MNIST, PTB, Text8 and CNN Question-Answering.

Novelty+Significance
Batch normalization has been key for training deeper and deeper networks (e.g. ResNets) and it seems natural that we would want to extend it to RNNs.  The paper shows that it is possible to do so with proper initialization of parameters, contrary to previous work from (Laurent 2015; Amodei 2016). Novelty comes from where to batch norm (i.e. not in the cell update) and in the per-time step statistics. 

Adding batch normalization to LSTMs incurs additional computational cost and bookkeeping; for training speed comparisons (e.g. Figure 2) the paper only compares LSTM and BN-LSTM by iteration count; given the additional complexity of the BN-LSTM I would have also liked to see a wall-clock comparison.

As RNNs are used across many tasks, this work is of interest to many.  However, the results gains are generally minor and require several tricks to work in practice. Also, this work doesn’t address a question about batch normalization that it seems natural that it helps with faster training, but why would it also improve generalization? 

Clarity
The paper is overall very clear and well-motivated. The model is well described and easy to understand, and the plots illustrate the points clearly.

Summary
Interesting though relatively incremental adaptation, but shows batch normalization to work for RNNs where previous works have not succeeded. Comprehensive set of experiments though it is questionable if the empirical gains are significant enough to justify the increased model complexity as well as computational overhead.

Pros
- Shows batch normalization to work for RNNs where previous works have not succeeded
- Good empirical analysis of hyper-parameter choices and of the activations
- Experiments on multiple tasks
- Clarity

Cons
- Relatively incremental
- Several ‘hacks’ for the method (per-time step statistics, adding noise for exploding variance, sequence-wise normalization)
- No mention of computational overhead
- Only character or pixel-level tasks, what about word-level?",1
"Contributions
The paper presents an adaptation of batch normalization for RNNs in the case of LSTMs, along the horizontal depth. Contrary to previous work from (Laurent 2015; Amodei 2016), the work demonstrates that batch-normalizing the hidden states of RNNs can improve optimization, and argues with quantitative experiments that the key factor to making this work is proper initialization of parameters, in particular gamma. Experiments show some gain in performance over vanilla LSTMs on Sequential MNIST, PTB, Text8 and CNN Question-Answering.

Novelty+Significance
Batch normalization has been key for training deeper and deeper networks (e.g. ResNets) and it seems natural that we would want to extend it to RNNs.  The paper shows that it is possible to do so with proper initialization of parameters, contrary to previous work from (Laurent 2015; Amodei 2016). Novelty comes from where to batch norm (i.e. not in the cell update) and in the per-time step statistics. 

Adding batch normalization to LSTMs incurs additional computational cost and bookkeeping; for training speed comparisons (e.g. Figure 2) the paper only compares LSTM and BN-LSTM by iteration count; given the additional complexity of the BN-LSTM I would have also liked to see a wall-clock comparison.

As RNNs are used across many tasks, this work is of interest to many.  However, the results gains are generally minor and require several tricks to work in practice. Also, this work doesn’t address a question about batch normalization that it seems natural that it helps with faster training, but why would it also improve generalization? 

Clarity
The paper is overall very clear and well-motivated. The model is well described and easy to understand, and the plots illustrate the points clearly.

Summary
Interesting though relatively incremental adaptation, but shows batch normalization to work for RNNs where previous works have not succeeded. Comprehensive set of experiments though it is questionable if the empirical gains are significant enough to justify the increased model complexity as well as computational overhead.

Pros
- Shows batch normalization to work for RNNs where previous works have not succeeded
- Good empirical analysis of hyper-parameter choices and of the activations
- Experiments on multiple tasks
- Clarity

Cons
- Relatively incremental
- Several ‘hacks’ for the method (per-time step statistics, adding noise for exploding variance, sequence-wise normalization)
- No mention of computational overhead
- Only character or pixel-level tasks, what about word-level?",1
"This paper describes a way to speed up convergence through sudden increases of otherwise monotonically decreasing learning rates. Several techniques are presented in a clear way and parameterized method is proposed and evaluated on the CIFAR task. The concept is easy to understand and the authors chose state-of-the-art models to show the performance of their algorithm. The relevance of these results goes beyond image classification.


Pros:

- Simple and effective method to improve convergence
- Good evaluation on well known database


Cons:

- Connection of introduction and topic of the paper is a bit unclear
- Fig 2, 4 and 5 are hard to read. Lines are out of bounds and maybe only the best setting for T_0 and T_mult would be clearer. The baseline also doesn't seem to converge

Remarks:
An loss surface for T_0 against T_mult would be very helpful. Also understanding the relationship of network depth and the performance of this method would add value to this analysis.",1
"This paper describes a way to speed up convergence through sudden increases of otherwise monotonically decreasing learning rates. Several techniques are presented in a clear way and parameterized method is proposed and evaluated on the CIFAR task. The concept is easy to understand and the authors chose state-of-the-art models to show the performance of their algorithm. The relevance of these results goes beyond image classification.


Pros:

- Simple and effective method to improve convergence
- Good evaluation on well known database


Cons:

- Connection of introduction and topic of the paper is a bit unclear
- Fig 2, 4 and 5 are hard to read. Lines are out of bounds and maybe only the best setting for T_0 and T_mult would be clearer. The baseline also doesn't seem to converge

Remarks:
An loss surface for T_0 against T_mult would be very helpful. Also understanding the relationship of network depth and the performance of this method would add value to this analysis.",1
"This paper proposes a weakly supervised, end-to-end neural network model for solving a challenging natural language understanding task. 
As an extension of the Neural Programmer, this work aims at overcoming the ambiguities imposed by natural language. 
By predefining a set of operations, the model is able to learn the interface between the language reasoning and answer composition using backpropagation. 
On the WikiTableQuestions dataset, it is able to achieve a slightly better performance than the traditional semantic parser methods. 

Overall, this is a very interesting and promising work as it involves a lot of real-world challenges about natural language understanding. 
The intuitions and design of the model are very clear, but the complication makes the paper a bit difficult to read, which means the model is also difficult to be reimplemented. I would expect to see more details about model ablation and it would help us figure out the prominent parts of the model design.",1
"This paper proposes a weakly supervised, end-to-end neural network model for solving a challenging natural language understanding task. 
As an extension of the Neural Programmer, this work aims at overcoming the ambiguities imposed by natural language. 
By predefining a set of operations, the model is able to learn the interface between the language reasoning and answer composition using backpropagation. 
On the WikiTableQuestions dataset, it is able to achieve a slightly better performance than the traditional semantic parser methods. 

Overall, this is a very interesting and promising work as it involves a lot of real-world challenges about natural language understanding. 
The intuitions and design of the model are very clear, but the complication makes the paper a bit difficult to read, which means the model is also difficult to be reimplemented. I would expect to see more details about model ablation and it would help us figure out the prominent parts of the model design.",1
"This paper introduces a 'GPU-friendly' variant of A3C which relaxes some synchronicity constraints in the original A3C algorithm to make it more friendly to a high-throughput GPU device. The analysis of the effects of this added latency is thorough. The systems analysis of the algorithm is extensive. 

One caveat is that the performance figures in Table 3 are hard to compare since the protocols vary so much. I understand that DeepMind didn't provide reproducible code for A3C, but I gather from the comment that the authors have re-implemented vanilla A3C as well, in which case it would be good to show what this reimplementation of A3C achieves in the same setting used by DM, and in the setting of the experiment conducted using GA3C (1 day). It would be good to clarify in the text that the experimental protocol differed (top 5 out of 50 vs single run), and clarify why the discrepancy, even if the answer is that the authors didn't have time / resources to reproduce the same protocol. A bit more care would go a long way to establishing that indeed, there is no price to pay for the approximations that were made.

I applaud the authors for open-sourcing the code, especially since there is a relative shortage of properly tested open-source implementations in that general area, and getting these algorithms right is non-trivial.

A disclaimer: having never implemented A3C myself, I have a low confidence in my ability to appropriately assess of the algorithmic aspects of the work.",1
"This paper introduces a 'GPU-friendly' variant of A3C which relaxes some synchronicity constraints in the original A3C algorithm to make it more friendly to a high-throughput GPU device. The analysis of the effects of this added latency is thorough. The systems analysis of the algorithm is extensive. 

One caveat is that the performance figures in Table 3 are hard to compare since the protocols vary so much. I understand that DeepMind didn't provide reproducible code for A3C, but I gather from the comment that the authors have re-implemented vanilla A3C as well, in which case it would be good to show what this reimplementation of A3C achieves in the same setting used by DM, and in the setting of the experiment conducted using GA3C (1 day). It would be good to clarify in the text that the experimental protocol differed (top 5 out of 50 vs single run), and clarify why the discrepancy, even if the answer is that the authors didn't have time / resources to reproduce the same protocol. A bit more care would go a long way to establishing that indeed, there is no price to pay for the approximations that were made.

I applaud the authors for open-sourcing the code, especially since there is a relative shortage of properly tested open-source implementations in that general area, and getting these algorithms right is non-trivial.

A disclaimer: having never implemented A3C myself, I have a low confidence in my ability to appropriately assess of the algorithmic aspects of the work.",1
"This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks.

The paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general.  As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.",1
"This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks.

The paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general.  As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.",1
"This paper presents an approach to learn to generate programs. Instead of directly trying to generate the program, the authors propose to train a neural net to estimate a fix set of attributes, which then condition a search procedure. This is an interesting approach, which make sense, as building a generative model of programs is a very complex task.

Faster computation times are shown in the experimental section with respect to baselines including DFS, Enumeration, etc. in a setup with very small programs of length up to 5 instructions have to be found. 
It is not clear to me how the proposed approach scales to larger programs, where perhaps many attributes will be on. Is there still an advantage?

The authors use as metric the time to find a single program, whose execution will result in the set of 5 input-output pairs given as input. However, as mentioned in the paper, one is not after a generic program but after the best program, or a rank list of all programs (or top-k programs) that result in a correct execution.
Could the authors show experiments in this setting? would still be useful to have the proposed approach? what would the challenges be in this more realistic scenario?

In the second experiment the authors show results where the length of the program at training time is different than the length at test time. However, the results are shown when only 20% of the programs are finished. Could you show results for finding all programs? 

The paper is missing an analysis of the results. What type of programs are difficult? how often is the NNet wrong? how does this affect speed? what are the failure modes of the proposed method?

The authors proposed to have a fix-length representation of the each input-output pair, and then use average pooling to get the final representation. However, why would average pooling make sense here? would it make more sense to combine the predictions at the decoder, not the encoder?

Learning from only 5 executions seems very difficult to me. For programs so small it might be ok, but going to more difficult and longer programs this setting does not seem reasonable. 

In summary an interesting paper. This paper tackles a problem that is outside my area of expertise so I might have miss something important.",1
"This paper presents an approach to learn to generate programs. Instead of directly trying to generate the program, the authors propose to train a neural net to estimate a fix set of attributes, which then condition a search procedure. This is an interesting approach, which make sense, as building a generative model of programs is a very complex task.

Faster computation times are shown in the experimental section with respect to baselines including DFS, Enumeration, etc. in a setup with very small programs of length up to 5 instructions have to be found. 
It is not clear to me how the proposed approach scales to larger programs, where perhaps many attributes will be on. Is there still an advantage?

The authors use as metric the time to find a single program, whose execution will result in the set of 5 input-output pairs given as input. However, as mentioned in the paper, one is not after a generic program but after the best program, or a rank list of all programs (or top-k programs) that result in a correct execution.
Could the authors show experiments in this setting? would still be useful to have the proposed approach? what would the challenges be in this more realistic scenario?

In the second experiment the authors show results where the length of the program at training time is different than the length at test time. However, the results are shown when only 20% of the programs are finished. Could you show results for finding all programs? 

The paper is missing an analysis of the results. What type of programs are difficult? how often is the NNet wrong? how does this affect speed? what are the failure modes of the proposed method?

The authors proposed to have a fix-length representation of the each input-output pair, and then use average pooling to get the final representation. However, why would average pooling make sense here? would it make more sense to combine the predictions at the decoder, not the encoder?

Learning from only 5 executions seems very difficult to me. For programs so small it might be ok, but going to more difficult and longer programs this setting does not seem reasonable. 

In summary an interesting paper. This paper tackles a problem that is outside my area of expertise so I might have miss something important.",1
"This paper considers the problem of model-based policy search. The authors 
consider the use of Bayesian Neural Networks to learn a model of the environment
and advocate for the $\alpha$-divergence minimization rather than the more usual 
variational Bayes. 

The ability of alpha-divergence to capture bi-modality however 
comes at a price and most of the paper is devoted to finding tractable approximations. 
The authors therefore use the approach of Hernandez-Lobato
et al. (2016) as proxy to the alpha-divergence . 

The environment/system dynamics is clearly defined as a well as the policy parametrization 
(section 3) and would constitute a useful reference point for other researchers. 
Simulated roll-outs, using the learned model, then provide samples of the expected 
return. Since a model of the environment is available, stochastic gradient descent 
can be performed in the usual way, without policy gradient estimators, via automatic 
differentiation tools. 

The experiments demonstrate that alpha-divergence is capable of capturing multi-model 
structure which competing methods (variational Bayes and GP) would otherwise
struggle with. The proposed approach also compares favorably in a real-world
batch setting.

The paper is well-written, technically rich and combines many recent tools 
into a coherent algorithm. However, the repeated use of approximations to original 
quantities seems to somehow defeat the benefits of the original problem formulation. 
The scalability and computational effectiveness of this approach is also questionable 
and I am uncertain if many problem would warrant such complexity in their solution. 
As with other Bayesian methods, the proposed approach would probably shine in low-samples 
regime and in this case might be preferable to other methods in the same class (VB, GP).",1
"This paper considers the problem of model-based policy search. The authors 
consider the use of Bayesian Neural Networks to learn a model of the environment
and advocate for the $\alpha$-divergence minimization rather than the more usual 
variational Bayes. 

The ability of alpha-divergence to capture bi-modality however 
comes at a price and most of the paper is devoted to finding tractable approximations. 
The authors therefore use the approach of Hernandez-Lobato
et al. (2016) as proxy to the alpha-divergence . 

The environment/system dynamics is clearly defined as a well as the policy parametrization 
(section 3) and would constitute a useful reference point for other researchers. 
Simulated roll-outs, using the learned model, then provide samples of the expected 
return. Since a model of the environment is available, stochastic gradient descent 
can be performed in the usual way, without policy gradient estimators, via automatic 
differentiation tools. 

The experiments demonstrate that alpha-divergence is capable of capturing multi-model 
structure which competing methods (variational Bayes and GP) would otherwise
struggle with. The proposed approach also compares favorably in a real-world
batch setting.

The paper is well-written, technically rich and combines many recent tools 
into a coherent algorithm. However, the repeated use of approximations to original 
quantities seems to somehow defeat the benefits of the original problem formulation. 
The scalability and computational effectiveness of this approach is also questionable 
and I am uncertain if many problem would warrant such complexity in their solution. 
As with other Bayesian methods, the proposed approach would probably shine in low-samples 
regime and in this case might be preferable to other methods in the same class (VB, GP).",1
"Thank you for an interesting read. I personally like the information bottleneck principle and am very happy to see its application to deep neural networks. To my knowledge, this is the first paper that applies IB to train deep networks (the original papers only presented the concept), but see below for the note of independent work claim. 

The derivation of the variational lowerbound is very clear, even for those who are not very familiar with variational inference. Also the explanation of the IB principle is clear. Experimental results seem to be very promising.

I found the presentation for the model a bit confusing. In variational inference/information maximisation, p usually denotes the model and q represents the ""inference engine"". This means the choice of inference method is independent to the modelling procedure. However the presented VIB assumed p(x, y) as the **underlying data distribution** (and approximated by the empirical distribution), thus here the model is actually q(y|z)p(z|x). Then the authors presented p(y|x) as the **predictive distribution** in page 8, paragraph 2 of section 4.2.3. Predictive in what sense? I guess you meant p(y|x) = \int q(y|z) p(z|x) dz in this case, but this makes the two definitions contradict to each other!

The authors have made an interesting connection to variational auto-encoder and the warm-up training (by tuning beta). However, even when the loss function formula is the same to the variational lowerbound used in VAE (in this case beta = 1), the underlying model is different! For example, r(z) in VIB is the variational approximation to p(z) (which means r(z) is not a component in the model), while in VAE it is the prior distribution which is actually defined in the modelling procedure. Similaly p(z|x) in VIB is included in the model, while in VAE that is the approximate posterior and can be independently chosen (e.g. you can use p(x|z) as a deep NN but p(z|x) as a deep NN or a Gaussian process).

In summary, I think the presentation for the modelling procedure is unclear. I hope these point would be made clearer in revision since the current presentation makes me uncomfortable as a Bayesian person. In the VAE part, it's better to clearly mention the difference between VIB and VAE, and provide some intuitions if the VIB interpretation is preferred.


Typos:
Eq. 9-11: did you mean q(y|z) instead of q(z|y)?
Fig 2 ""as beta becomes smaller"": did you mean ""larger""?

**claim for independent work**
The authors claimed that the manuscript presented an independent work to Chalk et al. 2016 which is online since May 2016. It seems to me that nowadays deep learning research is very competitve that many people publish the same idea at the same time. So I would trust this claim and commend the authors' honesty, but in case this is not true, I would not recommend the manuscript for acceptance.",1
"Thank you for an interesting read. I personally like the information bottleneck principle and am very happy to see its application to deep neural networks. To my knowledge, this is the first paper that applies IB to train deep networks (the original papers only presented the concept), but see below for the note of independent work claim. 

The derivation of the variational lowerbound is very clear, even for those who are not very familiar with variational inference. Also the explanation of the IB principle is clear. Experimental results seem to be very promising.

I found the presentation for the model a bit confusing. In variational inference/information maximisation, p usually denotes the model and q represents the ""inference engine"". This means the choice of inference method is independent to the modelling procedure. However the presented VIB assumed p(x, y) as the **underlying data distribution** (and approximated by the empirical distribution), thus here the model is actually q(y|z)p(z|x). Then the authors presented p(y|x) as the **predictive distribution** in page 8, paragraph 2 of section 4.2.3. Predictive in what sense? I guess you meant p(y|x) = \int q(y|z) p(z|x) dz in this case, but this makes the two definitions contradict to each other!

The authors have made an interesting connection to variational auto-encoder and the warm-up training (by tuning beta). However, even when the loss function formula is the same to the variational lowerbound used in VAE (in this case beta = 1), the underlying model is different! For example, r(z) in VIB is the variational approximation to p(z) (which means r(z) is not a component in the model), while in VAE it is the prior distribution which is actually defined in the modelling procedure. Similaly p(z|x) in VIB is included in the model, while in VAE that is the approximate posterior and can be independently chosen (e.g. you can use p(x|z) as a deep NN but p(z|x) as a deep NN or a Gaussian process).

In summary, I think the presentation for the modelling procedure is unclear. I hope these point would be made clearer in revision since the current presentation makes me uncomfortable as a Bayesian person. In the VAE part, it's better to clearly mention the difference between VIB and VAE, and provide some intuitions if the VIB interpretation is preferred.


Typos:
Eq. 9-11: did you mean q(y|z) instead of q(z|y)?
Fig 2 ""as beta becomes smaller"": did you mean ""larger""?

**claim for independent work**
The authors claimed that the manuscript presented an independent work to Chalk et al. 2016 which is online since May 2016. It seems to me that nowadays deep learning research is very competitve that many people publish the same idea at the same time. So I would trust this claim and commend the authors' honesty, but in case this is not true, I would not recommend the manuscript for acceptance.",1
"This paper proposes the neural noisy channel model, P(x|y), where (x, y) is a input-to-out sequence pair,  based on the authors' previous work on segment to segment neural transduction (SSNT) model. For the noisy channel model, the key difference from sequence-to-sequence is that the complete sequence y is not observed beforehand. SSNT handles this problem elegantly by performing incremental alignment and prediction. However, this paper does not present anything that is particular novel on top of the SSNT. The SSNT model is still applicable by reverting the input and output sequences. The authors said that an unidirectional LSTM has to be used as an encoder instead of the bidirectional LSTM, but I think the difference is minor. The decoding algorithm presented in the appendix is relatively new. 

The experimental study is very comprehensive and strong, however, there is one important baseline number that is missing for all the experiments. Can you give the number that uses direct + LM + bias, and if you can give direct + bias number would be even better. Although using a LM for the direct model does not make a lot of sense mathematically, however, it works pretty well in practice, and the LM can rescore and smooth your predictions, see 

Deep Speech 2: End-to-End Speech Recognition in English and Mandarin

from Baidu for example. I think the LM may be also the key to explain why noisy channel is much better than direct model in Table 3. A couple minor questions are

1. it is not very clear to me is your direct model in the experiments SSNT or sequence-to-sequence model?

2. O(|x|^2*|y|) training complexity is OK, but it would be great to further cut down the computational cost, as it is still very expensive for long input sequences, for example, for paragraph or document level modeling, or speech sequences. 

The paper is well written, and overall, it is still an interesting paper, as the channel model is always of great interest to the general public.",1
"This paper proposes the neural noisy channel model, P(x|y), where (x, y) is a input-to-out sequence pair,  based on the authors' previous work on segment to segment neural transduction (SSNT) model. For the noisy channel model, the key difference from sequence-to-sequence is that the complete sequence y is not observed beforehand. SSNT handles this problem elegantly by performing incremental alignment and prediction. However, this paper does not present anything that is particular novel on top of the SSNT. The SSNT model is still applicable by reverting the input and output sequences. The authors said that an unidirectional LSTM has to be used as an encoder instead of the bidirectional LSTM, but I think the difference is minor. The decoding algorithm presented in the appendix is relatively new. 

The experimental study is very comprehensive and strong, however, there is one important baseline number that is missing for all the experiments. Can you give the number that uses direct + LM + bias, and if you can give direct + bias number would be even better. Although using a LM for the direct model does not make a lot of sense mathematically, however, it works pretty well in practice, and the LM can rescore and smooth your predictions, see 

Deep Speech 2: End-to-End Speech Recognition in English and Mandarin

from Baidu for example. I think the LM may be also the key to explain why noisy channel is much better than direct model in Table 3. A couple minor questions are

1. it is not very clear to me is your direct model in the experiments SSNT or sequence-to-sequence model?

2. O(|x|^2*|y|) training complexity is OK, but it would be great to further cut down the computational cost, as it is still very expensive for long input sequences, for example, for paragraph or document level modeling, or speech sequences. 

The paper is well written, and overall, it is still an interesting paper, as the channel model is always of great interest to the general public.",1
"This paper proposes a novel method for extracting rule-based classifiers from trained LSTM models. The proposed method is applied to a factoid question-answering task, where it is demonstrated that the extracted rules perform comparatively to the original LSTM. The analysis of the extracted rules illustrate the features the LSTM model picks up on.

Analyzing and visualizing the computations carried out by RNNs in order to understand the functions they compute is an important direction of research. This sort of analysis will help us understand the pitfalls of RNNs, and how we can improve them. Although the approach taken is relatively inflexible - each rule is defined as an ordered sequence of words - the authors experiment with three different scores for picking salient words (state-difference, cell-difference and gradient) and their approach yields comparable performance, which suggests that the extracted rules mimic the RNN closely. The results are also somewhat surprising, since most of the rules consist only of two or three words.

It would have been interesting to try extend the approach on other natural language processing tasks, such as machine translation. Presumably the rules learned here will be quite different.

Other comments:
- Eq. (12) is over-parametrized with two vectors $P$ and $Q$. The same function can be computed with a single vector. This becomes clear when you divide both the numerator and denominator by $e^{P h_t}$.
- Section 4.1. Is it correct that this section is focused on the forward LSTM? If so, please clarify it in the text.
- In Eq. (13), define $c_0 = 0$.
- Eq. (13) is exactly the same as Eq. (15). Is there a mistake?
- In Table 1, third column should have word ""film"" highlighted.
- ""are shown in 2"" -> ""are shown in Table 2"".
- Since there are some problems representing numbers, it may help to replace each digit with the hashtag symbol #.",1
"This paper proposes a novel method for extracting rule-based classifiers from trained LSTM models. The proposed method is applied to a factoid question-answering task, where it is demonstrated that the extracted rules perform comparatively to the original LSTM. The analysis of the extracted rules illustrate the features the LSTM model picks up on.

Analyzing and visualizing the computations carried out by RNNs in order to understand the functions they compute is an important direction of research. This sort of analysis will help us understand the pitfalls of RNNs, and how we can improve them. Although the approach taken is relatively inflexible - each rule is defined as an ordered sequence of words - the authors experiment with three different scores for picking salient words (state-difference, cell-difference and gradient) and their approach yields comparable performance, which suggests that the extracted rules mimic the RNN closely. The results are also somewhat surprising, since most of the rules consist only of two or three words.

It would have been interesting to try extend the approach on other natural language processing tasks, such as machine translation. Presumably the rules learned here will be quite different.

Other comments:
- Eq. (12) is over-parametrized with two vectors $P$ and $Q$. The same function can be computed with a single vector. This becomes clear when you divide both the numerator and denominator by $e^{P h_t}$.
- Section 4.1. Is it correct that this section is focused on the forward LSTM? If so, please clarify it in the text.
- In Eq. (13), define $c_0 = 0$.
- Eq. (13) is exactly the same as Eq. (15). Is there a mistake?
- In Table 1, third column should have word ""film"" highlighted.
- ""are shown in 2"" -> ""are shown in Table 2"".
- Since there are some problems representing numbers, it may help to replace each digit with the hashtag symbol #.",1
"SUMMARY: This paper describes a set of experiments evaluating techniques for
training a dialogue agent via reinforcement learning. A
standard memory network architecture is trained on both bAbI and a version of
the WikiMovies dataset (as in Weston 2016, which this work extends). Numerous
experiments are performed comparing the behavior of different training
algorithms under various experimental conditions.

STRENGTHS: The experimentation is comprehensive. I agree with the authors that
these results provide additional useful insight into the performance of the
model in the 2016 paper (henceforth W16).

WEAKNESSES: This is essentially an appendix to the earlier paper. There is no
new machine learning content. Secondarily, the paper seems to confuse the
distinction between ""training with an adaptive sampling procedure"" and ""training
in interactive environments"" more generally. In particular, no comparisons are
presented to the to the experiments with a static exploration policy presented
in W16, when the two training can & should be evaluated side-by-side.
The only meaningful changes between this work and W16 involve simple
(and already well-studied) changes to the form of this exploration policy.

My primary concern remains about novelty: the extra data introduced here is
welcome enough, but probably belongs in a *ACL short paper or a technical
report. This work does not stand on its own, and an ICLR submission is not an
appropriate vehicle for presenting it.

""REINFORCEMENT LEARNING""

[Update: concerns in this section have been addressed by the authors.]

This paper attempts to make a hard distinction between the reinforcement
learning condition considered here and the (""non-RL"") condition considered in
W16. I don't think this distinction is nearly as sharp as it's
made out to be. 

As already noted in Weston 2016, the RBI objective is a special case of vanilla
policy gradient with a zero baseline and off-policy samples. In this sense the
version of RBI considered in this paper is the same as in W16, but with a
different exploration policy; REINFORCE is the same objective with a nontrivial
baseline. Similarly, the change in FP is only a change to the sampling policy.
The fixed dataset / online learning distinction is not especially meaningful
when the fixed dataset consists of endless synthetic data.

It should be noted that some variants of the exploration policy in W16 provide a
stronger training signal than is available in the RL ""from scratch"" setting
here: in particular, when $\pi_acc = 0.5$ the training samples will feature much
denser reward. However, if I correctly understand Figures 3 and 4 in this paper,
the completely random initial policy achieves an average reward of ~0.3 on bAbI
and ~0.1 on movies---as good or better than the other exploration policies in
W16!

I think this paper would be a lot clearer if the delta from W16 were expressed
directly in terms of their different exploration policies, rather than trying to
cast all of the previous work as ""not RL"" when it can be straightforwardly
accommodated in the RL framework.

I was quite confused by the fact that no direct comparisons are made to the
training conditions in the earlier work. I think this is a symptom of the
problem discussed above: once this paper adopts the position that this work is
about RL and the previous work is not, it becomes possible to declare that the
two training scenarios are incomparable. I really think this is a mistake---to
the extent that the off-policy sample generators used in the previous paper are
worse than chance, it is always possible to compare to them fairly here.
Evaluating everything in the ""online"" setting and presenting side-by-side
experiments would provide a much more informative picture of the comparative
behavior of the various training objectives.

ON-POLICY VS OFF-POLICY

Vanilla policy gradient methods like the ones here typically can't use
off-policy samples without a little extra hand-holding (importance sampling,
trust region methods, etc.). They seem to work out of the box for a few of the
experiments in this paper, which is an interesting result on its own. It would
be nice to have some discussion of why that might be the case.

OTHER NOTES

- The claim that ""batch size is related to off-policy learning"" is a little
odd. There are lots of on-policy algorithms that require the agent to collect a
large batch of transitions from the current policy before performing an 
(on-policy) update.

- I think the experiments on fine-tuning to human workers are the most exciting
part of this work, and I would have preferred to see these discussed (and
explored with) in much more detail rather than being relegated to the
penultimate paragraphs.",1
"SUMMARY: This paper describes a set of experiments evaluating techniques for
training a dialogue agent via reinforcement learning. A
standard memory network architecture is trained on both bAbI and a version of
the WikiMovies dataset (as in Weston 2016, which this work extends). Numerous
experiments are performed comparing the behavior of different training
algorithms under various experimental conditions.

STRENGTHS: The experimentation is comprehensive. I agree with the authors that
these results provide additional useful insight into the performance of the
model in the 2016 paper (henceforth W16).

WEAKNESSES: This is essentially an appendix to the earlier paper. There is no
new machine learning content. Secondarily, the paper seems to confuse the
distinction between ""training with an adaptive sampling procedure"" and ""training
in interactive environments"" more generally. In particular, no comparisons are
presented to the to the experiments with a static exploration policy presented
in W16, when the two training can & should be evaluated side-by-side.
The only meaningful changes between this work and W16 involve simple
(and already well-studied) changes to the form of this exploration policy.

My primary concern remains about novelty: the extra data introduced here is
welcome enough, but probably belongs in a *ACL short paper or a technical
report. This work does not stand on its own, and an ICLR submission is not an
appropriate vehicle for presenting it.

""REINFORCEMENT LEARNING""

[Update: concerns in this section have been addressed by the authors.]

This paper attempts to make a hard distinction between the reinforcement
learning condition considered here and the (""non-RL"") condition considered in
W16. I don't think this distinction is nearly as sharp as it's
made out to be. 

As already noted in Weston 2016, the RBI objective is a special case of vanilla
policy gradient with a zero baseline and off-policy samples. In this sense the
version of RBI considered in this paper is the same as in W16, but with a
different exploration policy; REINFORCE is the same objective with a nontrivial
baseline. Similarly, the change in FP is only a change to the sampling policy.
The fixed dataset / online learning distinction is not especially meaningful
when the fixed dataset consists of endless synthetic data.

It should be noted that some variants of the exploration policy in W16 provide a
stronger training signal than is available in the RL ""from scratch"" setting
here: in particular, when $\pi_acc = 0.5$ the training samples will feature much
denser reward. However, if I correctly understand Figures 3 and 4 in this paper,
the completely random initial policy achieves an average reward of ~0.3 on bAbI
and ~0.1 on movies---as good or better than the other exploration policies in
W16!

I think this paper would be a lot clearer if the delta from W16 were expressed
directly in terms of their different exploration policies, rather than trying to
cast all of the previous work as ""not RL"" when it can be straightforwardly
accommodated in the RL framework.

I was quite confused by the fact that no direct comparisons are made to the
training conditions in the earlier work. I think this is a symptom of the
problem discussed above: once this paper adopts the position that this work is
about RL and the previous work is not, it becomes possible to declare that the
two training scenarios are incomparable. I really think this is a mistake---to
the extent that the off-policy sample generators used in the previous paper are
worse than chance, it is always possible to compare to them fairly here.
Evaluating everything in the ""online"" setting and presenting side-by-side
experiments would provide a much more informative picture of the comparative
behavior of the various training objectives.

ON-POLICY VS OFF-POLICY

Vanilla policy gradient methods like the ones here typically can't use
off-policy samples without a little extra hand-holding (importance sampling,
trust region methods, etc.). They seem to work out of the box for a few of the
experiments in this paper, which is an interesting result on its own. It would
be nice to have some discussion of why that might be the case.

OTHER NOTES

- The claim that ""batch size is related to off-policy learning"" is a little
odd. There are lots of on-policy algorithms that require the agent to collect a
large batch of transitions from the current policy before performing an 
(on-policy) update.

- I think the experiments on fine-tuning to human workers are the most exciting
part of this work, and I would have preferred to see these discussed (and
explored with) in much more detail rather than being relegated to the
penultimate paragraphs.",1
"After reading the rebuttal, I decided to increase my score. I think ALI somehow stabilizes the GAN training as demonstrated in Fig. 8 and learns a reasonable inference network.

---------------
Initial Review:

This paper proposes a new method for learning an inference network in the GAN framework. ALI's objective is to match the joint distribution of hidden and visible units imposed by an encoder and decoder network. ALI is trained on multiple datasets, and it seems to have a good reconstruction even though it does not have an explicit reconstruction term in the cost function. This shows it is learning a decent inference network for GAN.

There are currently many ways to learn an inference network for GANs: One can learn an inference network after training the GAN by sampling from the GAN and learning a separate network to map X to Z. There is also the infoGAN approach (not cited) which trains the inference network at the same time with the generative path. I think this paper should have an extensive comparison with these other methods and have a discussion for why ALI's inference network is superior to previous works.

Since ALI's inference network is stochastic, it would be great if different reconstructions of a same image is included. I believe the inference network of the BiGAN paper is deterministic which is the main difference with this work. So maybe it is worth highlighting this difference.

The quality of samples is very good, but there is no quantitative experiment to compare ALI's samples with other GAN variants. So I am not sure if learning an inference network has contributed to better generative samples. Maybe including an inception score for comparison can help.

There are two sets of semi-supervised results: 
The first one concatenate the hidden layers of the inference network and uses an L2-SVM afterwards. Ideally, concatenating feature maps is not the best way for semi-supervised learning and one would want to train the semi-supervised path at the same time with the generative path. It would have been much more interesting if part of the hidden code was a categorical distribution and another part of it was a continuous distribution like Gaussian, and the inference network on the categorical latent variable was used directly for classification (like semi-supervised VAE). In this case, the inference network would be trained at the same time with the generative path. Also if the authors can show that ALI can disentangle factors of variations with a discrete latent variable like infoGAN, it will significantly improve the quality of the paper.

The second semi-supervised learning results show that ALI can match the state-of-the-art. But my impression is that the significant gain is mainly coming from the adaptation of Salimans et al. (2016) in which the discriminator is used for classification. It is unclear to me why learning an inference network help the discriminator do a better job in classification. How do we know the proposed method is improving the stability of the GAN? My understanding is that one of the main points of learning an inference network is to learn a mapping from the image to the high-level features such as class labels. So it would have been more interesting if the inference path was directly used for semi-supervised learning as I explained above.",1
"After reading the rebuttal, I decided to increase my score. I think ALI somehow stabilizes the GAN training as demonstrated in Fig. 8 and learns a reasonable inference network.

---------------
Initial Review:

This paper proposes a new method for learning an inference network in the GAN framework. ALI's objective is to match the joint distribution of hidden and visible units imposed by an encoder and decoder network. ALI is trained on multiple datasets, and it seems to have a good reconstruction even though it does not have an explicit reconstruction term in the cost function. This shows it is learning a decent inference network for GAN.

There are currently many ways to learn an inference network for GANs: One can learn an inference network after training the GAN by sampling from the GAN and learning a separate network to map X to Z. There is also the infoGAN approach (not cited) which trains the inference network at the same time with the generative path. I think this paper should have an extensive comparison with these other methods and have a discussion for why ALI's inference network is superior to previous works.

Since ALI's inference network is stochastic, it would be great if different reconstructions of a same image is included. I believe the inference network of the BiGAN paper is deterministic which is the main difference with this work. So maybe it is worth highlighting this difference.

The quality of samples is very good, but there is no quantitative experiment to compare ALI's samples with other GAN variants. So I am not sure if learning an inference network has contributed to better generative samples. Maybe including an inception score for comparison can help.

There are two sets of semi-supervised results: 
The first one concatenate the hidden layers of the inference network and uses an L2-SVM afterwards. Ideally, concatenating feature maps is not the best way for semi-supervised learning and one would want to train the semi-supervised path at the same time with the generative path. It would have been much more interesting if part of the hidden code was a categorical distribution and another part of it was a continuous distribution like Gaussian, and the inference network on the categorical latent variable was used directly for classification (like semi-supervised VAE). In this case, the inference network would be trained at the same time with the generative path. Also if the authors can show that ALI can disentangle factors of variations with a discrete latent variable like infoGAN, it will significantly improve the quality of the paper.

The second semi-supervised learning results show that ALI can match the state-of-the-art. But my impression is that the significant gain is mainly coming from the adaptation of Salimans et al. (2016) in which the discriminator is used for classification. It is unclear to me why learning an inference network help the discriminator do a better job in classification. How do we know the proposed method is improving the stability of the GAN? My understanding is that one of the main points of learning an inference network is to learn a mapping from the image to the high-level features such as class labels. So it would have been more interesting if the inference path was directly used for semi-supervised learning as I explained above.",1
"This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent's ability to learn from user feedback. For solving these tasks, the paper uses memory networks (Sukhbaatar et al., 2015) learned through previously proposed supervised learning and reinforcement learning methods. In this setup, it is demonstrated that the agent learning from feedback (e.g. through question asking or question clarification) performs better.

The motivation for the paper is excellent; dialogue agents which learn directly from unstructured human feedback (as opposed to reward signals alone) could be very useful in real-world applications. However, the paper falls short on the execution. All the numerous experiments presented are based on the synthetic dialogue simulator, which is highly artificial and different from real-world dialogues. The simulator is based on a simple factoid question-answering framework, which normally is not considered dialogue and which appears to be solvable with a few hand-crafted rules. The framework also assumes that the user's feedback is always correct and is given in one of a handful of forms (e.g. paraphrase of original question without typos) and that the agent can learn from examples of another agent asking questions or making clarifications, which simplifies the task even further.

Because of the artificial setting and limited scope of the experiments, it seems difficult to draw conclusions about how to learn from unstructured user feedback. To test the hypothesis that it is possible to learn from such user feedback, I would strongly recommend the authors to continue working on this project by carrying out experiments with real human users (even in the factoid question answering domain, if necessary). This would provide much stronger evidence that a dialogue agent can learn from such feedback.


Other comments:
- The abstract uses the phrase ""interactive dialogue agents"". What is meant by ""interactive"" dialogue agents? All dialogue agents interact with the user, so isn't it redundant to call them interactive?
- A major limitation of the experiments is that the questions the agent can ask are specified a priori. If I understand correctly, in the supervised learning setting the agent is trained to imitate the questions of another rule-based agent. While in the RL setting, the paper states ""For each dialogue, the bot takes two sequential actions $(a_1 , a_2)$: to ask or not to ask a question (denoted as a_1 ); and guessing the final answer (denoted as a_2)"". This means the agent learns *when* to ask questions but not *what* questions to ask.
- Related to the previous comment, in the sub-section ""ONLINE REINFORCEMENT LEARNING (RL)"" the paper states ""We also explored scenarios where the student learns the ability to decide when to ask a question and what to ask."". Please clarify this by removing the part ""what to ask"".
- The paper presents an overwhelming amount of results. I understand the benefit of synthetic tasks is precisely the ability to measure many aspects of model performance, but in this case it confuses the reader to present so many results. For example, what was the reason for including the ""TrainAQ(+FP)"" and ""TrainMix"" training settings? How do these results help validate the original hypothesis? If they don't, they should be taken out or moved to the appendix.
- Since the contribution of the paper lies in the tasks and evaluation, it might be better to move either the vanilla-MemN2N (Table 2) to the appendix or to move the Cont-MemN2N results (Table 3) to the appendix.

--- UPDATE ---

Following the discussion below and the additional experiments provided by the authors, I have increased my score to 8.",1
"This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent's ability to learn from user feedback. For solving these tasks, the paper uses memory networks (Sukhbaatar et al., 2015) learned through previously proposed supervised learning and reinforcement learning methods. In this setup, it is demonstrated that the agent learning from feedback (e.g. through question asking or question clarification) performs better.

The motivation for the paper is excellent; dialogue agents which learn directly from unstructured human feedback (as opposed to reward signals alone) could be very useful in real-world applications. However, the paper falls short on the execution. All the numerous experiments presented are based on the synthetic dialogue simulator, which is highly artificial and different from real-world dialogues. The simulator is based on a simple factoid question-answering framework, which normally is not considered dialogue and which appears to be solvable with a few hand-crafted rules. The framework also assumes that the user's feedback is always correct and is given in one of a handful of forms (e.g. paraphrase of original question without typos) and that the agent can learn from examples of another agent asking questions or making clarifications, which simplifies the task even further.

Because of the artificial setting and limited scope of the experiments, it seems difficult to draw conclusions about how to learn from unstructured user feedback. To test the hypothesis that it is possible to learn from such user feedback, I would strongly recommend the authors to continue working on this project by carrying out experiments with real human users (even in the factoid question answering domain, if necessary). This would provide much stronger evidence that a dialogue agent can learn from such feedback.


Other comments:
- The abstract uses the phrase ""interactive dialogue agents"". What is meant by ""interactive"" dialogue agents? All dialogue agents interact with the user, so isn't it redundant to call them interactive?
- A major limitation of the experiments is that the questions the agent can ask are specified a priori. If I understand correctly, in the supervised learning setting the agent is trained to imitate the questions of another rule-based agent. While in the RL setting, the paper states ""For each dialogue, the bot takes two sequential actions $(a_1 , a_2)$: to ask or not to ask a question (denoted as a_1 ); and guessing the final answer (denoted as a_2)"". This means the agent learns *when* to ask questions but not *what* questions to ask.
- Related to the previous comment, in the sub-section ""ONLINE REINFORCEMENT LEARNING (RL)"" the paper states ""We also explored scenarios where the student learns the ability to decide when to ask a question and what to ask."". Please clarify this by removing the part ""what to ask"".
- The paper presents an overwhelming amount of results. I understand the benefit of synthetic tasks is precisely the ability to measure many aspects of model performance, but in this case it confuses the reader to present so many results. For example, what was the reason for including the ""TrainAQ(+FP)"" and ""TrainMix"" training settings? How do these results help validate the original hypothesis? If they don't, they should be taken out or moved to the appendix.
- Since the contribution of the paper lies in the tasks and evaluation, it might be better to move either the vanilla-MemN2N (Table 2) to the appendix or to move the Cont-MemN2N results (Table 3) to the appendix.

--- UPDATE ---

Following the discussion below and the additional experiments provided by the authors, I have increased my score to 8.",1
"This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice.",1
"This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice.",1
"This paper proposes a design principle for computation blocks in convolutional networks based on repeated application of expand and join operations resulting in a fractal-like structure. 

This paper is primarily about experimental evaluation, since the objective is to show that a residual formulation is not necessary to obtain good performance, at least on some tasks.

However, in my opinion the evaluations in the paper are not convincing. The primary issue is lack of a proper baseline, against which the improvements can be clearly demonstrated by making isolated changes. I understand that for this paper such a baseline is hard to construct, since it is about a novel architecture principle. This is why more effort should be put into this, so that core insights from this paper can be useful even after better performing architectures are discovered.
The number of parameters and amount of computation should be used to indicate how fair the comparisons are between architectures. Some detailed comments:

- In Table 1 comparisons to Resnets, the resnets from He et al. 2016b and Wide Resnets should be compared to FractalNet (in lieu of a proper baseline). The first outperforms FractalNet on CIFAR-100 while the second outperforms it on both. The authors compare to other results without augmentation, but did not perform additional experiments without augmentation for these architectures.

- The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.

- A proper comparison to Inception networks should also be performed for these networks. My guess is that the reason behind a seemingly 'ad-hoc' design of Inception modules is to reduce the computational footprint of the model (which is not a central motivation of fractal nets). Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts, one can easily simplify the Inception design to build a strong baseline e.g. by converting the concatenation operation to a mean operation among equally sized convolution outputs. As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance [1].

- It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.

- The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.

Overall, it's not clear to me that the experiments clearly demonstrate the utility of the proposed architecture. 

[1] Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. ""Inception-v4, inception-resnet and the impact of residual connections on learning."" arXiv preprint arXiv:1602.07261 (2016).",1
"This paper proposes a design principle for computation blocks in convolutional networks based on repeated application of expand and join operations resulting in a fractal-like structure. 

This paper is primarily about experimental evaluation, since the objective is to show that a residual formulation is not necessary to obtain good performance, at least on some tasks.

However, in my opinion the evaluations in the paper are not convincing. The primary issue is lack of a proper baseline, against which the improvements can be clearly demonstrated by making isolated changes. I understand that for this paper such a baseline is hard to construct, since it is about a novel architecture principle. This is why more effort should be put into this, so that core insights from this paper can be useful even after better performing architectures are discovered.
The number of parameters and amount of computation should be used to indicate how fair the comparisons are between architectures. Some detailed comments:

- In Table 1 comparisons to Resnets, the resnets from He et al. 2016b and Wide Resnets should be compared to FractalNet (in lieu of a proper baseline). The first outperforms FractalNet on CIFAR-100 while the second outperforms it on both. The authors compare to other results without augmentation, but did not perform additional experiments without augmentation for these architectures.

- The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.

- A proper comparison to Inception networks should also be performed for these networks. My guess is that the reason behind a seemingly 'ad-hoc' design of Inception modules is to reduce the computational footprint of the model (which is not a central motivation of fractal nets). Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts, one can easily simplify the Inception design to build a strong baseline e.g. by converting the concatenation operation to a mean operation among equally sized convolution outputs. As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance [1].

- It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.

- The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.

Overall, it's not clear to me that the experiments clearly demonstrate the utility of the proposed architecture. 

[1] Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. ""Inception-v4, inception-resnet and the impact of residual connections on learning."" arXiv preprint arXiv:1602.07261 (2016).",1
"## Paper summary

The paper reconsiders the idea of using a binary classifier to do two-sample testing. The idea is to split the sample into two disjoint training and test sets, train a classifier on the training set, and use the accuracy on the test set as the test statistic. If the accuracy is above chance level, one concludes that the two samples are from different distributions i.e., reject H0.

A theoretical result on an asymptotic approximate test power is provided. One implication is that the test is consistent, assuming that the classifier is better than coin tossing. Experiments on toy problems, evaluation of GANs, and causal discovery verify the effectiveness of the test. In addition, when the classifier is a neural net, examining the first linear filter layer allows one to see features which are most activated. The result is an interpretable visual indicator of how the two samples differ.

## Review summary 

The paper is well written and easy to follow. The idea of using a binary classifier for a two-sample testing is not new, as made clear in the paper. The main contributions are the analysis of the asymptotic test power, the use of modern deep nets as the classifier in this context, and the empirical studies on various tasks. The empirical results are satisfactorily convincing.  Although not much discussion is made on why the method works well in practice, overall contributions have a potential to start a new direction of research on model criticisms of generative models, as well as visualization of where a model fails. I vote for an acceptance.

## Major comments / questions 

My main concern is on Theorem 1 (asymptotic test power) and its assumptions.  But, I understand that these can be fixed as discussed below.

* Under H0, the distribution of the test statistic (i.e., sum of 0-1 classification results) follows Binomial(nte, 1/2) as stated.  However, under H1, terms in the sum are independent but *not* identical Bernoulli random variable. This is because each term depends on a data point z_i, which can be from either P or Q. So, in the paragraph in Sec3.1: ""... the random variable n_te \hat{t} follows a Binomial(nte, p)..."" is not correct. Essentially p depends on z_i. It should follow a Poisson binomial distribution.

* In the same paragraph, for the same reason, the alternative distribution of Binomial(nte, p=p_{risk}) is probably not correct. I guess you mention it to use Moivre-Laplace to get the asymptotic normality. 

Anyway, I see no reason why you would need this statement as the Binomial is not required in the proof, but only its asymptotic normality. A variant of the central limit theorem (instead of the Moivre-Laplace theorem) for independent, non-identical variables would still allow you to conclude the asymptotic normality of the Poisson binomial (with some conditions). See for example",1
"## Paper summary

The paper reconsiders the idea of using a binary classifier to do two-sample testing. The idea is to split the sample into two disjoint training and test sets, train a classifier on the training set, and use the accuracy on the test set as the test statistic. If the accuracy is above chance level, one concludes that the two samples are from different distributions i.e., reject H0.

A theoretical result on an asymptotic approximate test power is provided. One implication is that the test is consistent, assuming that the classifier is better than coin tossing. Experiments on toy problems, evaluation of GANs, and causal discovery verify the effectiveness of the test. In addition, when the classifier is a neural net, examining the first linear filter layer allows one to see features which are most activated. The result is an interpretable visual indicator of how the two samples differ.

## Review summary 

The paper is well written and easy to follow. The idea of using a binary classifier for a two-sample testing is not new, as made clear in the paper. The main contributions are the analysis of the asymptotic test power, the use of modern deep nets as the classifier in this context, and the empirical studies on various tasks. The empirical results are satisfactorily convincing.  Although not much discussion is made on why the method works well in practice, overall contributions have a potential to start a new direction of research on model criticisms of generative models, as well as visualization of where a model fails. I vote for an acceptance.

## Major comments / questions 

My main concern is on Theorem 1 (asymptotic test power) and its assumptions.  But, I understand that these can be fixed as discussed below.

* Under H0, the distribution of the test statistic (i.e., sum of 0-1 classification results) follows Binomial(nte, 1/2) as stated.  However, under H1, terms in the sum are independent but *not* identical Bernoulli random variable. This is because each term depends on a data point z_i, which can be from either P or Q. So, in the paragraph in Sec3.1: ""... the random variable n_te \hat{t} follows a Binomial(nte, p)..."" is not correct. Essentially p depends on z_i. It should follow a Poisson binomial distribution.

* In the same paragraph, for the same reason, the alternative distribution of Binomial(nte, p=p_{risk}) is probably not correct. I guess you mention it to use Moivre-Laplace to get the asymptotic normality. 

Anyway, I see no reason why you would need this statement as the Binomial is not required in the proof, but only its asymptotic normality. A variant of the central limit theorem (instead of the Moivre-Laplace theorem) for independent, non-identical variables would still allow you to conclude the asymptotic normality of the Poisson binomial (with some conditions). See for example",1
"This work contributes to understanding the landscape of deep networks in terms of its topology and geometry. The paper analyzes the former theoretically, and studies the latter empirically. Although the provided contributions are very specific (ReLU nets with single hidden layer, and a heuristic to calculate the normalized geodesic), the results are original and of interest. Thus, they could potentially be used as stepping stones for deeper developments in this area.

Pros:
1. Providing new theory about existence of ""poor"" local minima for ReLU networks with a hidden unit that relies on input distribution properties as well as the size of the hidden layer.
2. Coming up with a heuristic algorithm to compute the normalized geodesic between two solution points. The latter reflects how curved the path between the two is. 

Cons:
The results are very specific in both topology and geometry analysis.
1. The analysis is performed only over a ""single"" hidden layer ReLU network. Given the importance of depth in deep architectures, this result cannot really explain the kinds of architectures we are interested in practically. 
2. The normalized geodesic criterion is somewhat limited in representing how easy it is to connect two equally good points. For example, there might exist a straight line between the two (which is considered as easy by the geodesic criterion), but this line might be going through a very narrow valley, challenging gradient based optimization algorithms (and thus extremely difficult to navigate in practice). In addition, the proposed algorithm for computing the normalized geodesic is a greedy heuristic, which as far as I can tell, makes it difficult to know how we can trust in the estimated geodesics obtained by this algorithm.

With all cons said, I stress that I understand both problems tackled in the paper are challenging, and thus I find the contributions valuable and interesting.",1
"This work contributes to understanding the landscape of deep networks in terms of its topology and geometry. The paper analyzes the former theoretically, and studies the latter empirically. Although the provided contributions are very specific (ReLU nets with single hidden layer, and a heuristic to calculate the normalized geodesic), the results are original and of interest. Thus, they could potentially be used as stepping stones for deeper developments in this area.

Pros:
1. Providing new theory about existence of ""poor"" local minima for ReLU networks with a hidden unit that relies on input distribution properties as well as the size of the hidden layer.
2. Coming up with a heuristic algorithm to compute the normalized geodesic between two solution points. The latter reflects how curved the path between the two is. 

Cons:
The results are very specific in both topology and geometry analysis.
1. The analysis is performed only over a ""single"" hidden layer ReLU network. Given the importance of depth in deep architectures, this result cannot really explain the kinds of architectures we are interested in practically. 
2. The normalized geodesic criterion is somewhat limited in representing how easy it is to connect two equally good points. For example, there might exist a straight line between the two (which is considered as easy by the geodesic criterion), but this line might be going through a very narrow valley, challenging gradient based optimization algorithms (and thus extremely difficult to navigate in practice). In addition, the proposed algorithm for computing the normalized geodesic is a greedy heuristic, which as far as I can tell, makes it difficult to know how we can trust in the estimated geodesics obtained by this algorithm.

With all cons said, I stress that I understand both problems tackled in the paper are challenging, and thus I find the contributions valuable and interesting.",1
"This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it.

Comments:

- The scores reported on A3C in this paper and in the Mnih et al. publication (table S3) differ significantly. Where does this discrepancy come from? If it's from a different training regime (fewer iterations, for instance), did the authors confirm that running  their replication to the same settings as Mnih et al provide similar results?

- It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate. This seems to imply that for those, the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 (and therefore has 4 times fewer gradient updates). A3C could be run for a comparable computation cost with a lower action repeat, which would probably result in increased performance of A3C.  Nevertheless,  the automatic determination of the appropriate action repeat is interesting, even if the overall message seems to be to not repeat actions too often.

- Slightly problematic notation, where r sometimes denotes rewards, sometimes denotes elements of the repetition set R (top of page 5)

- In the equation at the bottom of page 5 - since the sum is not indexed over decision steps, not time steps, shouldn't the rewards r_k be modified to be the sum of rewards (appropriately discounted) between those time steps?

- The section on DDPG is confusingly written. ""Concatenating"" loss is a strange operation; doesn't FiGAR correspond to a loss to roughly looks like Q(x,mu(x)) + R log p(x) (with separate loss for learning the critic)? It feels that REINFORCE should be applied for the repetition variable x (second term of the sum) and reparametrization for the action a (first term)? 

- Is the 'name_this_game' name in the tables  intentional?

- A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps, independently of what happens next. Have the authors considered a scheme in which, at each time step, the agent decides to stick with the current decision or not? (It feels like it might be a relatively simple modification of FiGAR).",1
"This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it.

Comments:

- The scores reported on A3C in this paper and in the Mnih et al. publication (table S3) differ significantly. Where does this discrepancy come from? If it's from a different training regime (fewer iterations, for instance), did the authors confirm that running  their replication to the same settings as Mnih et al provide similar results?

- It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate. This seems to imply that for those, the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 (and therefore has 4 times fewer gradient updates). A3C could be run for a comparable computation cost with a lower action repeat, which would probably result in increased performance of A3C.  Nevertheless,  the automatic determination of the appropriate action repeat is interesting, even if the overall message seems to be to not repeat actions too often.

- Slightly problematic notation, where r sometimes denotes rewards, sometimes denotes elements of the repetition set R (top of page 5)

- In the equation at the bottom of page 5 - since the sum is not indexed over decision steps, not time steps, shouldn't the rewards r_k be modified to be the sum of rewards (appropriately discounted) between those time steps?

- The section on DDPG is confusingly written. ""Concatenating"" loss is a strange operation; doesn't FiGAR correspond to a loss to roughly looks like Q(x,mu(x)) + R log p(x) (with separate loss for learning the critic)? It feels that REINFORCE should be applied for the repetition variable x (second term of the sum) and reparametrization for the action a (first term)? 

- Is the 'name_this_game' name in the tables  intentional?

- A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps, independently of what happens next. Have the authors considered a scheme in which, at each time step, the agent decides to stick with the current decision or not? (It feels like it might be a relatively simple modification of FiGAR).",1
"Taking into account the loss in the binarization step through a proximal Newton algorithm is a nice idea. This is at least one approach to bringing in the missing loss in the binarization step, which has recently gone from a two step process of train and binarize to a single step simultaneous train/compress. Performance on a few small tasks show the benefit. It would be nice to see some results on substantial networks and tasks which really need compression on embedded systems (a point made in the introduction). Is it necessary to discuss exploding/vanishing gradients when the RNN experiments are carried out by an LSTM, and handled by the cell error carousel? We see the desire to tie into proposition 2, but not clear that the degradation we see in the binary connect is related. Adam is used in the LSTM optimization, was gradient clipping really needed, or is the degradation of binary connect simply related to capacity? For proposition 3.1, theorem 3.1 and proposition 3.2 put the pointers to proofs in appendix.",1
"Taking into account the loss in the binarization step through a proximal Newton algorithm is a nice idea. This is at least one approach to bringing in the missing loss in the binarization step, which has recently gone from a two step process of train and binarize to a single step simultaneous train/compress. Performance on a few small tasks show the benefit. It would be nice to see some results on substantial networks and tasks which really need compression on embedded systems (a point made in the introduction). Is it necessary to discuss exploding/vanishing gradients when the RNN experiments are carried out by an LSTM, and handled by the cell error carousel? We see the desire to tie into proposition 2, but not clear that the degradation we see in the binary connect is related. Adam is used in the LSTM optimization, was gradient clipping really needed, or is the degradation of binary connect simply related to capacity? For proposition 3.1, theorem 3.1 and proposition 3.2 put the pointers to proofs in appendix.",1
"This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching. The main solution of both papers is introducing autoencoder into discriminator to improve the stability and quality of GAN. Different to Denoising Feature Matching, EBGAN uses encoder-decoder instead of denoising only, and use hingle loss to replace original loss function.

The theoretical results are good, and empirical result of high resolution image is unique among all recent GAN advantages.

I suggest to introduce Improving Generative Adversarial Networks with Denoising Feature Matching as related work.",1
"This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching. The main solution of both papers is introducing autoencoder into discriminator to improve the stability and quality of GAN. Different to Denoising Feature Matching, EBGAN uses encoder-decoder instead of denoising only, and use hingle loss to replace original loss function.

The theoretical results are good, and empirical result of high resolution image is unique among all recent GAN advantages.

I suggest to introduce Improving Generative Adversarial Networks with Denoising Feature Matching as related work.",1
"Variational auto-encoders, adversarial networks, and kernel scoring rules like MMD have recently gained popularity as methods for learning directed generative models and for other applications like domain adaptation. This paper gives an additional method along the scoring rules direction that uses the matching of central moments to match two probability distributions. The technique is simple, and in the case of domain adaptation, highly effective.

CMD seems like a very nice and straightforward solution to the domain adaptation problem. The method is computationally straightforward to implement, and seems quite stable with respect to the tuning parameters when compared to MMD. I was skeptical reading through this, especially given the fact that you only use K=5 in your experiments, but the results seem quite good. The natural question that I have now is: how will this method do in training generative models? This is beyond the scope of this paper, but it’s the lowest hanging fruit.

Below I give more detailed feedback.

One way to speed up MMD is to use a random Fourier basis as was done in “Fastmmd: Ensemble of circular discrepancy for efficient two-sample test” by Zhao and Meng, 2015. There are also linear time estimators, e.g., in “A Kernel Two-Sample Test“ by Gretton et al., 2012. I don’t think you need to compare against these approaches since you compare to the full MMD, but they should be cited.

The paper “Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy” by Sutherland et al. submitted to ICLR 2017 as well, discusses techniques for optimizing the kernel used in MMD and is worth citing in section 3.

How limiting is the assumption that the distribution has independent marginals?

The sample complexity of MMD depends heavily on the dimensionality of the input space - do you have any intuitions about the sample complexity of CMD? It seems like it's relatively insensitive based on the results in Figure 4, but I would be surprised if this were the case with 10,000 hidden units. I mainly ask this because with generative models, the output space can be quite high-dimensional.

I’m concerned that the central moments won’t be numerically stable at higher orders when backpropagating. This doesn’t seem to be a problem in the experimental results, but perhaps the authors could comment a bit on this? I’m referring to the fact that ck(X) can be very large for k >= 3. Proposition 1 alleviates my concerns that the overall objective is unstable, I’m referring specifically to the individual terms within.

Figure 3 is rather cluttered, and aside from the mouse class it’s not clear to me from the visualization that the CMD regularizer is actually helping. It would be useful to remove some of the classes for the purpose of visualization.

I would like some clarification about the natural geometric interpretations of K=5. Do you mean that the moments up to K=5 have been well-studied? Do you have any references for this? Why does K >= 6 not have a natural geometric interpretation?

Figure 4 should have a legend",1
"Variational auto-encoders, adversarial networks, and kernel scoring rules like MMD have recently gained popularity as methods for learning directed generative models and for other applications like domain adaptation. This paper gives an additional method along the scoring rules direction that uses the matching of central moments to match two probability distributions. The technique is simple, and in the case of domain adaptation, highly effective.

CMD seems like a very nice and straightforward solution to the domain adaptation problem. The method is computationally straightforward to implement, and seems quite stable with respect to the tuning parameters when compared to MMD. I was skeptical reading through this, especially given the fact that you only use K=5 in your experiments, but the results seem quite good. The natural question that I have now is: how will this method do in training generative models? This is beyond the scope of this paper, but it’s the lowest hanging fruit.

Below I give more detailed feedback.

One way to speed up MMD is to use a random Fourier basis as was done in “Fastmmd: Ensemble of circular discrepancy for efficient two-sample test” by Zhao and Meng, 2015. There are also linear time estimators, e.g., in “A Kernel Two-Sample Test“ by Gretton et al., 2012. I don’t think you need to compare against these approaches since you compare to the full MMD, but they should be cited.

The paper “Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy” by Sutherland et al. submitted to ICLR 2017 as well, discusses techniques for optimizing the kernel used in MMD and is worth citing in section 3.

How limiting is the assumption that the distribution has independent marginals?

The sample complexity of MMD depends heavily on the dimensionality of the input space - do you have any intuitions about the sample complexity of CMD? It seems like it's relatively insensitive based on the results in Figure 4, but I would be surprised if this were the case with 10,000 hidden units. I mainly ask this because with generative models, the output space can be quite high-dimensional.

I’m concerned that the central moments won’t be numerically stable at higher orders when backpropagating. This doesn’t seem to be a problem in the experimental results, but perhaps the authors could comment a bit on this? I’m referring to the fact that ck(X) can be very large for k >= 3. Proposition 1 alleviates my concerns that the overall objective is unstable, I’m referring specifically to the individual terms within.

Figure 3 is rather cluttered, and aside from the mouse class it’s not clear to me from the visualization that the CMD regularizer is actually helping. It would be useful to remove some of the classes for the purpose of visualization.

I would like some clarification about the natural geometric interpretations of K=5. Do you mean that the moments up to K=5 have been well-studied? Do you have any references for this? Why does K >= 6 not have a natural geometric interpretation?

Figure 4 should have a legend",1
"Nice idea but not complete, model size is not reduced by the large factors found in one of your references (Song 2016), where they go to 5 bits, but this is ontop of pruning which gives overall 49X reduction in model size of VGG (without loss of accuracy). You may achieve similar reductions with inclusion of pruning (or better since you go to 4 bits with no loss) but we should see this in the paper, so at the moment it is difficult to compare",1
"Nice idea but not complete, model size is not reduced by the large factors found in one of your references (Song 2016), where they go to 5 bits, but this is ontop of pruning which gives overall 49X reduction in model size of VGG (without loss of accuracy). You may achieve similar reductions with inclusion of pruning (or better since you go to 4 bits with no loss) but we should see this in the paper, so at the moment it is difficult to compare",1
__Note__: An earlier version of the review (almost identical to the present one) for an earlier version of the paper (available on arXiV) can be found here:,1
__Note__: An earlier version of the review (almost identical to the present one) for an earlier version of the paper (available on arXiV) can be found here:,1
"The paper proposed a nice framework leveraging Tucker and Tensor train low-rank tensor factorization to induce parameter sharing for multi-task learning.

The framework is nice and appealing. 

However, MTL is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``Deep Learning"" for these simple datasets. A comparison with existing shallow MTL is necessary to show the benefits of the proposed methods (and in particular being deep) on the dataset. The authors ignore them on the basis of speculation and it is not clear if the proposed framework is really superior to simple regularizations like the nuclear norm. The idea of nuclear norm regularization can also be extended to deep learning as gradient descent are popular in all methods.",1
"The paper proposed a nice framework leveraging Tucker and Tensor train low-rank tensor factorization to induce parameter sharing for multi-task learning.

The framework is nice and appealing. 

However, MTL is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``Deep Learning"" for these simple datasets. A comparison with existing shallow MTL is necessary to show the benefits of the proposed methods (and in particular being deep) on the dataset. The authors ignore them on the basis of speculation and it is not clear if the proposed framework is really superior to simple regularizations like the nuclear norm. The idea of nuclear norm regularization can also be extended to deep learning as gradient descent are popular in all methods.",1
"This paper presents a semi-supervised technique for “self-ensembling” where the model uses a consensus prediction (computed from previous epochs) as a target to regress to, in addition to the usual supervised learning loss. This has connections to the “dark knowledge” idea, ladder networks work is shown in this paper to be a promising technique for scenarios with few labeled examples (but not only). The paper presents two versions of the idea: one which is computationally expensive (and high variance) in that it needs two passes through the same example at a given step, and a temporal ensembling method that is stabler, cheaper computationally but more memory hungry and requires an extra hyper-parameter. 


My thoughts on this work are mostly positive. The drawbacks that I see are that the temporal ensembling work requires potentially a lot of memory, and non-trivial infrastructure / book-keeping for imagenet-sized experiments. I am quite confused by the Figure 2 / Section 3.4 experiments about tolerance to noisy labels: it’s *very* incredible to me that by making 90% of the labels random one can still train a classifier that is either 30% accurate or ~78% accurate (depending on whether or not temporal ensembling was used). I don’t see how that can happen, basically.


Minor stuff:
Please bold the best-in-category results in your tables. 
I think it would be nice to talk about the ramp-up of w(t) in the main paper. 
The authors should consider putting the state of the art results for the fully-supervised case in their tables, instead of just their own.
I am confused as to why the authors chose not to use more SVHN examples. The stated reason that it’d be “too easy” seems a bit contrived: if they used all examples it would also make it easy to compare to previous work.",1
"This paper presents a semi-supervised technique for “self-ensembling” where the model uses a consensus prediction (computed from previous epochs) as a target to regress to, in addition to the usual supervised learning loss. This has connections to the “dark knowledge” idea, ladder networks work is shown in this paper to be a promising technique for scenarios with few labeled examples (but not only). The paper presents two versions of the idea: one which is computationally expensive (and high variance) in that it needs two passes through the same example at a given step, and a temporal ensembling method that is stabler, cheaper computationally but more memory hungry and requires an extra hyper-parameter. 


My thoughts on this work are mostly positive. The drawbacks that I see are that the temporal ensembling work requires potentially a lot of memory, and non-trivial infrastructure / book-keeping for imagenet-sized experiments. I am quite confused by the Figure 2 / Section 3.4 experiments about tolerance to noisy labels: it’s *very* incredible to me that by making 90% of the labels random one can still train a classifier that is either 30% accurate or ~78% accurate (depending on whether or not temporal ensembling was used). I don’t see how that can happen, basically.


Minor stuff:
Please bold the best-in-category results in your tables. 
I think it would be nice to talk about the ramp-up of w(t) in the main paper. 
The authors should consider putting the state of the art results for the fully-supervised case in their tables, instead of just their own.
I am confused as to why the authors chose not to use more SVHN examples. The stated reason that it’d be “too easy” seems a bit contrived: if they used all examples it would also make it easy to compare to previous work.",1
"I reviewed the manuscript on December 5th.

Summary:
The authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.

Major comments:

The authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. 

A potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure.

My second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.

Minor comments:

If there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points.

- X-axis label is wrong in Figure 2 right.

Measure the transferability of the detector?

- How is \sigma labeled on Figure 5?

- Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?",1
"I reviewed the manuscript on December 5th.

Summary:
The authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.

Major comments:

The authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. 

A potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure.

My second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.

Minor comments:

If there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points.

- X-axis label is wrong in Figure 2 right.

Measure the transferability of the detector?

- How is \sigma labeled on Figure 5?

- Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?",1
"This work address the problem of supervised learning from strongly labeled data with label noise. This is a very practical and relevant problem in applied machine learning.  The authors note that using sampling approaches such as EM isn't effective, too slow and cannot be integrated into end-to-end training. Thus, they propose to simulate the effects of EM by a noisy adaptation layer, effectively a softmax, that is added to the architecture during training, and is omitted at inference time. The proposed algorithm is evaluated on MNIST and shows improvements over existing approaches that deal with noisy labeled data.

A few comments.
1. There is no discussion in the work about the increased complexity of training for the model with two softmaxes. 

2. What is the rationale for having consecutive (serialized) softmaxes, instead of having a compound objective with two losses, or a network with parallel losses and two sets of gradients?

3. The proposed architecture with only two hidden layers isn't not representative of larger and deeper models that are practically used, and it is not clear that shown results will scale to bigger networks. 

4. Why is the approach only evaluated on MNIST, a dataset that is unrealistically simple.",1
"This work address the problem of supervised learning from strongly labeled data with label noise. This is a very practical and relevant problem in applied machine learning.  The authors note that using sampling approaches such as EM isn't effective, too slow and cannot be integrated into end-to-end training. Thus, they propose to simulate the effects of EM by a noisy adaptation layer, effectively a softmax, that is added to the architecture during training, and is omitted at inference time. The proposed algorithm is evaluated on MNIST and shows improvements over existing approaches that deal with noisy labeled data.

A few comments.
1. There is no discussion in the work about the increased complexity of training for the model with two softmaxes. 

2. What is the rationale for having consecutive (serialized) softmaxes, instead of having a compound objective with two losses, or a network with parallel losses and two sets of gradients?

3. The proposed architecture with only two hidden layers isn't not representative of larger and deeper models that are practically used, and it is not clear that shown results will scale to bigger networks. 

4. Why is the approach only evaluated on MNIST, a dataset that is unrealistically simple.",1
"In this paper, the authors propose a new method to learn hierarchical representations of sentences, based on reinforcement learning. They propose to learn a neural shift-reduce parser, such that the induced tree structures lead to good performance on a downstream task. They use reinforcement learning (more specifically, the policy gradient method REINFORCE) to learn their model. The reward of the algorithm is the evaluation metric of the downstream task. The authors compare two settings, (1) no structure information is given (hence, the only supervision comes from the downstream task) and (2) actions from an external parser is used as supervision to train the policy network, in addition to the supervision from the downstream task. The proposed approach is evaluated on four tasks: sentiment analysis, semantic relatedness, textual entailment and sentence generation.

I like the idea of learning tree representations of text which are useful for a downstream task. The paper is clear and well written. However, I am not convinced by the experimental results presented in the paper. Indeed, on most tasks, the proposed model is far from state-of-the-art models:
 - sentiment analysis, 86.5 v.s. 89.7 (accuracy);
 - semantic relatedness, 0.32 v.s. 0.25 (MSE);
 - textual entailment, 80.5 v.s. 84.6 (accuracy).
From the results presented in the paper, it is hard to know if these results are due to the model, or because of the reinforcement learning algorithm.

PROS:
 - interesting idea: learning structures of sentences adapted for a downstream task.
 - well written paper.
CONS:
 - weak experimental results (do not really support the claim of the authors).

Minor comments:
In the second paragraph of the introduction, one might argue that bag-of-words is also a predominant approach to represent sentences.
Paragraph titles (e.g. in section 3.2) should have a period at the end.

----------------------------------------------------------------------------------------------------------------------
UPDATE

I am still not convinced by the results presented in the paper, and in particular by the fact that one must combine the words in a different way than left-to-right to obtain state of the art results.
However, I do agree that this is an interesting research direction, and that the results presented in the paper are promising. I am thus updating my score from 5 to 6.",1
"In this paper, the authors propose a new method to learn hierarchical representations of sentences, based on reinforcement learning. They propose to learn a neural shift-reduce parser, such that the induced tree structures lead to good performance on a downstream task. They use reinforcement learning (more specifically, the policy gradient method REINFORCE) to learn their model. The reward of the algorithm is the evaluation metric of the downstream task. The authors compare two settings, (1) no structure information is given (hence, the only supervision comes from the downstream task) and (2) actions from an external parser is used as supervision to train the policy network, in addition to the supervision from the downstream task. The proposed approach is evaluated on four tasks: sentiment analysis, semantic relatedness, textual entailment and sentence generation.

I like the idea of learning tree representations of text which are useful for a downstream task. The paper is clear and well written. However, I am not convinced by the experimental results presented in the paper. Indeed, on most tasks, the proposed model is far from state-of-the-art models:
 - sentiment analysis, 86.5 v.s. 89.7 (accuracy);
 - semantic relatedness, 0.32 v.s. 0.25 (MSE);
 - textual entailment, 80.5 v.s. 84.6 (accuracy).
From the results presented in the paper, it is hard to know if these results are due to the model, or because of the reinforcement learning algorithm.

PROS:
 - interesting idea: learning structures of sentences adapted for a downstream task.
 - well written paper.
CONS:
 - weak experimental results (do not really support the claim of the authors).

Minor comments:
In the second paragraph of the introduction, one might argue that bag-of-words is also a predominant approach to represent sentences.
Paragraph titles (e.g. in section 3.2) should have a period at the end.

----------------------------------------------------------------------------------------------------------------------
UPDATE

I am still not convinced by the results presented in the paper, and in particular by the fact that one must combine the words in a different way than left-to-right to obtain state of the art results.
However, I do agree that this is an interesting research direction, and that the results presented in the paper are promising. I am thus updating my score from 5 to 6.",1
"I reviewed the manuscript as of December 7th.

Summary:
The authors investigate the transferability of adversarial examples in deep networks. The authors confirm that transferability exists even in large models but demonstrate that it is difficult to manipulate the network to adversarially perturb an image into a specifically desired label. The authors additionally demonstrate real world attacks on a vision web service and explore the geometric properties of adversarial examples.

Major Comments:
1. The paper contains a list of many results and it is not clear what single message this paper provides. As mentioned in the comments, this paper is effectively 15 pages and 9 page of results in the Appendix heavily discussed throughout the main body of the paper. Although there is no strict page limit for this conference, I do feel this pushes the spirit of a conference publication. I do not rule out this paper for acceptance based on the length but I do hold it as a negative because clarity of presentation is an important quality. If this paper is ultimately accepted, I would suggest that the authors make some effort to cut down the length even further beyond the 13 pages posted elsewhere. I have marked some sections to highlight areas that may be trimmed.

2. The section of geometric understanding is similar to results of 'Adversarial Perturbations of Deep Neural Networks' in Warde-Farley and Goodfellow (2015). See Figure 1.2. I am not clear what the authors show above-and-beyond these results. If there are additional findings, the authors should emphasize them.

3. The authors expand on observations by Goodfellow et al (2014) and Szegedy et al (2013) demonstrating that large-scale models are susceptible to adversarial perturbations (see also Kurakin et al (2016)). The authors additionally demonstrate that attempting to perform adversarial manipulation to convert an image to a particular, desired label is more difficult.

4. The authors demonstrate that they can target a real-world vision API. These results are compelling but it is not clear what these results demonstrate above-and-beyond Papernot et al (2016). 

As far I can understand, I think that the most interesting result from this paper not previously described in the literature is to note about the unique difficulty about performing adversarial manipulation to convert an image to a particular, desired label. The rest of the results appear to expand on other results that have already appeared in the literature and the authors need to better explain what these makes these results unique above-and-beyond previous work.

Areas to Trim the Paper:
- Table 1 is not necessary. Just cite other results or write the Top-1 numbers in the text.
- Condense Section 2.2.1 and cite heavily.
- Figure 2 panels may be overlaid to highlight a comparison.",1
"I reviewed the manuscript as of December 7th.

Summary:
The authors investigate the transferability of adversarial examples in deep networks. The authors confirm that transferability exists even in large models but demonstrate that it is difficult to manipulate the network to adversarially perturb an image into a specifically desired label. The authors additionally demonstrate real world attacks on a vision web service and explore the geometric properties of adversarial examples.

Major Comments:
1. The paper contains a list of many results and it is not clear what single message this paper provides. As mentioned in the comments, this paper is effectively 15 pages and 9 page of results in the Appendix heavily discussed throughout the main body of the paper. Although there is no strict page limit for this conference, I do feel this pushes the spirit of a conference publication. I do not rule out this paper for acceptance based on the length but I do hold it as a negative because clarity of presentation is an important quality. If this paper is ultimately accepted, I would suggest that the authors make some effort to cut down the length even further beyond the 13 pages posted elsewhere. I have marked some sections to highlight areas that may be trimmed.

2. The section of geometric understanding is similar to results of 'Adversarial Perturbations of Deep Neural Networks' in Warde-Farley and Goodfellow (2015). See Figure 1.2. I am not clear what the authors show above-and-beyond these results. If there are additional findings, the authors should emphasize them.

3. The authors expand on observations by Goodfellow et al (2014) and Szegedy et al (2013) demonstrating that large-scale models are susceptible to adversarial perturbations (see also Kurakin et al (2016)). The authors additionally demonstrate that attempting to perform adversarial manipulation to convert an image to a particular, desired label is more difficult.

4. The authors demonstrate that they can target a real-world vision API. These results are compelling but it is not clear what these results demonstrate above-and-beyond Papernot et al (2016). 

As far I can understand, I think that the most interesting result from this paper not previously described in the literature is to note about the unique difficulty about performing adversarial manipulation to convert an image to a particular, desired label. The rest of the results appear to expand on other results that have already appeared in the literature and the authors need to better explain what these makes these results unique above-and-beyond previous work.

Areas to Trim the Paper:
- Table 1 is not necessary. Just cite other results or write the Top-1 numbers in the text.
- Condense Section 2.2.1 and cite heavily.
- Figure 2 panels may be overlaid to highlight a comparison.",1
"Paper Summary:

Authors investigate identity re-parametrization in the linear and the non linear case. 

Detailed comments:

— Linear Residual Network:

The paper shows that for a linear residual network any critical point is a global optimum. This problem is non convex it is interesting that this simple re-parametrization leads to such a result. 

 — Non linear Residual Network:

Authors propose a construction that maps the points to their labels via a resnet , using an initial random projection, followed by a residual block that clusters the data based on their label, and a last layer that maps the clusters to the label. 

1- In Eq 3.4  seems the dimensions are not matching q_j in R^k and e_j in R^r. please clarify 

2- The construction seems fine, but what is special about the resnet here in this construction? One can do a similar construction if we did not have the identity? can you discuss this point?
In the linear case it is clear from a spectral point of view how the identity is helping the optimization. Please provide some intuition.  

3-   Existence of a network in the residual  class that overfits does it give us any intuition on why residual network outperform other architectures? What does an existence result of such a network tell us about its representation power ? 
A simple linear model under the assumption that points can not be too close can overfit the data, and get fast convergence rate (see for instance tsybakov noise condition).

4- What does the construction tell us about the number of layers? 

5- clustering the activation independently from the label, is an old way to pretrain the network. One could use those centroids as weights for the next layer (this is also related to Nystrom approximation see for instance",1
"Paper Summary:

Authors investigate identity re-parametrization in the linear and the non linear case. 

Detailed comments:

— Linear Residual Network:

The paper shows that for a linear residual network any critical point is a global optimum. This problem is non convex it is interesting that this simple re-parametrization leads to such a result. 

 — Non linear Residual Network:

Authors propose a construction that maps the points to their labels via a resnet , using an initial random projection, followed by a residual block that clusters the data based on their label, and a last layer that maps the clusters to the label. 

1- In Eq 3.4  seems the dimensions are not matching q_j in R^k and e_j in R^r. please clarify 

2- The construction seems fine, but what is special about the resnet here in this construction? One can do a similar construction if we did not have the identity? can you discuss this point?
In the linear case it is clear from a spectral point of view how the identity is helping the optimization. Please provide some intuition.  

3-   Existence of a network in the residual  class that overfits does it give us any intuition on why residual network outperform other architectures? What does an existence result of such a network tell us about its representation power ? 
A simple linear model under the assumption that points can not be too close can overfit the data, and get fast convergence rate (see for instance tsybakov noise condition).

4- What does the construction tell us about the number of layers? 

5- clustering the activation independently from the label, is an old way to pretrain the network. One could use those centroids as weights for the next layer (this is also related to Nystrom approximation see for instance",1
"The authors extend GANs by an inference path from the data space to the latent space and a discriminator that operates on the joint latend/data space. They show that the theoretical properties of GANs still hold for BiGAN and evaluate the features learned unsupervised in the inference path with respect to performance on supervised tasks after retraining deeper layers.

I see one structural issue with this paper: Given that, as stated in the abstract, the main purpose of the paper is to learn unsupervised features (and not to improve GANs), the paper might spent too much space on detailing the relationship to GANs and all the theoretical properties. It is not clear whether they actually would help with the goal of learning good features. While reading the paper, I actually totally forgot about the unsupervised features until they reappeared on page 6. I think it would be helpful if the text of the paper would be more aligned with this main story.

Still, the BiGAN framework is an elegant and compelling extension to GANs. However, it is not obvious how much the theoretical properties help us as the model is clearly not fully converged. To me, especially Figure 4 seems to suggest that G(E(x)) might be doing not much more than some kind of nearest neighbour retrival (and indeed one criticism for GANs has always been that they might just memorize some samples). By the way, it would be very interesting to know how well the discriminator actually performs after training.

Coming back to the goal of learning powerful features: The method does not reach state-of-the-art performance on most evaluated tasks (Table 2 and 3) but performs competitive and it would be interesting to see how much this improves if the BiGAN training (and the convolutional architecture used) would be improved.

The paper is very well written and provides most necessary details, although some more details on the training (learning rates, initialization) would be helpful for reproducing the results.

Overall I think the paper provides a very interesting framework for further research, even though the results presented here are not too impressive both with respect to the feature evaluation (and the GAN learning).

Minor: It might be helpful to highlight the best performance numbers in Tables 2 and 3.",1
"The authors extend GANs by an inference path from the data space to the latent space and a discriminator that operates on the joint latend/data space. They show that the theoretical properties of GANs still hold for BiGAN and evaluate the features learned unsupervised in the inference path with respect to performance on supervised tasks after retraining deeper layers.

I see one structural issue with this paper: Given that, as stated in the abstract, the main purpose of the paper is to learn unsupervised features (and not to improve GANs), the paper might spent too much space on detailing the relationship to GANs and all the theoretical properties. It is not clear whether they actually would help with the goal of learning good features. While reading the paper, I actually totally forgot about the unsupervised features until they reappeared on page 6. I think it would be helpful if the text of the paper would be more aligned with this main story.

Still, the BiGAN framework is an elegant and compelling extension to GANs. However, it is not obvious how much the theoretical properties help us as the model is clearly not fully converged. To me, especially Figure 4 seems to suggest that G(E(x)) might be doing not much more than some kind of nearest neighbour retrival (and indeed one criticism for GANs has always been that they might just memorize some samples). By the way, it would be very interesting to know how well the discriminator actually performs after training.

Coming back to the goal of learning powerful features: The method does not reach state-of-the-art performance on most evaluated tasks (Table 2 and 3) but performs competitive and it would be interesting to see how much this improves if the BiGAN training (and the convolutional architecture used) would be improved.

The paper is very well written and provides most necessary details, although some more details on the training (learning rates, initialization) would be helpful for reproducing the results.

Overall I think the paper provides a very interesting framework for further research, even though the results presented here are not too impressive both with respect to the feature evaluation (and the GAN learning).

Minor: It might be helpful to highlight the best performance numbers in Tables 2 and 3.",1
"This paper tackles the problem of compressing trained convnets with the goal of reducing memory overhead and speeding up the forward pass. As I understand it, the main contribution of this work is to develop fast convolution routines for sparse conv weights int he case of general sparsity (as compared with structured sparsity). They evaluate their method on both AlexNet and GoogLeNet as well as on various platforms. The authors make code available online. The paper is well written and does a good job of putting this work in the context of past model reduction techniques.

My main request of the authors would be to provide a concise summary of the speedup/memory gains achievable with this new work compared with previously published work. The authors do show the various sparsity level obtained with various methods of pruning but it is unclear to me how to translate the information given in the paper into an understanding of gains relative to other methods.",1
"This paper tackles the problem of compressing trained convnets with the goal of reducing memory overhead and speeding up the forward pass. As I understand it, the main contribution of this work is to develop fast convolution routines for sparse conv weights int he case of general sparsity (as compared with structured sparsity). They evaluate their method on both AlexNet and GoogLeNet as well as on various platforms. The authors make code available online. The paper is well written and does a good job of putting this work in the context of past model reduction techniques.

My main request of the authors would be to provide a concise summary of the speedup/memory gains achievable with this new work compared with previously published work. The authors do show the various sparsity level obtained with various methods of pruning but it is unclear to me how to translate the information given in the paper into an understanding of gains relative to other methods.",1
"The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the authors test this model on MNIST and SVHN in an unsupervised and semi-supervised fashion.
After reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are ""used"" (which actually enable backpropagation) but the latent variables are parametrized differently (into $\pi$) and the decoding process is altered as to give the impression of sparsity. The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model.
With respect to the Figure 5b showing the decoder input weights: component collapsing probably does not have the same effect as Gaussian prior. $\pi$ is positive therefore having a very small average value might mean that its value is close to zero most of the time, not requiring any update on the weight. For the standard Gaussian prior, component collapsing means having a very noisy input with no signal involved, which forces the decoder to shut down this channel, i.e. have small incoming weights from this collapsed variable.
Adding a histogram of the latent variables in addition to that might help decide if the associated weights are relatively large because they are actually used or if it's because the inputs are zero anyway.
The semi-supervised results are better than a weaker version of the model used in (Kingma et al., 2014), but as to have a fairer comparison, the results should be compared with the M1+M2 model in that paper, even if that requires also using two VAEs.",1
"The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the authors test this model on MNIST and SVHN in an unsupervised and semi-supervised fashion.
After reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are ""used"" (which actually enable backpropagation) but the latent variables are parametrized differently (into $\pi$) and the decoding process is altered as to give the impression of sparsity. The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model.
With respect to the Figure 5b showing the decoder input weights: component collapsing probably does not have the same effect as Gaussian prior. $\pi$ is positive therefore having a very small average value might mean that its value is close to zero most of the time, not requiring any update on the weight. For the standard Gaussian prior, component collapsing means having a very noisy input with no signal involved, which forces the decoder to shut down this channel, i.e. have small incoming weights from this collapsed variable.
Adding a histogram of the latent variables in addition to that might help decide if the associated weights are relatively large because they are actually used or if it's because the inputs are zero anyway.
The semi-supervised results are better than a weaker version of the model used in (Kingma et al., 2014), but as to have a fairer comparison, the results should be compared with the M1+M2 model in that paper, even if that requires also using two VAEs.",1
"The paper discuss a ""batch"" method for RL setup to improve chat-bots.
The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. 

I find the writing clear, and the algorithm a natural extension of the online version.

Below are some constructive remarks:
- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:
- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.
- section 2.2:
   sentence before last: s' is not defined. 
   last sentence: missing ""... in the stochastic case."" at the end.
- Section 4.1 last paragraph: ""While Bot-1 is not significant ..."" => ""While Bot-1 is not significantly different from ML ...""",1
"The paper discuss a ""batch"" method for RL setup to improve chat-bots.
The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. 

I find the writing clear, and the algorithm a natural extension of the online version.

Below are some constructive remarks:
- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:
- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.
- section 2.2:
   sentence before last: s' is not defined. 
   last sentence: missing ""... in the stochastic case."" at the end.
- Section 4.1 last paragraph: ""While Bot-1 is not significant ..."" => ""While Bot-1 is not significantly different from ML ...""",1
"I'd like to thank the authors for their detailed response to my questions.

The paper proposes a support regularized version of sparse coding that takes into account the underlying manifold structure of the data. For this purpose, the authors augment the classic sparse coding loss with a term that encourages near by points to have similar active set. Convergence guarantees for the optimization procedure are presented. Experimental evaluation on clustering and semi-supervised learning shows the benefits of the proposed approach.

The paper is well written and a nice read. The most relevant contribution of this work is to including (and optimizing) the regularization function, and not an approximation or surrogate. The authors derive a a PGD-styple iterative method and present convergence analysis for it. 

Thanks for the clarifications regarding the assumptions used in Section 3. It would be nice to include some of that in the manuscript.

The authors also propose a fast encoding scheme for their proposed method. 
The authors included a new experiment in semi-supervised consists of a very interesting use (of the method and the fast approximation). While this is an interesting addition, I think that using fast encoders is not particularly novel or the main part of the work. ""Converting"" iterative optimization algorithms into feed-forward nets for accelerating the inference process has been done in the past (several times with quite similar problems). Is natural that this can be done, and not very surprising. Maybe would be interesting to evaluate how important is to have an architecture matching the optimization algorithm, compared to a generic network (though some of this analysis has also been performed in the past).",1
"I'd like to thank the authors for their detailed response to my questions.

The paper proposes a support regularized version of sparse coding that takes into account the underlying manifold structure of the data. For this purpose, the authors augment the classic sparse coding loss with a term that encourages near by points to have similar active set. Convergence guarantees for the optimization procedure are presented. Experimental evaluation on clustering and semi-supervised learning shows the benefits of the proposed approach.

The paper is well written and a nice read. The most relevant contribution of this work is to including (and optimizing) the regularization function, and not an approximation or surrogate. The authors derive a a PGD-styple iterative method and present convergence analysis for it. 

Thanks for the clarifications regarding the assumptions used in Section 3. It would be nice to include some of that in the manuscript.

The authors also propose a fast encoding scheme for their proposed method. 
The authors included a new experiment in semi-supervised consists of a very interesting use (of the method and the fast approximation). While this is an interesting addition, I think that using fast encoders is not particularly novel or the main part of the work. ""Converting"" iterative optimization algorithms into feed-forward nets for accelerating the inference process has been done in the past (several times with quite similar problems). Is natural that this can be done, and not very surprising. Maybe would be interesting to evaluate how important is to have an architecture matching the optimization algorithm, compared to a generic network (though some of this analysis has also been performed in the past).",1
"This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs. The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding. 

I see two main drawbacks from this framework:
The augmented loss function has no trainable parameters and is used for only for regularization. This is not expected to give gains with large enough datasets. 
The augmented loss is heavily “engineered” to produce the desired result of parameter tying. It’s not clear what happens if you try to relax it a bit, by adding parameters, or estimating y~ in a different way. 

Nevertheless the argument is very interesting, and clearly written.
The simulated results indeed validate the argument, and the PTB results seem promising.

Minor comments:
Section 3:
Can you clarify if y~ is conditioned on the t example or on the entire history.
Eq. 3.5: i is enumerated over V (not |V|)",1
"This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs. The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding. 

I see two main drawbacks from this framework:
The augmented loss function has no trainable parameters and is used for only for regularization. This is not expected to give gains with large enough datasets. 
The augmented loss is heavily “engineered” to produce the desired result of parameter tying. It’s not clear what happens if you try to relax it a bit, by adding parameters, or estimating y~ in a different way. 

Nevertheless the argument is very interesting, and clearly written.
The simulated results indeed validate the argument, and the PTB results seem promising.

Minor comments:
Section 3:
Can you clarify if y~ is conditioned on the t example or on the entire history.
Eq. 3.5: i is enumerated over V (not |V|)",1
"This paper proposes an interesting framework (as a follow-up work of the author's previous paper) to learn compositional rules used to compose better music. The system consists of two components, a generative component (student) and a discriminative component (teacher). The generative component is a Probabilistic Graphical Models, generating the music following learned rules. The teacher compares the generated music with the empirical distribution of exemplar music (e.g, Bach’s chorales) and propose new rules for the student to learn so that it could improve.

The framework is different from GANs that the both the generative and discriminative components are interpretable. From the paper, it seems that the system can indeed learn sensible rules from the composed music and apply them in the next iteration, if trained in a curriculum manner. However, there is no comparison between the proposed system and its previous version, nor comparison between the proposed system and other simple baselines, e.g., an LSTM generative model. This might pose a concern here. 

I found this paper a bit hard to read, partly due to (1) lots of music terms (e.g, Tbl. 1 does not make sense to me) that hinders understanding of how the system performs, and (2) over-complicated math symbols and concept. For example, In Page 4, the concept of raw/high-level feature, Feature-Induced Partition and Conceptual Hierarchy, all means a non-overlapping hierarchical clustering on the 4-dimensional feature space. Also, there seems to be no hierarchy in Informational Hierarchy, but a list of rules. It would be much clearer if the authors write the paper in a plain way. 

Overall, the paper proposes a working system that seems to be interesting. But I am not confident enough to give strong conclusions.",1
"This paper proposes an interesting framework (as a follow-up work of the author's previous paper) to learn compositional rules used to compose better music. The system consists of two components, a generative component (student) and a discriminative component (teacher). The generative component is a Probabilistic Graphical Models, generating the music following learned rules. The teacher compares the generated music with the empirical distribution of exemplar music (e.g, Bach’s chorales) and propose new rules for the student to learn so that it could improve.

The framework is different from GANs that the both the generative and discriminative components are interpretable. From the paper, it seems that the system can indeed learn sensible rules from the composed music and apply them in the next iteration, if trained in a curriculum manner. However, there is no comparison between the proposed system and its previous version, nor comparison between the proposed system and other simple baselines, e.g., an LSTM generative model. This might pose a concern here. 

I found this paper a bit hard to read, partly due to (1) lots of music terms (e.g, Tbl. 1 does not make sense to me) that hinders understanding of how the system performs, and (2) over-complicated math symbols and concept. For example, In Page 4, the concept of raw/high-level feature, Feature-Induced Partition and Conceptual Hierarchy, all means a non-overlapping hierarchical clustering on the 4-dimensional feature space. Also, there seems to be no hierarchy in Informational Hierarchy, but a list of rules. It would be much clearer if the authors write the paper in a plain way. 

Overall, the paper proposes a working system that seems to be interesting. But I am not confident enough to give strong conclusions.",1
"This is an interesting paper on how to handle reparameterization in VAEs when you have discrete variables. The idea is to introduce a smoothing transformation that is shared between the generative model and the recognition model (leading to cancellations). 
A second contribution is to introduce an RBM as the prior model P(z) and to use autoregressive connections in generative and recognition models. The whole package becomes a bit entangled and complex and it is hard to figure out what causes the claimed good performance. Experiments that study these contributions separately would have been nice. 
The framework does become a little complex but this should not be a problem if nice software is delivered that can be used in a plug and play mode.
Overall, the paper is very rich with ideas so I think it would be a great contribution to the conference.",1
"This is an interesting paper on how to handle reparameterization in VAEs when you have discrete variables. The idea is to introduce a smoothing transformation that is shared between the generative model and the recognition model (leading to cancellations). 
A second contribution is to introduce an RBM as the prior model P(z) and to use autoregressive connections in generative and recognition models. The whole package becomes a bit entangled and complex and it is hard to figure out what causes the claimed good performance. Experiments that study these contributions separately would have been nice. 
The framework does become a little complex but this should not be a problem if nice software is delivered that can be used in a plug and play mode.
Overall, the paper is very rich with ideas so I think it would be a great contribution to the conference.",1
"This paper describes a careful experimental study on the CIFAR-10 task that uses data augmentation and Bayesian hyperparameter optimization to train a large number of high-quality, deep convolutional network classification models from hard (0-1) targets.  An ensemble of the 16 best models is then used as a teacher model in the distillation framework, where student models are trained to match the averaged logits from the teacher ensemble.  Data augmentation and Bayesian hyperparameter optimization is also applied in the training of the student models.  Both non-convolutional (MLP) and convolutional student models of varying depths and parameter counts are trained.  Convolutional models with the same architecture and parameter count as some of the convolutional students are also trained using hard targets and cross-entropy loss.  The experimental results show that convolutional students with only one or two convolutional layers are unable to match the results of students having more convolutional layers under the constraint that the number of parameters in all students is kept constant.

Pros
+ This is a very thorough and well designed study that make use of the best existing tools to try to answer the question of whether or not deep convolutional models need both depth and convolution.
+ It builds nicely on the preliminary results in Ba & Caruana, 2014.

Cons
- It is difficult to prove a negative, as the authors admit.  That said, this study is as convincing as possible given current theory and practice in deep learning.

Section 2.2 should state that the logits are unnormalized log-probabilities (they don't include the log partition function).

The paper does not follow the ICLR citation style.  Quoting from the template:  ""When the authors or the publication are included in the sentence, the citation should not be in parenthesis (as in “See Hinton et al. (2006) for more information.”). Otherwise, the citation should be in parenthesis (as in “Deep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).”).""

There are a few minor issues with English usage and typos that should be cleaned up in the final manuscript.

necessary when training student models with more than 1 convolutional layers → necessary when training student models with more than 1 convolutional layer

remaining 10,000 images as validation set → remaining 10,000 images as the validation set

evaluate the ensemble’s predictions (logits) on these samples, and save all data → evaluated the ensemble’s predictions (logits) on these samples, and saved all data

more detail about hyperparamter optimization → more detail about hyperparameter optimization

We trained 129 deep CNN models with spearmint → We trained 129 deep CNN models with Spearmint

The best model obtained an accuracy of 92.78%, the fifth best achieved 92.67%. → The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%.

the sizes and architectures of three best models → the sizes and architectures of the three best models

clearly suggests that convolutional is critical →  clearly suggests that convolution is critical

similarly from the hyperparameter-opimizer’s point of view → similarly from the hyperparameter-optimizer’s point of view",1
"This paper describes a careful experimental study on the CIFAR-10 task that uses data augmentation and Bayesian hyperparameter optimization to train a large number of high-quality, deep convolutional network classification models from hard (0-1) targets.  An ensemble of the 16 best models is then used as a teacher model in the distillation framework, where student models are trained to match the averaged logits from the teacher ensemble.  Data augmentation and Bayesian hyperparameter optimization is also applied in the training of the student models.  Both non-convolutional (MLP) and convolutional student models of varying depths and parameter counts are trained.  Convolutional models with the same architecture and parameter count as some of the convolutional students are also trained using hard targets and cross-entropy loss.  The experimental results show that convolutional students with only one or two convolutional layers are unable to match the results of students having more convolutional layers under the constraint that the number of parameters in all students is kept constant.

Pros
+ This is a very thorough and well designed study that make use of the best existing tools to try to answer the question of whether or not deep convolutional models need both depth and convolution.
+ It builds nicely on the preliminary results in Ba & Caruana, 2014.

Cons
- It is difficult to prove a negative, as the authors admit.  That said, this study is as convincing as possible given current theory and practice in deep learning.

Section 2.2 should state that the logits are unnormalized log-probabilities (they don't include the log partition function).

The paper does not follow the ICLR citation style.  Quoting from the template:  ""When the authors or the publication are included in the sentence, the citation should not be in parenthesis (as in “See Hinton et al. (2006) for more information.”). Otherwise, the citation should be in parenthesis (as in “Deep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).”).""

There are a few minor issues with English usage and typos that should be cleaned up in the final manuscript.

necessary when training student models with more than 1 convolutional layers → necessary when training student models with more than 1 convolutional layer

remaining 10,000 images as validation set → remaining 10,000 images as the validation set

evaluate the ensemble’s predictions (logits) on these samples, and save all data → evaluated the ensemble’s predictions (logits) on these samples, and saved all data

more detail about hyperparamter optimization → more detail about hyperparameter optimization

We trained 129 deep CNN models with spearmint → We trained 129 deep CNN models with Spearmint

The best model obtained an accuracy of 92.78%, the fifth best achieved 92.67%. → The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%.

the sizes and architectures of three best models → the sizes and architectures of the three best models

clearly suggests that convolutional is critical →  clearly suggests that convolution is critical

similarly from the hyperparameter-opimizer’s point of view → similarly from the hyperparameter-optimizer’s point of view",1
"Pros : 
- New representation with nice properties that are derived and compared with a mathematical baseline and background
- A simple algorithm to obtain the representation

Cons :
- The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first.",1
"Pros : 
- New representation with nice properties that are derived and compared with a mathematical baseline and background
- A simple algorithm to obtain the representation

Cons :
- The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first.",1
"This paper proposed an iterative query updating mechanism for cloze-style QA. The approach is novel and interesting and while it is only verified in the paper for two Cloze-style tasks (CBT and WDW), the concept of read/compose/write operations seem to be more general and can be potentially applied to other reasoning tasks beyond Cloze-style QA. Another advantage of the proposed model is to learn when to terminate the iteration by the so-called adaptive computation model, such that it avoids the issue of treating the number of iterations as another hyper-parameter, which is a common practice of iterative models/multi-hop reasoning in previous papers.

There are a couple places that this paper can improve. First, I would like to see the results from CNN/Daily Mail as well to have a more comprehensive comparison. Secondly, it will be useful to visualize the entire M^q sequence over time t (not just z or the query gating) to help understand better the query regression and if it is human interpretable.",1
"This paper proposed an iterative query updating mechanism for cloze-style QA. The approach is novel and interesting and while it is only verified in the paper for two Cloze-style tasks (CBT and WDW), the concept of read/compose/write operations seem to be more general and can be potentially applied to other reasoning tasks beyond Cloze-style QA. Another advantage of the proposed model is to learn when to terminate the iteration by the so-called adaptive computation model, such that it avoids the issue of treating the number of iterations as another hyper-parameter, which is a common practice of iterative models/multi-hop reasoning in previous papers.

There are a couple places that this paper can improve. First, I would like to see the results from CNN/Daily Mail as well to have a more comprehensive comparison. Secondly, it will be useful to visualize the entire M^q sequence over time t (not just z or the query gating) to help understand better the query regression and if it is human interpretable.",1
"This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents.
The paper is well written, clear in its presentation and backed up by good experiments.
They demonstrate that by forcing the network to predict motion has beneficial consequences on the classification of actions states,
allowing more accurate classification with less training data.
They also show how the information learned by the network is interpretable and organised in a hierarchy.

Weaknesses:
- a critical discussion on the interplay between motion an behaviour that is needed to experience the benefits of their proposed model is missing from the paper.
- moreover, a discussion on how this approach could scale to more challenging scenarios ""involving animals"" and visual input for instance and more general ""behaviours"" is also missing;
The criticism here is pointed at the fact that the title/abstract claim general behaviour modelling, whilst the experiments are focused on two very specific and relatively simple scenarios,
making the original claim a little bit far fetched unless its backed up by additional evidence.
Using ""Insects"", or ""fruit flies"" would be more appropriate than ""animals"".",1
"This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents.
The paper is well written, clear in its presentation and backed up by good experiments.
They demonstrate that by forcing the network to predict motion has beneficial consequences on the classification of actions states,
allowing more accurate classification with less training data.
They also show how the information learned by the network is interpretable and organised in a hierarchy.

Weaknesses:
- a critical discussion on the interplay between motion an behaviour that is needed to experience the benefits of their proposed model is missing from the paper.
- moreover, a discussion on how this approach could scale to more challenging scenarios ""involving animals"" and visual input for instance and more general ""behaviours"" is also missing;
The criticism here is pointed at the fact that the title/abstract claim general behaviour modelling, whilst the experiments are focused on two very specific and relatively simple scenarios,
making the original claim a little bit far fetched unless its backed up by additional evidence.
Using ""Insects"", or ""fruit flies"" would be more appropriate than ""animals"".",1
"This paper is a well written paper. This paper can be divided into 2 parts:
1.Adversary training on ImageNet 
2.Empirical study of label leak, single/multiple step attack, transferability and importance of model capacity

For part [1], I don’t think training without clean example will not make reasonable ImageNet level model. Ian’s experiment in “Explaining and Harnessing Adversarial Examples” didn't use BatchNorm, which may be important for training large scale model. This part looks like an extension to Ian’s work with Inception-V3 model. I suggest to add an experiment of training without clean samples.

For part [2], The experiments cover most variables in adversary training, yet lack technical depth.  The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM, if we do careful observe on some special MNIST FGSM example, we can find augmentation effect on numbers, which makes grey part on image to make the number look more like the other numbers. Although this effect is hard to be observed with complex data such as CIFAR-10 or ImageNet, they may be related to the authors' observation ""FGSM examples are most transferable"".  

In this part the authors raise many interesting problems or guess, but lack theoretical explanations. 

Overall I think these empirical observations are useful for future work.",1
"This paper is a well written paper. This paper can be divided into 2 parts:
1.Adversary training on ImageNet 
2.Empirical study of label leak, single/multiple step attack, transferability and importance of model capacity

For part [1], I don’t think training without clean example will not make reasonable ImageNet level model. Ian’s experiment in “Explaining and Harnessing Adversarial Examples” didn't use BatchNorm, which may be important for training large scale model. This part looks like an extension to Ian’s work with Inception-V3 model. I suggest to add an experiment of training without clean samples.

For part [2], The experiments cover most variables in adversary training, yet lack technical depth.  The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM, if we do careful observe on some special MNIST FGSM example, we can find augmentation effect on numbers, which makes grey part on image to make the number look more like the other numbers. Although this effect is hard to be observed with complex data such as CIFAR-10 or ImageNet, they may be related to the authors' observation ""FGSM examples are most transferable"".  

In this part the authors raise many interesting problems or guess, but lack theoretical explanations. 

Overall I think these empirical observations are useful for future work.",1
"This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.

The authors also did address the questions of the reviewers.

My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.",1
"This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.

The authors also did address the questions of the reviewers.

My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.",1
"This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet (in particular, C3D) to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in LSTM is used to generate the parameters in a Gaussian mixture model. Finally, the visual attention map is generated from the Gaussian mixture model.

Overall, the idea in this paper is reasonable and the paper is well written. RNN/LSTM has been used in lots of vision problem where the outputs are discrete sequences, there has not been much work on using RNN/LSTM for problems where the output is continuous like in this paper.

The experimental results have demonstrated the effectiveness of the proposed approach. In particular, it outperforms other state-of-the-art on the saliency prediction task on the Hollywood2 datasets. It also shows improvement over baselines (e.g. C3D + SVM) on the action recognition task.

My only ""gripe"" of this paper is that this paper is missing some important baseline comparisons. In particular, it does not seem to show how the ""recurrent"" part help the overall performance. Although Table 2 shows RMDN outperforms other state-of-the-art, it might be due to the fact that it uses strong C3D features (while other methods in Table 2 use traditional handcrafted features). Since saliency prediction is essentially a dense image labeling problem (similar to semantic segmentation). For dense image labeling, there has been lots of methods proposed in the past two years, e.g. fully convolution neural network (FCN) or deconvnet. A straightforward baseline is to simply take FCN and apply it on each frame. If the proposed method still outperforms this baseline, we can know that the ""recurrent"" part really helps.",1
"This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet (in particular, C3D) to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in LSTM is used to generate the parameters in a Gaussian mixture model. Finally, the visual attention map is generated from the Gaussian mixture model.

Overall, the idea in this paper is reasonable and the paper is well written. RNN/LSTM has been used in lots of vision problem where the outputs are discrete sequences, there has not been much work on using RNN/LSTM for problems where the output is continuous like in this paper.

The experimental results have demonstrated the effectiveness of the proposed approach. In particular, it outperforms other state-of-the-art on the saliency prediction task on the Hollywood2 datasets. It also shows improvement over baselines (e.g. C3D + SVM) on the action recognition task.

My only ""gripe"" of this paper is that this paper is missing some important baseline comparisons. In particular, it does not seem to show how the ""recurrent"" part help the overall performance. Although Table 2 shows RMDN outperforms other state-of-the-art, it might be due to the fact that it uses strong C3D features (while other methods in Table 2 use traditional handcrafted features). Since saliency prediction is essentially a dense image labeling problem (similar to semantic segmentation). For dense image labeling, there has been lots of methods proposed in the past two years, e.g. fully convolution neural network (FCN) or deconvnet. A straightforward baseline is to simply take FCN and apply it on each frame. If the proposed method still outperforms this baseline, we can know that the ""recurrent"" part really helps.",1
"The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts  in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs).

While the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation.
My SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case.

To summarize my understanding of the key theorem 1 result:
- The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer). So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition.
- In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (I^low, J^high), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear.

If tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting. Hopefully, someone will come one day with an explanation that holds in a single slide.



While this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant:

On the theory side, we are still very far from the completeness of the PAC bound papers of the ""shallow era"". In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition). Also, in the prediction of the inductive bias, the other half is missing. This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds). If we consider the expected risk as  the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk. 

On the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings. For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry. Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job. What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet (",1
"The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts  in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs).

While the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation.
My SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case.

To summarize my understanding of the key theorem 1 result:
- The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer). So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition.
- In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (I^low, J^high), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear.

If tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting. Hopefully, someone will come one day with an explanation that holds in a single slide.



While this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant:

On the theory side, we are still very far from the completeness of the PAC bound papers of the ""shallow era"". In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition). Also, in the prediction of the inductive bias, the other half is missing. This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds). If we consider the expected risk as  the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk. 

On the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings. For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry. Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job. What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet (",1
"SUMMARY 
This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. 

PROS 
Interesting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation. 

CONS 
The paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less). 

COMMENTS 
It would be interesting to study the ramifications of the presented observations for the case of deep(er) networks. 
Also, to study to what extent the proposed picture describes the totality of functions that are representable by the networks. 

MINOR COMMENTS 
- Figure 1 could be referenced first in the text.  
- ``Color coded'' where the color codes what? 
- Thank you for thinking about revising the points from my first questions. Note: Isometry on the manifold. 
- On page 5, mention how the orthogonal projection on S_k is realized in the network. 
- On page 6 ``divided into segments'' here `segments' is maybe not the best word. 
- On page 6 ``The mean relative error is 0.98'' what is the baseline here, or what does this number mean?",1
"SUMMARY 
This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. 

PROS 
Interesting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation. 

CONS 
The paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less). 

COMMENTS 
It would be interesting to study the ramifications of the presented observations for the case of deep(er) networks. 
Also, to study to what extent the proposed picture describes the totality of functions that are representable by the networks. 

MINOR COMMENTS 
- Figure 1 could be referenced first in the text.  
- ``Color coded'' where the color codes what? 
- Thank you for thinking about revising the points from my first questions. Note: Isometry on the manifold. 
- On page 5, mention how the orthogonal projection on S_k is realized in the network. 
- On page 6 ``divided into segments'' here `segments' is maybe not the best word. 
- On page 6 ``The mean relative error is 0.98'' what is the baseline here, or what does this number mean?",1
"This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.

This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.

It is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.

Even though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?

Overall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted.",1
"This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.

This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.

It is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.

Even though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?

Overall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted.",1
"From my original comments:

The results looks good but the baselines proposed are quite bad.

For instance in the table 2 ""Misclassification rate for a 784-1024-1024-1024-10 "" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see ""significant"" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers...

In CIFAR-10 experiments, i do not understand  why ""Sparsely-Connected 90% + Single-Precision Floating-Point"" is worse than ""Sparsely-Connected 90% + BinaryConnect"". So it is better to use binary than float. 

Again i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations). Therefore under my point of view the comparison between float and binary is not fair. This is a critic also for the original papers about binary and ternary precision. 

In fact with this convolutional network, floating (standard) precision we can get lower that 9% of error rate. Again bad baselines.

----

The authors reply still does not convince me.

I still think that the same technique should be applied on more challenging scenarios.",1
"From my original comments:

The results looks good but the baselines proposed are quite bad.

For instance in the table 2 ""Misclassification rate for a 784-1024-1024-1024-10 "" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see ""significant"" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers...

In CIFAR-10 experiments, i do not understand  why ""Sparsely-Connected 90% + Single-Precision Floating-Point"" is worse than ""Sparsely-Connected 90% + BinaryConnect"". So it is better to use binary than float. 

Again i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations). Therefore under my point of view the comparison between float and binary is not fair. This is a critic also for the original papers about binary and ternary precision. 

In fact with this convolutional network, floating (standard) precision we can get lower that 9% of error rate. Again bad baselines.

----

The authors reply still does not convince me.

I still think that the same technique should be applied on more challenging scenarios.",1
"*** Paper Summary ***

This paper applies adversarial and virtual adversarial training to LSTM for text classification. Since text inputs are discrete adversarial perturbation are applied to the (normalized) word embeddings. Extensive experiments are reported and demonstrate the advantage of these methods.

*** Review Summary ***

The paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.

*** Detailed Review ***

The paper reads well. I have only a few comments regarding experiments and link to prior resarch:

Experiments:

- In Table 2 (and for other datasets as well), could you include an SVM baseline? e.g. S Wang and C Manning 2012?
- As another baseline, did you consider dropping words, i.e. masking noise? It is generally better than dropout/gaussian noise for text application (e.g. denoising autoencoders)?
- I am not sure I understand why virtual adversarial is worse than the baseline in Table 5. If you tune epsilon, in the worse case you would get the same performance as the baseline? Was it that validation was unreliable?

Related Work:

I think it would be interesting to point at SVM, transductive SVM who achieve something similar to adversarial training. When maximizing the margin in a (transductive) SVM, it is equivalent to move the example toward the decision boundary, i.e. moving them in the direction of increase of the loss gradient.

Also it would be interesting to draw a parallel between adversarial training and contrastive divergence. The adversarial samples are very close in nature to the one step Markov Chain samples from CD. See Bengio 2009. Related to this technique are also approaches that try to explicitely cancel the Jacobian at data points, e.g. Rifai et al 2011.

*** References ***

Marginalized Denoising Autoencoders for Domain Adaptation. Minmin Chen, K Weinberger.
Stacked Denoising Autoencoders. Pascal Vincent. JMLR 2011.
Learning invariant features through local space contraction, Salah Rifai, Xavier Muller, Xavier Glorot, Gregoire Mesnil, Yoshua Bengio and Pascal Vincent, 2011.
Learning Deep Architectures for AI, Yoshua Bengio 2009
Large Scale Transductive SVMs. Ronan Collobert et al 2006
Optimization for Transductive SVM.  O Chapelle, V Sindhwani, SS Keerthi JMLR 2008",1
"*** Paper Summary ***

This paper applies adversarial and virtual adversarial training to LSTM for text classification. Since text inputs are discrete adversarial perturbation are applied to the (normalized) word embeddings. Extensive experiments are reported and demonstrate the advantage of these methods.

*** Review Summary ***

The paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.

*** Detailed Review ***

The paper reads well. I have only a few comments regarding experiments and link to prior resarch:

Experiments:

- In Table 2 (and for other datasets as well), could you include an SVM baseline? e.g. S Wang and C Manning 2012?
- As another baseline, did you consider dropping words, i.e. masking noise? It is generally better than dropout/gaussian noise for text application (e.g. denoising autoencoders)?
- I am not sure I understand why virtual adversarial is worse than the baseline in Table 5. If you tune epsilon, in the worse case you would get the same performance as the baseline? Was it that validation was unreliable?

Related Work:

I think it would be interesting to point at SVM, transductive SVM who achieve something similar to adversarial training. When maximizing the margin in a (transductive) SVM, it is equivalent to move the example toward the decision boundary, i.e. moving them in the direction of increase of the loss gradient.

Also it would be interesting to draw a parallel between adversarial training and contrastive divergence. The adversarial samples are very close in nature to the one step Markov Chain samples from CD. See Bengio 2009. Related to this technique are also approaches that try to explicitely cancel the Jacobian at data points, e.g. Rifai et al 2011.

*** References ***

Marginalized Denoising Autoencoders for Domain Adaptation. Minmin Chen, K Weinberger.
Stacked Denoising Autoencoders. Pascal Vincent. JMLR 2011.
Learning invariant features through local space contraction, Salah Rifai, Xavier Muller, Xavier Glorot, Gregoire Mesnil, Yoshua Bengio and Pascal Vincent, 2011.
Learning Deep Architectures for AI, Yoshua Bengio 2009
Large Scale Transductive SVMs. Ronan Collobert et al 2006
Optimization for Transductive SVM.  O Chapelle, V Sindhwani, SS Keerthi JMLR 2008",1
"This paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising. For example, they show that it is possible to reconstruct word order from bag of words representations, and they show that LSTM sentence autoencoders encode interpretable features even for randomly permuted nonsense sentences.

Effective unsupervised sentence representation learning is an important and largely unsolved problem in NLP, and this kind of work seems like it should be straightforwardly helpful towards that end. In addition, the experimental paradigm presented here is likely more broadly applicable to a range of representation learning systems. Some of the results seem somewhat strange, but I see no major technical concerns, and think that that they are informative. I recommend acceptance.

One minor red flag: 
- The massive drop in CBOW performance in Figures 1b and 4b are not explained, and seem implausible enough to warrant serious further investigation. Can you be absolutely certain that those results would appear with a different codebase and different random seed implementing the same model? Fortunately, this point is largely orthogonal to the major results of the paper.

Two writing comments:
- I agree that the results with word order and CBOW are surprising, but I think it's slightly misleading to say that CBOW is predictive of word order. It doesn't represent word order at all, but it's possible to probabilistically reconstruct word order from the information that it does encode.
- Saying that ""LSTM auto-encoders are more effective at encoding word order than word content"" doesn't really make sense. These two quantities aren't comparable.",1
"This paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising. For example, they show that it is possible to reconstruct word order from bag of words representations, and they show that LSTM sentence autoencoders encode interpretable features even for randomly permuted nonsense sentences.

Effective unsupervised sentence representation learning is an important and largely unsolved problem in NLP, and this kind of work seems like it should be straightforwardly helpful towards that end. In addition, the experimental paradigm presented here is likely more broadly applicable to a range of representation learning systems. Some of the results seem somewhat strange, but I see no major technical concerns, and think that that they are informative. I recommend acceptance.

One minor red flag: 
- The massive drop in CBOW performance in Figures 1b and 4b are not explained, and seem implausible enough to warrant serious further investigation. Can you be absolutely certain that those results would appear with a different codebase and different random seed implementing the same model? Fortunately, this point is largely orthogonal to the major results of the paper.

Two writing comments:
- I agree that the results with word order and CBOW are surprising, but I think it's slightly misleading to say that CBOW is predictive of word order. It doesn't represent word order at all, but it's possible to probabilistically reconstruct word order from the information that it does encode.
- Saying that ""LSTM auto-encoders are more effective at encoding word order than word content"" doesn't really make sense. These two quantities aren't comparable.",1
"This work is an extension of previous works on pointer models, that mixes its outputs with standard softmax outputs. 
The idea is appealing in general for context biasing and the specific approach appears quite simple.

The idea is novel to some extent, as previous paper had already tried to combine pointer-based and standard models,
but not as a mixture model, as in this paper.

The paper is clearly written and the results seem promising.
The new dataset the authors created (WikiText) also seems of high interest. 

A comment regarding notation:
The symbol p_ptr is used in two different ways in eq. 3 and eq. 5. : p_ptr(w) vs. p_ptr(y_i|x_i) 
This is confusing as these are two different domains: for eq 3. the domain is a *set* of words and for eq. 5 the domain is a *list* of context words.
It would be helpful to use different symbol for the two objects.",1
"This work is an extension of previous works on pointer models, that mixes its outputs with standard softmax outputs. 
The idea is appealing in general for context biasing and the specific approach appears quite simple.

The idea is novel to some extent, as previous paper had already tried to combine pointer-based and standard models,
but not as a mixture model, as in this paper.

The paper is clearly written and the results seem promising.
The new dataset the authors created (WikiText) also seems of high interest. 

A comment regarding notation:
The symbol p_ptr is used in two different ways in eq. 3 and eq. 5. : p_ptr(w) vs. p_ptr(y_i|x_i) 
This is confusing as these are two different domains: for eq 3. the domain is a *set* of words and for eq. 5 the domain is a *list* of context words.
It would be helpful to use different symbol for the two objects.",1
"The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The argument is that the conditional log. likelihood is an upper bound of the Bayes error which becomes lousy during training. The paper then proposes better bounds computed and optimized in an iterative algorithm. Extensions of this idea are developed for regularized losses and a weak form of policy learning. Tests are performed on different datasets.

An interesting aspect of the contribution is to revisit a well-accepted methodology for training classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the ideas pushed further. Globally, the paper lacks coherence and depth: the part on policy learning is not well connected to the rest of the paper and the link with RL is not motivated in the two examples (ROC optimization and uncertainties). The experimental part needs a rewriting, e.g. I did not find a legend for identifying the different curves in the figures, which makes difficult to appreciate the results.",1
"The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The argument is that the conditional log. likelihood is an upper bound of the Bayes error which becomes lousy during training. The paper then proposes better bounds computed and optimized in an iterative algorithm. Extensions of this idea are developed for regularized losses and a weak form of policy learning. Tests are performed on different datasets.

An interesting aspect of the contribution is to revisit a well-accepted methodology for training classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the ideas pushed further. Globally, the paper lacks coherence and depth: the part on policy learning is not well connected to the rest of the paper and the link with RL is not motivated in the two examples (ROC optimization and uncertainties). The experimental part needs a rewriting, e.g. I did not find a legend for identifying the different curves in the figures, which makes difficult to appreciate the results.",1
"The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data (in this case the sequence of subproofs) is a good launching point for possibly super-human performance. Super-human ATPs are clearly extremely valuable. Although relatively smaller than the original Go datasets, this dataset seems to be a great first step. Unfortunately, the ATP and HOL aspect of this work is not my area of expertise. I can't comment on the quality of this aspect.

It would be great to see future work scale up the baselines and integrate the networks into state of the art ATPs. The capacity of deep learning methods to scale and take advantage of larger datasets means there's a possibility of an iterative approach to improving ATPs: as the ATPs get stronger they may generate more data in the form of new theorems. This may be a long way off, but the possibility is exciting.",1
"The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data (in this case the sequence of subproofs) is a good launching point for possibly super-human performance. Super-human ATPs are clearly extremely valuable. Although relatively smaller than the original Go datasets, this dataset seems to be a great first step. Unfortunately, the ATP and HOL aspect of this work is not my area of expertise. I can't comment on the quality of this aspect.

It would be great to see future work scale up the baselines and integrate the networks into state of the art ATPs. The capacity of deep learning methods to scale and take advantage of larger datasets means there's a possibility of an iterative approach to improving ATPs: as the ATPs get stronger they may generate more data in the form of new theorems. This may be a long way off, but the possibility is exciting.",1
"This paper shows:

  1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units.
  2. Extensions of the previous results to more general function classes, such as smooth or vector-valued functions.
  3. Lower bounds on the neural network size, as a function of its number of layers. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures.

The paper is well written and easy to follow. The technical content, including the proofs in the Appendix, look correct. Although the proof techniques are simple (and are sometimes modifications of arguments by Gil, Telgarsky, or Dasgupta), they are brought together in a coherent manner to produce sharp results. Therefore, I am leaning toward acceptance.",1
"This paper shows:

  1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units.
  2. Extensions of the previous results to more general function classes, such as smooth or vector-valued functions.
  3. Lower bounds on the neural network size, as a function of its number of layers. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures.

The paper is well written and easy to follow. The technical content, including the proofs in the Appendix, look correct. Although the proof techniques are simple (and are sometimes modifications of arguments by Gil, Telgarsky, or Dasgupta), they are brought together in a coherent manner to produce sharp results. Therefore, I am leaning toward acceptance.",1
"This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).

Their model is organized as a set of layers that aim at capturing the information form different “level of abstraction”. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. 

The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.

Overall this paper presents a strong and novel model with promising experimental results.



On a minor note, I have few remarks/complaints about the writing and the related work:

- In the introduction:
“One of the key principles of learning in deep neural networks as well as in the human brain” : please provide evidence for the “human brain” part of this claim.
“For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances” I believe you re missing Mikolov et al. 2010 in the references.
“in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data”: missing reference to Lin et al., 1996

- in the related work:
“A more recent model, the clockwork RNN (CW-RNN) (Koutník et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)” : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.
While the above models focus on online prediction problems, where a prediction needs to be made…”: I believe there is a lot of missing references, in particular to Socher’s work or older recursive networks.
“The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)”: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.

Missing references:
“Recurrent neural network based language model.”, Mikolov et al. 2010
“Learning long-term dependencies in NARX recurrent neural networks”, Lin et al. 1996
“Sequence labelling in structured domains with hierarchical recurrent neural networks“, Fernandez et al. 2007
“Learning sequential tasks by incrementally adding  higher  orders”, Ring, 1993",1
"This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).

Their model is organized as a set of layers that aim at capturing the information form different “level of abstraction”. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. 

The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.

Overall this paper presents a strong and novel model with promising experimental results.



On a minor note, I have few remarks/complaints about the writing and the related work:

- In the introduction:
“One of the key principles of learning in deep neural networks as well as in the human brain” : please provide evidence for the “human brain” part of this claim.
“For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances” I believe you re missing Mikolov et al. 2010 in the references.
“in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data”: missing reference to Lin et al., 1996

- in the related work:
“A more recent model, the clockwork RNN (CW-RNN) (Koutník et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)” : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.
While the above models focus on online prediction problems, where a prediction needs to be made…”: I believe there is a lot of missing references, in particular to Socher’s work or older recursive networks.
“The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)”: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.

Missing references:
“Recurrent neural network based language model.”, Mikolov et al. 2010
“Learning long-term dependencies in NARX recurrent neural networks”, Lin et al. 1996
“Sequence labelling in structured domains with hierarchical recurrent neural networks“, Fernandez et al. 2007
“Learning sequential tasks by incrementally adding  higher  orders”, Ring, 1993",1
"This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.

Their proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don’t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of “Dropout as a bayesian approximation” and the insights and theorems in this paper might enable future work in that direction.",1
"This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.

Their proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don’t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of “Dropout as a bayesian approximation” and the insights and theorems in this paper might enable future work in that direction.",1
"*** Paper Summary ***

The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer. Two instances are presented (i) a CNN where each layer weight is computed from a lower dimensional layer embedding vector; (ii) an RNN where each layer weight is computed from a secondary RNN state.

*** Review Summary ***

Pros: 
- I like the idea of bringing multiplicative RNNs and their predecessors back into the spotlight. 
- LM and MT results are excellent.

Cons:  
- The paper could be better written. It is too long for the conference format and need refocussing. 
- On related work, the relation with multiplicative RNN and their generic tensor product predecessor (Order 2 networks, wrt C. Lee Giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated (by the way it is better to say that something is revisiting an old idea or training it at modern scale/on modern tasks than ommitting it).
- on focus, it is not clear if your goal is to achieve better performance or more compact networks. In the RNN section you lean toward the former, in the CNN section you seem to lean toward the latter.

I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. The relation with multiplicative/order 2 networks and eventual differences need to be explained.

*** Detailed Review ***

Multiplicative networks are an extremely powerfull architecture and bringing them back into the spotlight is excellent. This paper has excellent results but suffer poor presentation, lack of a clear focus. It spends time on details and ommit important points. In its current form, it is much too long to long and his not self contained without the appendices.

Spending more time on multiplicative RNNs, order 2 networks at the begining of the paper would be excellent. This will let you highlight the difference between this paper and earlier work. It would also be necessary to spend a little time on why multiplicative RNN were less used than gated RNN: it seems that the optimization problem their training involve is tricker and it would be helpful to explain whether you had a harder time tweaking optimization parameters or whether you needed longer training sessions compared to LSTMs, regular CNN. On name, I am not sure that ""hypernetwork"" help the reader understand better what the proposed architecture compared to multiplicative interactions.

In section 3.2, you seem to imply that there are different settings of hypernetworks that allow to vary from an RNN to a CNN, this is not clear to me, maybe you could show how this would work on a simple temporal problem with equations. 

The work on CNN and RNN are rather disconnected to me: for CNN, you seem to be interested in a low rank structure of the weights, showing that similar performance can be achieved with less weights. It is not clear to me why to pursue that goal. Do you expect speedups? less memory for embedded applications? In that case you should compare with alternative strategies, e.g. model compression (Caruana et al 2006, aka Dark Knowledge, Hinton et al 2014) or hashed networks (Chen et al 2015). 

For RNN, you seem to target better perplexity/BLEU and model compactness is not a priority. Instead of making the weights have a simpler structure, you make them richer, i.e. dependent over time. It seems in that case models might be bigger and take longer to train. You might want to comment on training time, inference time, memory requirement in that case, as you highlight it might be an important goal in the CNN section. Overall, I am not sure it helps to have this mixed message. I would rather see the paper fit in the conference format with the RNN results alone and a clearer explanation and defers the publications of the CNN results when a proper comparison with memory concerned methods is performed.

Some of the discussions are not clear to me, I am not sure what message the reader should get from Figure 2 or from the discussion on saturation statistics (p10, Figure 5). Similarly, I am not sure if Figure 4 is showing anything: everything should change more drastically at word boundaries even in a regular LSTM (states, gates units should look very different before/after a space); without such a comparison it is hard to see if this is unique to your network.

The results on handwriting generation are harder to compare for me. Log-loss are hard to understand, I have no sense whether the difference between models is significant (what would be the variance in this metric under boostrap sampling of the training set?). I am not sold either on qualitative metric were human can assess quality but human cannot evaluate if the network is repeating the training set. Did you thing at precision/recall metric for ink, possibly with some spatial tolerance ? (e.g. evaluation of segmentation tasks in vision).

The MT experiments are insufficiently discussed in the main text.

Overall, I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. You need to properly discuss the relation to multiplicative/order 2 networks and highlight the differences. Unclear discussion can be eliminated to make the experimental setup and the results presentation clearer in the main text.

*** References ***

M.W. Goudreau, C.L. Giles, S.T. Chakradhar, D. Chen, ""First-Order Vs. Second-Order Single Layer Recurrent Neural Networks,""IEEE Trans. on Neural Networks, 5 (3), p. 511, 1994.

Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil, ""Model Compression,"" The Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2006), August 2006, pp. 535-541.

Dark knowledge, G Hinton, O Vinyals, J Dean 2014

W. Chen, J. Wilson, S. Tyree, K. Weinberger and Y. Chen, Compressing Neural Networks with the Hashing Trick, Proc. International Conference on Machine Learning (ICML-15)",1
"*** Paper Summary ***

The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer. Two instances are presented (i) a CNN where each layer weight is computed from a lower dimensional layer embedding vector; (ii) an RNN where each layer weight is computed from a secondary RNN state.

*** Review Summary ***

Pros: 
- I like the idea of bringing multiplicative RNNs and their predecessors back into the spotlight. 
- LM and MT results are excellent.

Cons:  
- The paper could be better written. It is too long for the conference format and need refocussing. 
- On related work, the relation with multiplicative RNN and their generic tensor product predecessor (Order 2 networks, wrt C. Lee Giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated (by the way it is better to say that something is revisiting an old idea or training it at modern scale/on modern tasks than ommitting it).
- on focus, it is not clear if your goal is to achieve better performance or more compact networks. In the RNN section you lean toward the former, in the CNN section you seem to lean toward the latter.

I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. The relation with multiplicative/order 2 networks and eventual differences need to be explained.

*** Detailed Review ***

Multiplicative networks are an extremely powerfull architecture and bringing them back into the spotlight is excellent. This paper has excellent results but suffer poor presentation, lack of a clear focus. It spends time on details and ommit important points. In its current form, it is much too long to long and his not self contained without the appendices.

Spending more time on multiplicative RNNs, order 2 networks at the begining of the paper would be excellent. This will let you highlight the difference between this paper and earlier work. It would also be necessary to spend a little time on why multiplicative RNN were less used than gated RNN: it seems that the optimization problem their training involve is tricker and it would be helpful to explain whether you had a harder time tweaking optimization parameters or whether you needed longer training sessions compared to LSTMs, regular CNN. On name, I am not sure that ""hypernetwork"" help the reader understand better what the proposed architecture compared to multiplicative interactions.

In section 3.2, you seem to imply that there are different settings of hypernetworks that allow to vary from an RNN to a CNN, this is not clear to me, maybe you could show how this would work on a simple temporal problem with equations. 

The work on CNN and RNN are rather disconnected to me: for CNN, you seem to be interested in a low rank structure of the weights, showing that similar performance can be achieved with less weights. It is not clear to me why to pursue that goal. Do you expect speedups? less memory for embedded applications? In that case you should compare with alternative strategies, e.g. model compression (Caruana et al 2006, aka Dark Knowledge, Hinton et al 2014) or hashed networks (Chen et al 2015). 

For RNN, you seem to target better perplexity/BLEU and model compactness is not a priority. Instead of making the weights have a simpler structure, you make them richer, i.e. dependent over time. It seems in that case models might be bigger and take longer to train. You might want to comment on training time, inference time, memory requirement in that case, as you highlight it might be an important goal in the CNN section. Overall, I am not sure it helps to have this mixed message. I would rather see the paper fit in the conference format with the RNN results alone and a clearer explanation and defers the publications of the CNN results when a proper comparison with memory concerned methods is performed.

Some of the discussions are not clear to me, I am not sure what message the reader should get from Figure 2 or from the discussion on saturation statistics (p10, Figure 5). Similarly, I am not sure if Figure 4 is showing anything: everything should change more drastically at word boundaries even in a regular LSTM (states, gates units should look very different before/after a space); without such a comparison it is hard to see if this is unique to your network.

The results on handwriting generation are harder to compare for me. Log-loss are hard to understand, I have no sense whether the difference between models is significant (what would be the variance in this metric under boostrap sampling of the training set?). I am not sold either on qualitative metric were human can assess quality but human cannot evaluate if the network is repeating the training set. Did you thing at precision/recall metric for ink, possibly with some spatial tolerance ? (e.g. evaluation of segmentation tasks in vision).

The MT experiments are insufficiently discussed in the main text.

Overall, I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. You need to properly discuss the relation to multiplicative/order 2 networks and highlight the differences. Unclear discussion can be eliminated to make the experimental setup and the results presentation clearer in the main text.

*** References ***

M.W. Goudreau, C.L. Giles, S.T. Chakradhar, D. Chen, ""First-Order Vs. Second-Order Single Layer Recurrent Neural Networks,""IEEE Trans. on Neural Networks, 5 (3), p. 511, 1994.

Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil, ""Model Compression,"" The Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2006), August 2006, pp. 535-541.

Dark knowledge, G Hinton, O Vinyals, J Dean 2014

W. Chen, J. Wilson, S. Tyree, K. Weinberger and Y. Chen, Compressing Neural Networks with the Hashing Trick, Proc. International Conference on Machine Learning (ICML-15)",1
"Overall the paper address an important problem: how to evaluate more appropriately automatic dialogue responses given the fact that current practice to automatically evaluate (BLEU, METEOR, ...) is often insufficient and sometimes misleading. The proposed approach using an LSTM-based encoding of dialogue context, reference response and model response(s) that are then scored in a linearly transformed space. While the overall approach is simple it is also quite intuitiv and allows end-to-end training. As the authors rightly argue simplicity is a feature both for interpretation as well as for speed. 

The experimental section reports on quite a range of experiments that seem fine to me and aim to convince the reader about the applicability of the approach. As mentioned also by others more insights from the experiments would have been great. I mentioned an in-depth failure case analysis and I would also suggest to go beyond the current dataset to really show generalizability of the proposed approach. In my opinion the paper is somewhat weaker on that front that it should have been.

Overall I like the ideas put forward and the approach seems sensible though and the paper can thus be accepted.",0
"Overall the paper address an important problem: how to evaluate more appropriately automatic dialogue responses given the fact that current practice to automatically evaluate (BLEU, METEOR, ...) is often insufficient and sometimes misleading. The proposed approach using an LSTM-based encoding of dialogue context, reference response and model response(s) that are then scored in a linearly transformed space. While the overall approach is simple it is also quite intuitiv and allows end-to-end training. As the authors rightly argue simplicity is a feature both for interpretation as well as for speed. 

The experimental section reports on quite a range of experiments that seem fine to me and aim to convince the reader about the applicability of the approach. As mentioned also by others more insights from the experiments would have been great. I mentioned an in-depth failure case analysis and I would also suggest to go beyond the current dataset to really show generalizability of the proposed approach. In my opinion the paper is somewhat weaker on that front that it should have been.

Overall I like the ideas put forward and the approach seems sensible though and the paper can thus be accepted.",0
"The paper explores a simple approach to learning reward functions for reinforcement learning from visual observations of expert trajectories for cases were only little training data is available. To obtain descriptive rewards even under such challenging conditions the method re-uses a pre-trained neural network as feature extractor (this is similar to a large body of work on task transfer with neural nets in the area of computer vision) and represents the reward function as a weighted distance to features for automatically extracted ""key-frames"" of the provided expert trajectories.

The paper is well written and explains all involved concepts clearly while also embedding the presented approach in the literature on inverse reinforcement learning (IRL). The resulting algorithm is appealing due to its simplicity and could prove useful for many real world robotic applications. I have three main issues with the paper in its current form, if these can be addressed I believe the paper would be significantly strengthened:
1) Although the recursive splitting approach for extracting the ""key-frames"" seems reasonable and the feature selection is well motivated I am missing two baselines in the experiments:
   - what happens if the feature selection is disabled and the distance between all features is used ? will this immediately break the procedure ? If not, what is the trade-off here ? 
   - an even simpler baseline than what is proposed in the paper would be the following procedure: simply use all frames of the recorded trajectories, calculate the distance to them in feature space and weights them according to their time as in the approach proposed in the paper. How well would that work ?
2) I understand the desire to combine the extracted reward function with a simple RL method but believe the used simple controller could potentially introduce a significant bias in the experiments since it requires initialization from an expert trajectory. As a direct consequence of this initialization the RL procedure is already started close to a good solution and the extracted reward function is potentially only queried in a small region around what was observed in the initial set of images (perhaps with the exception of the human demonstrations). Without an additional experiment it is thus unclear how well the presented approach will work in combination with other RL methods for training the controller.
3) I understand that the low number of available images excludes training a deep neural net directly for the task at hand but one has to wonder how other baselines would do. What happens if one uses a random projection of the images to form a feature vector? How well would a distance measure using raw images (e.g. L2 norm of image differences) or a distance measure based on the first principal components work? It seems that occlusions etc. would exclude them from working well but without empirical evidence it is hard to confirm this.

Minor issues:
- Page 1: ""make use of ideas about imitation"" reads a bit awkwardly
- Page 3: ""We use the Inception network pre-trained ImageNet"" -> pre-trained for ImageNet classification
- Page 4: the definition of the transition function for the stochastic case seems broken
- Page 6: ""efficient enough to evaluate"" a bit strangely written sentence

Additional comments rather than real issues:
- The paper is mainly of empirical nature, little actual learning is performed to obtain the reward function and no theoretical advances are needed. This is not necessarily bad but makes the empirical evaluation all the more important. 
- While I liked the clear exposition the approach -- in the end -- boils down to computing quadratic distances to features of pre-extracted ""key-frames"", it is nice that you make a connection to standard IRL approaches in Section 2.1 but one could argue that this derivation is not strictly necessary.",0
"The paper explores a simple approach to learning reward functions for reinforcement learning from visual observations of expert trajectories for cases were only little training data is available. To obtain descriptive rewards even under such challenging conditions the method re-uses a pre-trained neural network as feature extractor (this is similar to a large body of work on task transfer with neural nets in the area of computer vision) and represents the reward function as a weighted distance to features for automatically extracted ""key-frames"" of the provided expert trajectories.

The paper is well written and explains all involved concepts clearly while also embedding the presented approach in the literature on inverse reinforcement learning (IRL). The resulting algorithm is appealing due to its simplicity and could prove useful for many real world robotic applications. I have three main issues with the paper in its current form, if these can be addressed I believe the paper would be significantly strengthened:
1) Although the recursive splitting approach for extracting the ""key-frames"" seems reasonable and the feature selection is well motivated I am missing two baselines in the experiments:
   - what happens if the feature selection is disabled and the distance between all features is used ? will this immediately break the procedure ? If not, what is the trade-off here ? 
   - an even simpler baseline than what is proposed in the paper would be the following procedure: simply use all frames of the recorded trajectories, calculate the distance to them in feature space and weights them according to their time as in the approach proposed in the paper. How well would that work ?
2) I understand the desire to combine the extracted reward function with a simple RL method but believe the used simple controller could potentially introduce a significant bias in the experiments since it requires initialization from an expert trajectory. As a direct consequence of this initialization the RL procedure is already started close to a good solution and the extracted reward function is potentially only queried in a small region around what was observed in the initial set of images (perhaps with the exception of the human demonstrations). Without an additional experiment it is thus unclear how well the presented approach will work in combination with other RL methods for training the controller.
3) I understand that the low number of available images excludes training a deep neural net directly for the task at hand but one has to wonder how other baselines would do. What happens if one uses a random projection of the images to form a feature vector? How well would a distance measure using raw images (e.g. L2 norm of image differences) or a distance measure based on the first principal components work? It seems that occlusions etc. would exclude them from working well but without empirical evidence it is hard to confirm this.

Minor issues:
- Page 1: ""make use of ideas about imitation"" reads a bit awkwardly
- Page 3: ""We use the Inception network pre-trained ImageNet"" -> pre-trained for ImageNet classification
- Page 4: the definition of the transition function for the stochastic case seems broken
- Page 6: ""efficient enough to evaluate"" a bit strangely written sentence

Additional comments rather than real issues:
- The paper is mainly of empirical nature, little actual learning is performed to obtain the reward function and no theoretical advances are needed. This is not necessarily bad but makes the empirical evaluation all the more important. 
- While I liked the clear exposition the approach -- in the end -- boils down to computing quadratic distances to features of pre-extracted ""key-frames"", it is nice that you make a connection to standard IRL approaches in Section 2.1 but one could argue that this derivation is not strictly necessary.",0
"First, let me praise the authors for generating and releasing an NLP data set: a socially useful task.

The authors use an algorithm to generate a 500-cluster-per-language data set in semantic similarity. This brings up a few points.

1. If the point of using the algorithm is to be scalable, why release such a small data set? It's roughly the same order of magnitude as the data sets released in the SemEval tasks over the recent years. I would have expected something orders of magnitude larger.

2. The authors hand checked a small subset of the clusters: they found one where it was ambiguous, and should probably have been removed. Mechanical Turk can scale pretty well -- why not post-facto filter all of the clusters using MT? This is (in effect) how ImageNet was created, and it has millions of items.

3. Evaluating data set papers is an tricky issue. What makes a data set ""good"" or publishable? There are a number of medium-sized NLP data sets released every year (e.g., through SemEval). Those are designed to address tasks in NLP that people find interesting. I don't know of a data set that exactly addresses the task that the authors propose: the task is trying to address the idea of semantic similarity, which has had multiple data sets thrown at it since SemEval 2012. I wish that the paper had included comparisons to show that the particular task / data combination is better suited for analyzing semantic similarity than other existing data sets.

Two final notes:

A. This paper doesn't seem very well-suited to ICLR.  New NLP data sets may be indirectly useful for evaluating word embeddings (and hence representations). But, I didn't learn much from the paper: GloVe is empirically less good for semantic similarity than other embeddings? If true, why? That would be interesting.

B. The first proposal for the ""put a word into a cluster and see if it stands out"" task (in the context of human evaluation of topic models), is

Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei. Reading Tea Leaves: HowHumans Interpret Topic Models. Neural Information Processing Systems, 2009

which deserves a citation, I think.",0
"First, let me praise the authors for generating and releasing an NLP data set: a socially useful task.

The authors use an algorithm to generate a 500-cluster-per-language data set in semantic similarity. This brings up a few points.

1. If the point of using the algorithm is to be scalable, why release such a small data set? It's roughly the same order of magnitude as the data sets released in the SemEval tasks over the recent years. I would have expected something orders of magnitude larger.

2. The authors hand checked a small subset of the clusters: they found one where it was ambiguous, and should probably have been removed. Mechanical Turk can scale pretty well -- why not post-facto filter all of the clusters using MT? This is (in effect) how ImageNet was created, and it has millions of items.

3. Evaluating data set papers is an tricky issue. What makes a data set ""good"" or publishable? There are a number of medium-sized NLP data sets released every year (e.g., through SemEval). Those are designed to address tasks in NLP that people find interesting. I don't know of a data set that exactly addresses the task that the authors propose: the task is trying to address the idea of semantic similarity, which has had multiple data sets thrown at it since SemEval 2012. I wish that the paper had included comparisons to show that the particular task / data combination is better suited for analyzing semantic similarity than other existing data sets.

Two final notes:

A. This paper doesn't seem very well-suited to ICLR.  New NLP data sets may be indirectly useful for evaluating word embeddings (and hence representations). But, I didn't learn much from the paper: GloVe is empirically less good for semantic similarity than other embeddings? If true, why? That would be interesting.

B. The first proposal for the ""put a word into a cluster and see if it stands out"" task (in the context of human evaluation of topic models), is

Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei. Reading Tea Leaves: HowHumans Interpret Topic Models. Neural Information Processing Systems, 2009

which deserves a citation, I think.",0
"Some of the key details in this paper are very poorly explained or not even explained at all. The model sounds interesting and there may be something good here, but it should not be published in it's current form. 

Specific comments:

The description of the R_l,pi convolutions in Section 2.1 was unclear. Specifically, I wasn't confident that I understood what the labels pi represented.

The description of the SAEN structure in section 2.2 was worded poorly. My understanding, based on Equation 1, is that the 'shift' operation is simply a summation of the representations of the member objects, and that the 'aggregate' operation simply concatenates the representations from multiple relations.  In the 'shift' step, it seems more appropriate to average over the object's member's representations h_j, rather than sum over them.

The compression technique presented in Section 2.3 requires that multiple objects at a level have the same representation. Why would this ever occur, given that the representations are real valued and high-dimensional? The text is unintelligible: ""two objects are equivalent if they are made by same sets of parts for all the pi-parameterizations of the R_l,pi decomposition relation."" 

The 'ego graph patterns' in Figure 1 and 'Ego Graph  Neural Network' used in the experiments are never explained in the text, and no references are given. Because of this, I cannot comment on the quality of the experiments.",0
"Some of the key details in this paper are very poorly explained or not even explained at all. The model sounds interesting and there may be something good here, but it should not be published in it's current form. 

Specific comments:

The description of the R_l,pi convolutions in Section 2.1 was unclear. Specifically, I wasn't confident that I understood what the labels pi represented.

The description of the SAEN structure in section 2.2 was worded poorly. My understanding, based on Equation 1, is that the 'shift' operation is simply a summation of the representations of the member objects, and that the 'aggregate' operation simply concatenates the representations from multiple relations.  In the 'shift' step, it seems more appropriate to average over the object's member's representations h_j, rather than sum over them.

The compression technique presented in Section 2.3 requires that multiple objects at a level have the same representation. Why would this ever occur, given that the representations are real valued and high-dimensional? The text is unintelligible: ""two objects are equivalent if they are made by same sets of parts for all the pi-parameterizations of the R_l,pi decomposition relation."" 

The 'ego graph patterns' in Figure 1 and 'Ego Graph  Neural Network' used in the experiments are never explained in the text, and no references are given. Because of this, I cannot comment on the quality of the experiments.",0
"This paper develops a differentiable interpreter for the Forth programming
language. This enables writing a program ""sketch"" (a program with parts left
out), with a hole to be filled in based upon learning from input-output
examples. The main technical development is to start with an abstract machine
for the Forth language, and then to make all of the operations differentiable.
The technique for making operations differentiable is analogous to what is done
in models like Neural Turing Machine and Stack RNN. Special syntax is developed
for specifying holes, which gives the pattern about what data should be read
when filling in the hole, which data should be written, and what the rough
structure of the model that fills the hole should be. Motivation for why one
should want to do this is that it enables composing program sketches with other
differentiable models like standard neural networks, but the experiments focus
on sorting and addition tasks with relatively small degrees of freedom for how
to fill in the holes.

Experimentally, result show that sorting and addition can be learned given
strong sketches.

The aim of this paper is very ambitious: convert a full programming language to
be differentiable, and I admire this ambition. The idea is provocative and I
think will inspire people in the ICLR community.

The main weakness is that the experiments are somewhat trivial and there are no
baselines. I believe that simply enumerating possible values to fill in the
holes would work better, and if that is possible, then it's not clear to me what
is practically gained from this formulation. (The authors argue that the point
is to compose differentiable Forth sketches with neural networks sitting below,
but if the holes can be filled by brute force, then could the underlying neural
network not be separately trained to maximize the probability assigned to any
filling of the hole that produces the correct input-output behavior?)

Related, one thing that is missing, in my opinion, is a more nuanced outlook of
where the authors believe this work is going. Based on the small scale of the
experiments and from reading other related papers in the area, I sense that it
is hard to scale up differentiable forth to large real-world problems. It
would be nice to have more discussion about this, and perhaps even an experiment
that demonstrates a failure case. Is there a problem that is somewhat more
complex than the ones that appear in the paper where the approach does not work?
What has been tried to make it work? What are the failure modes? What are the
challenges that the authors believe need to be overcome to make this work.

Overall, I think this paper deserves consideration for being provocative.
However, I'm hesitant to strongly recommend acceptance because the experiments
are weak.",0
"This paper develops a differentiable interpreter for the Forth programming
language. This enables writing a program ""sketch"" (a program with parts left
out), with a hole to be filled in based upon learning from input-output
examples. The main technical development is to start with an abstract machine
for the Forth language, and then to make all of the operations differentiable.
The technique for making operations differentiable is analogous to what is done
in models like Neural Turing Machine and Stack RNN. Special syntax is developed
for specifying holes, which gives the pattern about what data should be read
when filling in the hole, which data should be written, and what the rough
structure of the model that fills the hole should be. Motivation for why one
should want to do this is that it enables composing program sketches with other
differentiable models like standard neural networks, but the experiments focus
on sorting and addition tasks with relatively small degrees of freedom for how
to fill in the holes.

Experimentally, result show that sorting and addition can be learned given
strong sketches.

The aim of this paper is very ambitious: convert a full programming language to
be differentiable, and I admire this ambition. The idea is provocative and I
think will inspire people in the ICLR community.

The main weakness is that the experiments are somewhat trivial and there are no
baselines. I believe that simply enumerating possible values to fill in the
holes would work better, and if that is possible, then it's not clear to me what
is practically gained from this formulation. (The authors argue that the point
is to compose differentiable Forth sketches with neural networks sitting below,
but if the holes can be filled by brute force, then could the underlying neural
network not be separately trained to maximize the probability assigned to any
filling of the hole that produces the correct input-output behavior?)

Related, one thing that is missing, in my opinion, is a more nuanced outlook of
where the authors believe this work is going. Based on the small scale of the
experiments and from reading other related papers in the area, I sense that it
is hard to scale up differentiable forth to large real-world problems. It
would be nice to have more discussion about this, and perhaps even an experiment
that demonstrates a failure case. Is there a problem that is somewhat more
complex than the ones that appear in the paper where the approach does not work?
What has been tried to make it work? What are the failure modes? What are the
challenges that the authors believe need to be overcome to make this work.

Overall, I think this paper deserves consideration for being provocative.
However, I'm hesitant to strongly recommend acceptance because the experiments
are weak.",0
"I have no familiarity with the HJI PDE (I've only dealt with parabolic PDE's such as diffusion in the past). So the details of transforming this problem into a supervised loss escape me. Therefore, as indicated below, my review should be taken as an ""educated guess"". I imagine that many readers of ICLR will face a similar problem as me, and so, if this paper is accepted, at the least the authors should prepare an appendix that provides an introduction to the HJI PDE. At a high level, my comments are:

1. It seems that another disadvantage of this approach is that a new network must be trained for each new domain (including domain size), system function f(x) or boundary condition. If that is correct, I wonder if it's worth the trouble when existing tools already solve these PDE's. Can the authors shed light on a more ""unifying approach"" that would require minimal changes to generalize across PDE's?

2. How sensitive is the network's result to domains of different sizes? It seems only a single size 51 x 51 was tested. Do errors increase with domain size?

3. How general is this approach to PDE's of other types e.g. diffusion?",0
"I have no familiarity with the HJI PDE (I've only dealt with parabolic PDE's such as diffusion in the past). So the details of transforming this problem into a supervised loss escape me. Therefore, as indicated below, my review should be taken as an ""educated guess"". I imagine that many readers of ICLR will face a similar problem as me, and so, if this paper is accepted, at the least the authors should prepare an appendix that provides an introduction to the HJI PDE. At a high level, my comments are:

1. It seems that another disadvantage of this approach is that a new network must be trained for each new domain (including domain size), system function f(x) or boundary condition. If that is correct, I wonder if it's worth the trouble when existing tools already solve these PDE's. Can the authors shed light on a more ""unifying approach"" that would require minimal changes to generalize across PDE's?

2. How sensitive is the network's result to domains of different sizes? It seems only a single size 51 x 51 was tested. Do errors increase with domain size?

3. How general is this approach to PDE's of other types e.g. diffusion?",0
"This paper describes an approach to learning the non-linear activation function in deep neural nets.  This is achieved by representing the activation function in a basis of non-linear functions and learning the coefficients.  Authors use Fourier basis in the paper.  A theoretical analysis of the proposed approach is also presented, using algorithmic stability arguments, to demonstrate good generalization behavior (vanishing generalization error with large data sets) of networks with learnt non-linearities.

The main question I have about this paper is that writing a non-linear activation function as a linear or affine combination of other non-linear basis functions is equivalent to making a larger network whose nodes have the basis functions as non-linearities and whose weights have certain constraints on them.  Thus, the value of the proposed approach of learning non-linearities over optimizing network capacity for a given task (with fixed non-linearities) is not clear to me.  Or could it be argued that the constrained implied by learnt non-linearity approach are somehow good thing to do?

Another question - In the two stage training process for CNNs, when ReLU activation is replaced by NPFC(L,T), is the NPFC(L,T) activation initialized to approximate ReLU, or is it initialized using random coefficients?

Few minor corrections/questions:
- Pg 2. “ … the interval [-L+T, L+T] …” should be “ … the interval [-L+T, L-T] … “ ?
- Pg 2., Equation for f(x), should it be “ (-L+T) i \pi x / L “ in both sin and cos terms, or without “ x “ ?
- Theorem 4.2 “ … some algorithm \eps-uniformly stable …” remove the word “algorithm”
- Theorem 4.5.  SGM undefined",0
"This paper describes an approach to learning the non-linear activation function in deep neural nets.  This is achieved by representing the activation function in a basis of non-linear functions and learning the coefficients.  Authors use Fourier basis in the paper.  A theoretical analysis of the proposed approach is also presented, using algorithmic stability arguments, to demonstrate good generalization behavior (vanishing generalization error with large data sets) of networks with learnt non-linearities.

The main question I have about this paper is that writing a non-linear activation function as a linear or affine combination of other non-linear basis functions is equivalent to making a larger network whose nodes have the basis functions as non-linearities and whose weights have certain constraints on them.  Thus, the value of the proposed approach of learning non-linearities over optimizing network capacity for a given task (with fixed non-linearities) is not clear to me.  Or could it be argued that the constrained implied by learnt non-linearity approach are somehow good thing to do?

Another question - In the two stage training process for CNNs, when ReLU activation is replaced by NPFC(L,T), is the NPFC(L,T) activation initialized to approximate ReLU, or is it initialized using random coefficients?

Few minor corrections/questions:
- Pg 2. “ … the interval [-L+T, L+T] …” should be “ … the interval [-L+T, L-T] … “ ?
- Pg 2., Equation for f(x), should it be “ (-L+T) i \pi x / L “ in both sin and cos terms, or without “ x “ ?
- Theorem 4.2 “ … some algorithm \eps-uniformly stable …” remove the word “algorithm”
- Theorem 4.5.  SGM undefined",0
"*** Paper Summary ***

The paper proposes to learn a predictive model (aka predict the next video frames given an input image) and uses the prediction from this model to improve a supervised classifier. The effectiveness of the approach is illustrated on a tower stability dataset.

*** Review Summary ***

This work seems rather preliminary in terms of experimentation and using forward modeling as pretraining has already been proposed and applied to video and text classification tasks. Discussion on related work is insufficient. The end task choice (will there be motion?) might not be the best to advocate for unsupervised training.

*** Detailed Review ***

This work seems rather preliminary. There is no comparison with alternative semi-supervised strategies. Any approach that consider the next frames as latent variables (or privileged information) can be considered. Also I am not sure if the supervised stability prediction model is actually needed once the next frame is predicted. Basically the task can be reduced to predict whether there will be motion in the video following the current frame or not (for instance comparing the first frame and last prediction or the density of gray in the top part of the video might work just as well). Also training a model to predict the presence of motion from the unsupervised data only would probably do very well. I would suggest to stir away from task where the label can be inferred trivially from the unsupervised data, meaning that unlabeled videos can be considered labeled frames in that case.

The related work section misses a discussion on previous work on learning unsupervised features from video (through predictive models, dimensionality reduction...) for helping classification of still images or videos [Fathi et al 2008; Mabahi et al 2009; Srivastava et al 2015]. More recently, Wang and Gupta (2015) have obtained excellent ImageNet results from features pre trained on unlabeled videos. Vondrick et al (2016) have shown that generative models of video can help initialize models for video classification tasks. Also in the field of text classification, pre training of classifier with a language model is a form predictive modeling, e.g. Dai & Le 2015.

I would also suggest to report test results on the dataset from Lerrer et al 2016 (I understand that you need your own videos to pre train the predictive model) but stability prediction only require still images.

Overall, I feel the experimental section is too preliminary. It would be better to focus on a task where solving the unsupervised task does not necessarily imply that the supervised task is trivially solved (or conversely that a simple rule can turn the unlabeled data into label data).

*** Reference ***

Fathi, Alireza, and Greg Mori. ""Action recognition by learning mid-level motion features."" Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008.
Mobahi, Hossein, Ronan Collobert, and Jason Weston. ""Deep learning from temporal coherence in video."" Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009.
Srivastava, Nitish, Elman Mansimov, and Ruslan Salakhutdinov. ""Unsupervised learning of video representations using lstms."" CoRR, abs/1502.04681 2 (2015).
A. Dai, Q.V. Le, Semi-supervised Sequence Learning, NIPS, 2015
Unsupervised learning of visual representations using videos, X Wang, A Gupta, ICCV 2015 
Generating videos with scene dynamics, C Vondrick, H Pirsiavash, A Torralba, NIPS 16",0
"*** Paper Summary ***

The paper proposes to learn a predictive model (aka predict the next video frames given an input image) and uses the prediction from this model to improve a supervised classifier. The effectiveness of the approach is illustrated on a tower stability dataset.

*** Review Summary ***

This work seems rather preliminary in terms of experimentation and using forward modeling as pretraining has already been proposed and applied to video and text classification tasks. Discussion on related work is insufficient. The end task choice (will there be motion?) might not be the best to advocate for unsupervised training.

*** Detailed Review ***

This work seems rather preliminary. There is no comparison with alternative semi-supervised strategies. Any approach that consider the next frames as latent variables (or privileged information) can be considered. Also I am not sure if the supervised stability prediction model is actually needed once the next frame is predicted. Basically the task can be reduced to predict whether there will be motion in the video following the current frame or not (for instance comparing the first frame and last prediction or the density of gray in the top part of the video might work just as well). Also training a model to predict the presence of motion from the unsupervised data only would probably do very well. I would suggest to stir away from task where the label can be inferred trivially from the unsupervised data, meaning that unlabeled videos can be considered labeled frames in that case.

The related work section misses a discussion on previous work on learning unsupervised features from video (through predictive models, dimensionality reduction...) for helping classification of still images or videos [Fathi et al 2008; Mabahi et al 2009; Srivastava et al 2015]. More recently, Wang and Gupta (2015) have obtained excellent ImageNet results from features pre trained on unlabeled videos. Vondrick et al (2016) have shown that generative models of video can help initialize models for video classification tasks. Also in the field of text classification, pre training of classifier with a language model is a form predictive modeling, e.g. Dai & Le 2015.

I would also suggest to report test results on the dataset from Lerrer et al 2016 (I understand that you need your own videos to pre train the predictive model) but stability prediction only require still images.

Overall, I feel the experimental section is too preliminary. It would be better to focus on a task where solving the unsupervised task does not necessarily imply that the supervised task is trivially solved (or conversely that a simple rule can turn the unlabeled data into label data).

*** Reference ***

Fathi, Alireza, and Greg Mori. ""Action recognition by learning mid-level motion features."" Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008.
Mobahi, Hossein, Ronan Collobert, and Jason Weston. ""Deep learning from temporal coherence in video."" Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009.
Srivastava, Nitish, Elman Mansimov, and Ruslan Salakhutdinov. ""Unsupervised learning of video representations using lstms."" CoRR, abs/1502.04681 2 (2015).
A. Dai, Q.V. Le, Semi-supervised Sequence Learning, NIPS, 2015
Unsupervised learning of visual representations using videos, X Wang, A Gupta, ICCV 2015 
Generating videos with scene dynamics, C Vondrick, H Pirsiavash, A Torralba, NIPS 16",0
"This paper introduces a polynomial linear model for supervised classification tasks. The model is based on a combination of the Tensor Train (TT) tensor decomposition method and a form of stochastic Riemannian  optimization. A few empirical experiments are performed that demonstrate the good performance of the proposed model relative to appropriate baselines.

From a theoretical standpoint, I think the approach is interesting and elegant. The main machinery underlying this work are the TT decomposition and the geometric structure of the manifold of tensors with fixed TT-rank, which have been established in prior work. The novelty of this paper is in the combination of this machinery to form an efficient polynomial linear model. As such, I would have hoped that the paper mainly focused on the efficacy of this combination and how it is superior to obvious alternatives. For example, I would have really appreciated seeing how FMs performed when optimized over the manifold of positive definite matrices, as another reviewer mentioned. Instead, there is a bit too much effort devoted to explaining prior work.

I think the empirical analysis could be substantially improved. I am particularly puzzled by the significant performance boost obtained from initializing with the ordinary logistic regression solution. I would have liked some further analysis of this effect, especially whether or not it is possible to obtain a similar performance boost with other models. Regarding the synthetic data, I think an important baseline would be against a vanilla feed forward neural network, which would help readers understand how complicated the interactions are and how difficult the dataset is to model. I agree with the previous reviewer regarding a variety of other possible improvements to the experimental section.

A few typos: 'Bernoulli distrbution', 'reproduce the experiemnts', 'generilize better'.

Overall, I am on the fence regarding this paper. The main idea is quite good, but insufficient attention was devoted to analyzing the aspects of the model that make it interesting and novel.",0
"This paper introduces a polynomial linear model for supervised classification tasks. The model is based on a combination of the Tensor Train (TT) tensor decomposition method and a form of stochastic Riemannian  optimization. A few empirical experiments are performed that demonstrate the good performance of the proposed model relative to appropriate baselines.

From a theoretical standpoint, I think the approach is interesting and elegant. The main machinery underlying this work are the TT decomposition and the geometric structure of the manifold of tensors with fixed TT-rank, which have been established in prior work. The novelty of this paper is in the combination of this machinery to form an efficient polynomial linear model. As such, I would have hoped that the paper mainly focused on the efficacy of this combination and how it is superior to obvious alternatives. For example, I would have really appreciated seeing how FMs performed when optimized over the manifold of positive definite matrices, as another reviewer mentioned. Instead, there is a bit too much effort devoted to explaining prior work.

I think the empirical analysis could be substantially improved. I am particularly puzzled by the significant performance boost obtained from initializing with the ordinary logistic regression solution. I would have liked some further analysis of this effect, especially whether or not it is possible to obtain a similar performance boost with other models. Regarding the synthetic data, I think an important baseline would be against a vanilla feed forward neural network, which would help readers understand how complicated the interactions are and how difficult the dataset is to model. I agree with the previous reviewer regarding a variety of other possible improvements to the experimental section.

A few typos: 'Bernoulli distrbution', 'reproduce the experiemnts', 'generilize better'.

Overall, I am on the fence regarding this paper. The main idea is quite good, but insufficient attention was devoted to analyzing the aspects of the model that make it interesting and novel.",0
"The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph. Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework. Experiments on synthetic and real-world datasets show promising results compared with baselines.

In general, I think it is an interesting and novel paper. The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics. The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso.

The experiment results are also promising. In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region. 

However, the paper can be made clearer in describing the network architectures. For example, in page 5, each o^k_{i,j} is said be a d-dimensional vector. But from the context, it seems o^k_{i,j} is a scalar (from o^0_{i,j} = p_{i,j}). It is not clear what o^k_{i,j} is exactly and what d is. Is it the number of channels for the convolutional filters?

Figure 1 is also quite confusing. Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes? And from the figure, it seems there is only one channel in each layer? What do the black squares represent and why are there three blocks of them. There are some descriptions in the text, but it is still not clear what they mean exactly.

For real-world data, how are the training data (Y, Sigma) generated? Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse? This is also related to the more general question of how to sample from the distribution P, in the case of real-world data.",0
"The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph. Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework. Experiments on synthetic and real-world datasets show promising results compared with baselines.

In general, I think it is an interesting and novel paper. The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics. The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso.

The experiment results are also promising. In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region. 

However, the paper can be made clearer in describing the network architectures. For example, in page 5, each o^k_{i,j} is said be a d-dimensional vector. But from the context, it seems o^k_{i,j} is a scalar (from o^0_{i,j} = p_{i,j}). It is not clear what o^k_{i,j} is exactly and what d is. Is it the number of channels for the convolutional filters?

Figure 1 is also quite confusing. Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes? And from the figure, it seems there is only one channel in each layer? What do the black squares represent and why are there three blocks of them. There are some descriptions in the text, but it is still not clear what they mean exactly.

For real-world data, how are the training data (Y, Sigma) generated? Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse? This is also related to the more general question of how to sample from the distribution P, in the case of real-world data.",0
"This paper considers the energy-based model interpretation of GAN, where the discriminator is an unnormalized model for the likelihood of a generative model p(x|theta) and the generator is a directed model that approximates this distribution. The generator is used to draw approximate negative phase samples that are used in stochastic maximum likelihood / contrastive divergence learning of the EBM / discriminator.

The main idea in the paper is to fit the generator by following the Stein variational gradient. In practice this gradient consists of the usual gradient provided by the discriminator with an added term that provides a repulsive force between the sampled data points to increase sample diversity.

The idea of using a kernel to push apart the sampled points is interesting, and will work in low dimensions, but it is hard to see how it can work in full scale images. For high dimensional samples x, the proposed kernel is unlikely to provide a useful distance measure between points. There are no convincing experiments in the paper that show otherwise. Specifically:

- There is no experiment that compares between standard GAN and GAN + repulsion, using the same architecture. (please address this in the rebuttal)
- If the Stein variational idea is taken literally, the right thing to do would be to fully optimize the generator at every step, and then taking a single optimization step on the discriminator. Instead, each is updated in turn, and the learning rates of both steps are adjusted to keep the two ""in line"".
- The kernel used to fit the generator is defined in the auto-encoder space of the discriminator, and thus depends on the discriminator parameters. The objective that is used to fit the generator thus changes at every step, and the procedure can no longer be interpreted as stochastic gradient descent with respect to any single well defined objective.

The authors obtain good results: The generated images clearly look better than those generated by DCGAN. However, their approach has a number of changes compared to DCGAN, so it is not clear where the improvement comes from. In addition, by now the DCGAN is no longer a very strong baseline, as various other techniques have been proposed.

Note: The use of phi for both the ""particle gradient direction"" and energy function is confusing",0
"This paper considers the energy-based model interpretation of GAN, where the discriminator is an unnormalized model for the likelihood of a generative model p(x|theta) and the generator is a directed model that approximates this distribution. The generator is used to draw approximate negative phase samples that are used in stochastic maximum likelihood / contrastive divergence learning of the EBM / discriminator.

The main idea in the paper is to fit the generator by following the Stein variational gradient. In practice this gradient consists of the usual gradient provided by the discriminator with an added term that provides a repulsive force between the sampled data points to increase sample diversity.

The idea of using a kernel to push apart the sampled points is interesting, and will work in low dimensions, but it is hard to see how it can work in full scale images. For high dimensional samples x, the proposed kernel is unlikely to provide a useful distance measure between points. There are no convincing experiments in the paper that show otherwise. Specifically:

- There is no experiment that compares between standard GAN and GAN + repulsion, using the same architecture. (please address this in the rebuttal)
- If the Stein variational idea is taken literally, the right thing to do would be to fully optimize the generator at every step, and then taking a single optimization step on the discriminator. Instead, each is updated in turn, and the learning rates of both steps are adjusted to keep the two ""in line"".
- The kernel used to fit the generator is defined in the auto-encoder space of the discriminator, and thus depends on the discriminator parameters. The objective that is used to fit the generator thus changes at every step, and the procedure can no longer be interpreted as stochastic gradient descent with respect to any single well defined objective.

The authors obtain good results: The generated images clearly look better than those generated by DCGAN. However, their approach has a number of changes compared to DCGAN, so it is not clear where the improvement comes from. In addition, by now the DCGAN is no longer a very strong baseline, as various other techniques have been proposed.

Note: The use of phi for both the ""particle gradient direction"" and energy function is confusing",0
"The paper proposes an extension of weight normalization / normalization propagation to recurrent neural networks. Simple experiments suggest it works well.

The contribution is potentially useful to a lot of people, as LSTMs are one of the basic building blocks in our field.

The contribution is not extremely novel: the change with respect to weight normalization is minor. The experiments are also not very convincing: Layer normalization is reported to have higher test error as it overfits on their example, but in terms of optimization it seems to work better. Also the authors don't seem to use the data dependent parameter init for weight normalization as proposed in that paper.",0
"The paper proposes an extension of weight normalization / normalization propagation to recurrent neural networks. Simple experiments suggest it works well.

The contribution is potentially useful to a lot of people, as LSTMs are one of the basic building blocks in our field.

The contribution is not extremely novel: the change with respect to weight normalization is minor. The experiments are also not very convincing: Layer normalization is reported to have higher test error as it overfits on their example, but in terms of optimization it seems to work better. Also the authors don't seem to use the data dependent parameter init for weight normalization as proposed in that paper.",0
"While I understand the difficulty of collecting audio data from animals, I think this type of feature engineering does not go in the right direction. I would rather see a model than learns the feature representation from data.  I would think it should be possible to collect a more substantial corpus in zoos / nature etc, and then train a generative model. The underlying learned feature representation could be then used to feed a classifier. I'm not familiar with the particularities of this task, it's hard to judge the improvements by using chirplets.",0
"While I understand the difficulty of collecting audio data from animals, I think this type of feature engineering does not go in the right direction. I would rather see a model than learns the feature representation from data.  I would think it should be possible to collect a more substantial corpus in zoos / nature etc, and then train a generative model. The underlying learned feature representation could be then used to feed a classifier. I'm not familiar with the particularities of this task, it's hard to judge the improvements by using chirplets.",0
"In this paper, the author analyzes the convergence dynamics of a single layer non-linear network under Gaussian iid input assumptions. The first half of the paper, dealing with a single hidden node, was somewhat clear, although I have some specific questions below. The second half, dealing with multiple hidden nodes, was very difficult for me to understand, and the final ""punchline"" is quite unclear. I think the author should focus on intuition and hide detailed derivations and symbols in an appendix. 

In terms of significance, it is very hard for me to be sure how generalizable these results are: the Gaussian assumption is a very strong one, and so is the assumption of iid inputs. Real-world feature inputs are highly correlated and are probably not Gaussian. Such assumptions are not made (as far as I can tell) in recent papers analyzing the convergence of deep networks e.g. Kawaguchi, NIPS 2016. Although the author says the no assumption is made on the independence of activations, this assumption is shifted to the input instead. I think this means that the activations are combinations of iid random variables, and are probably Gaussian like, right? So I'm not sure where this leaves us.

Specific comments:

1. Please use D_w instead of D to show that D is a function of w, and not a constant. This gets particularly confusing when switching to D(w) and D(e) in Section 3. In general, notation in the paper is hard to follow and should be clearly introduced.

2. Section 3, statement that says ""when the neuron is cut off at sample l, then (D^(t))_u"" what is the relationship between l and u? Also, this is another example of notational inconsistency that causes problems to the reader.

3. Section 3.1, what is F(e, w) and why is D(e) introduced? This was unclear to me.

4. Theorem 3.3 suggests that (if \epsilon is > 0), then to have the maximal probability of convergence, \epsilon should be very close to 0, which means that the ball B_r has radius r -> 0? This seems contradictory from Figure 2. 

5. Section 4 was really unclear and I still do not understand what the symmetry group really represents. Is there an intuitive explanation why this is important?

6. Figure 5: what is a_j ?

I encourage the author to rewrite this paper for clarity. In it's present form, it would be very difficult to understand the takeaways from the paper.",0
"In this paper, the author analyzes the convergence dynamics of a single layer non-linear network under Gaussian iid input assumptions. The first half of the paper, dealing with a single hidden node, was somewhat clear, although I have some specific questions below. The second half, dealing with multiple hidden nodes, was very difficult for me to understand, and the final ""punchline"" is quite unclear. I think the author should focus on intuition and hide detailed derivations and symbols in an appendix. 

In terms of significance, it is very hard for me to be sure how generalizable these results are: the Gaussian assumption is a very strong one, and so is the assumption of iid inputs. Real-world feature inputs are highly correlated and are probably not Gaussian. Such assumptions are not made (as far as I can tell) in recent papers analyzing the convergence of deep networks e.g. Kawaguchi, NIPS 2016. Although the author says the no assumption is made on the independence of activations, this assumption is shifted to the input instead. I think this means that the activations are combinations of iid random variables, and are probably Gaussian like, right? So I'm not sure where this leaves us.

Specific comments:

1. Please use D_w instead of D to show that D is a function of w, and not a constant. This gets particularly confusing when switching to D(w) and D(e) in Section 3. In general, notation in the paper is hard to follow and should be clearly introduced.

2. Section 3, statement that says ""when the neuron is cut off at sample l, then (D^(t))_u"" what is the relationship between l and u? Also, this is another example of notational inconsistency that causes problems to the reader.

3. Section 3.1, what is F(e, w) and why is D(e) introduced? This was unclear to me.

4. Theorem 3.3 suggests that (if \epsilon is > 0), then to have the maximal probability of convergence, \epsilon should be very close to 0, which means that the ball B_r has radius r -> 0? This seems contradictory from Figure 2. 

5. Section 4 was really unclear and I still do not understand what the symmetry group really represents. Is there an intuitive explanation why this is important?

6. Figure 5: what is a_j ?

I encourage the author to rewrite this paper for clarity. In it's present form, it would be very difficult to understand the takeaways from the paper.",0
"In this paper authors propose a novel data augmentation scheme where instead of augmenting the input data, they augment intermediate feature representations.  Sequence auto-encoder based features are considered, and random perturbation, feature interpolation, and extrapolation based augmentation are evaluated. On three sequence classification tasks and on MNIST and CIFAR-10, it is shown that augmentation in feature space, specifically extrapolation based augmentation, results in good accuracy gains w.r.t. authors baseline.

My main questions and suggestions for further strengthening the paper are:

a) The proposed data augmentation approach is applied to a learnt auto-encoder based feature space termed ‘context vector’ in the paper.  The context vectors are then augmented and used as input to train classification models. Have the authors considered applying their feature space augmentation idea directly to the classification model during training, and applying it to potentially many layers of the model?  Also, have the authors considered convolutional neural network (CNN) architectures as well for feature space augmentation?  CNNs are now the state-of-the-art in many image and sequence classification task, it would be very valuable to see the impact of the proposed approach in that model.

b) When interpolation or extrapolation based augmentation was being applied, did the authors also consider utilizing nearby samples from competing classes as well?  Especially in case of extrapolation based augmentation it will be interesting to check if the extrapolated features are closer to competing classes than original ones.

c) With random interpolation or nearest neighbor interpolation based augmentation the accuracy seems to degrade pretty consistently.  This is counter-intuitive.  Do the authors have explanation for why the accuracy degraded with interpolation based augmentation?

d) The results on MNIST and CIFAR-10 are inconclusive.  For instance the error rate on CIFAR-10 is well below 10% these days, so I think it is hard to draw conclusions based on error rates above 30%.  For MNIST it is surprising to see that data augmentation in the input space substantially degrades the accuracy (1.093% -> 1.477%).  As mentioned above, I think this will require extending the feature space augmentation idea to CNN based models.",0
"In this paper authors propose a novel data augmentation scheme where instead of augmenting the input data, they augment intermediate feature representations.  Sequence auto-encoder based features are considered, and random perturbation, feature interpolation, and extrapolation based augmentation are evaluated. On three sequence classification tasks and on MNIST and CIFAR-10, it is shown that augmentation in feature space, specifically extrapolation based augmentation, results in good accuracy gains w.r.t. authors baseline.

My main questions and suggestions for further strengthening the paper are:

a) The proposed data augmentation approach is applied to a learnt auto-encoder based feature space termed ‘context vector’ in the paper.  The context vectors are then augmented and used as input to train classification models. Have the authors considered applying their feature space augmentation idea directly to the classification model during training, and applying it to potentially many layers of the model?  Also, have the authors considered convolutional neural network (CNN) architectures as well for feature space augmentation?  CNNs are now the state-of-the-art in many image and sequence classification task, it would be very valuable to see the impact of the proposed approach in that model.

b) When interpolation or extrapolation based augmentation was being applied, did the authors also consider utilizing nearby samples from competing classes as well?  Especially in case of extrapolation based augmentation it will be interesting to check if the extrapolated features are closer to competing classes than original ones.

c) With random interpolation or nearest neighbor interpolation based augmentation the accuracy seems to degrade pretty consistently.  This is counter-intuitive.  Do the authors have explanation for why the accuracy degraded with interpolation based augmentation?

d) The results on MNIST and CIFAR-10 are inconclusive.  For instance the error rate on CIFAR-10 is well below 10% these days, so I think it is hard to draw conclusions based on error rates above 30%.  For MNIST it is surprising to see that data augmentation in the input space substantially degrades the accuracy (1.093% -> 1.477%).  As mentioned above, I think this will require extending the feature space augmentation idea to CNN based models.",0
"The authors contribute an algorithm for building sum-product networks (SPNs) from data, assuming a Gaussian distribution for all dimensions of the observed data.  Due to the restricted structure of the SPN architecture, building a valid architecture that is tailored to a specific dataset is not an obvious exercise, and so structure-learning algorithms are employed.  For Gaussian distributed observations, the authors state that the previous state of the art is to chose a random SPN that satisfies the completeness and decomposibility constraints that SPNs must observe, and to then learn the parameters (as done in Jaini 2016).  In the contributed manuscript, the algorithm begins with a completely factorized model, and then by passing through the data, builds up more structure, while updating appropriate node statistics to maintain the validity of the SPN.

The above Jaini reference figures heavily into the reading of the paper because it is (to my limited knowledge) the previous work SOTA on SPNs applied to Gaussian distributed data, and also because the authors of the current manuscript compare performance to datasets studied in Jaini et al.  I personally was unfamiliar with most of these datasets, and so have no basis to judge loglikelihoods, given a particular model, as being either good or poor.  Nevertheless, the current manuscript reports results on these datasets that better (5 / 7) than other methods, such as SPNS (constructed randomly), Stacked Restricted Boltzmann Machines or Generative Moment Matching networks.

Overall: 
First let me say, I am not really qualified to make a decision on the acceptance or rejection of this manuscript (yet I am forced to make just such a choice) because I am not an expert in SPNs. I was also unfamiliar with the datasets, so I had no intuitive understanding of the algorithms performance, even when viewed as a black-box.  The algorithm is presented without theoretical inspiration or justification.  These latter are by no means a bad thing, but it again gives me little hold onto when evaluating the manuscript.  The manuscript is clearly written, and to my limited knowledge novel, and their algorithm does a good job (5/7) on selected datasets.  

My overall impression is that there isn't very much work here (e.g., much of the text is similar to Jaini, and most of the other experiments are repeated verbatim from Jaini), but again I may be missing something (this manuscript DOES mostly Jaini). I say this mostly because I am unfamiliar with the datasets.  Hopefully my reviewing peers will have enough background to know if the results are impressive or not, and my review should be weighted minimally.

Smallish Problems
I wanted to see nonuniform covariances in the data of the the toy task (Fig 3) for each gaussian component.

The SPN construction method has two obvious hyper parameters, it is important to see how those parameters affect the graph structure. (I submitted this as a pre-review question, to which the authors responded that they would look into this.)",0
"The authors contribute an algorithm for building sum-product networks (SPNs) from data, assuming a Gaussian distribution for all dimensions of the observed data.  Due to the restricted structure of the SPN architecture, building a valid architecture that is tailored to a specific dataset is not an obvious exercise, and so structure-learning algorithms are employed.  For Gaussian distributed observations, the authors state that the previous state of the art is to chose a random SPN that satisfies the completeness and decomposibility constraints that SPNs must observe, and to then learn the parameters (as done in Jaini 2016).  In the contributed manuscript, the algorithm begins with a completely factorized model, and then by passing through the data, builds up more structure, while updating appropriate node statistics to maintain the validity of the SPN.

The above Jaini reference figures heavily into the reading of the paper because it is (to my limited knowledge) the previous work SOTA on SPNs applied to Gaussian distributed data, and also because the authors of the current manuscript compare performance to datasets studied in Jaini et al.  I personally was unfamiliar with most of these datasets, and so have no basis to judge loglikelihoods, given a particular model, as being either good or poor.  Nevertheless, the current manuscript reports results on these datasets that better (5 / 7) than other methods, such as SPNS (constructed randomly), Stacked Restricted Boltzmann Machines or Generative Moment Matching networks.

Overall: 
First let me say, I am not really qualified to make a decision on the acceptance or rejection of this manuscript (yet I am forced to make just such a choice) because I am not an expert in SPNs. I was also unfamiliar with the datasets, so I had no intuitive understanding of the algorithms performance, even when viewed as a black-box.  The algorithm is presented without theoretical inspiration or justification.  These latter are by no means a bad thing, but it again gives me little hold onto when evaluating the manuscript.  The manuscript is clearly written, and to my limited knowledge novel, and their algorithm does a good job (5/7) on selected datasets.  

My overall impression is that there isn't very much work here (e.g., much of the text is similar to Jaini, and most of the other experiments are repeated verbatim from Jaini), but again I may be missing something (this manuscript DOES mostly Jaini). I say this mostly because I am unfamiliar with the datasets.  Hopefully my reviewing peers will have enough background to know if the results are impressive or not, and my review should be weighted minimally.

Smallish Problems
I wanted to see nonuniform covariances in the data of the the toy task (Fig 3) for each gaussian component.

The SPN construction method has two obvious hyper parameters, it is important to see how those parameters affect the graph structure. (I submitted this as a pre-review question, to which the authors responded that they would look into this.)",0
"It is interesting to derive such a bound and show it satisfies a regret bound along with empirical evidence on the CIFAR-10 for cross entropy loss and auto encoder for MSE loss. At least empirically, by comparing the observed training loss and taylor loss, the better a particular optimizer performs (training loss statement, not a validation or observed test statement) the smaller the difference between these two. Also shown is the regret loss is satisfied at different scales of the network, by layer, neuron and whole network. 

The Taylor approximation can be used to investigate activation configurations of the network, and used to connect this to difficulty in optimizing  at kinks in the loss surface, along with an empirical study of exploration of activation surface of the SGD/Adam/RMSprop optimizers, the more exploration the better the resulting training loss.

Not that it impacts the paper but the weaker performance of the SGD could be related to the fixed learning rate, if we anneal this learning rate, which should improve performance, does this translate to more exploration and tightening between the actual loss and the Taylor loss? 

- It might be useful to use a cross validation set for some of the empirical studies, in the end we would like to say something about generalization of the resulting network

- Is there a reason the subscript on the Jacobian changes to a_l in the",0
"It is interesting to derive such a bound and show it satisfies a regret bound along with empirical evidence on the CIFAR-10 for cross entropy loss and auto encoder for MSE loss. At least empirically, by comparing the observed training loss and taylor loss, the better a particular optimizer performs (training loss statement, not a validation or observed test statement) the smaller the difference between these two. Also shown is the regret loss is satisfied at different scales of the network, by layer, neuron and whole network. 

The Taylor approximation can be used to investigate activation configurations of the network, and used to connect this to difficulty in optimizing  at kinks in the loss surface, along with an empirical study of exploration of activation surface of the SGD/Adam/RMSprop optimizers, the more exploration the better the resulting training loss.

Not that it impacts the paper but the weaker performance of the SGD could be related to the fixed learning rate, if we anneal this learning rate, which should improve performance, does this translate to more exploration and tightening between the actual loss and the Taylor loss? 

- It might be useful to use a cross validation set for some of the empirical studies, in the end we would like to say something about generalization of the resulting network

- Is there a reason the subscript on the Jacobian changes to a_l in the",0
"Pros:
* Clearly written.
* New model mLSTM which seems to be useful according to the results.
* Some interesting experiments on big data.

Cons:
* Number of parameters in comparisons of different models is missing.
* mLSTM is behind some other models in most tasks.",0
"Pros:
* Clearly written.
* New model mLSTM which seems to be useful according to the results.
* Some interesting experiments on big data.

Cons:
* Number of parameters in comparisons of different models is missing.
* mLSTM is behind some other models in most tasks.",0
"The authors propose a solution for the task of synthesizing melodies. The authors claim that the ""language-model""-type approaches with LSTMs generate melodies with certain shortcomings. They tend to lack long-range structure, to repeat notes etc. To solve this problem the authors suggest that the model could be first trained as a pure LM-style LSTM and then trained with reinforcement learning to optimize an objective which includes some non-differentiable music-theory related constraints. 

The reinforcement learning methodology is appropriate but straightforward and closely resembles previous work for text modeling and dialogue generation. By itself the methodology doesn't offer a new technique. 

To me, the paper's contribution then comes down to the novelty / utility / impact of the application. The authors clearly put substantial of effort into crafting the rules and user study and that is commendable. On the other hand, music itself is dealt with somewhat naively. While the user study reflects hard work, it seems premature. The semi-plausible piano melodies here are only music in the way that LSTM Shakespeare passes as poetry. So it's analogous to conducting a user study comparing LSTM Shakespeare to n-gram Shakespeare. 

I'd caution the author's against the uncritical motivation that a problem has previously been studied. Research contains abundant dead ends (not to say this is necessarily one) and the burden to motivate research shouldn't be forgotten. This is especially true when the application is the primary thrust of a paper.

Generally the authors should be careful about describing this model as ""composing"". By analogy to a Shakespeare-LSTM, the language model is not really composing English prose. The relationship between constructing a statistical sequence model and creating art - an activity that involves communication grounded in real-world semantics should not be overstated. 

I appreciate the authors' efforts to respond to some criticisms of the problem setup and encourage them to anticipate these arguments in the paper and to better motivate the work in the future. If the main contribution is the application (the methods have been used elsewhere), then the motivation is of central importance. I also appreciate their contention that the field benefits from multiple datasets and not simply relying on language modeling. Further, they are correct in asserting that MIDI can capture all the information in a score (not merely ""Gameboy music"", and that for some musics (e.g. European classical) the score is of central importance. However, the authors may overstate the role of a score in jazz music.

Overall, for me, the application, while fun, doesn't add enough to the impact of the paper. And the methodology, while appropriate, doesn't stand on its own. 

--Update-- Thanks for your modifications and arguments. I've revised my scores to add a point.",0
"The authors propose a solution for the task of synthesizing melodies. The authors claim that the ""language-model""-type approaches with LSTMs generate melodies with certain shortcomings. They tend to lack long-range structure, to repeat notes etc. To solve this problem the authors suggest that the model could be first trained as a pure LM-style LSTM and then trained with reinforcement learning to optimize an objective which includes some non-differentiable music-theory related constraints. 

The reinforcement learning methodology is appropriate but straightforward and closely resembles previous work for text modeling and dialogue generation. By itself the methodology doesn't offer a new technique. 

To me, the paper's contribution then comes down to the novelty / utility / impact of the application. The authors clearly put substantial of effort into crafting the rules and user study and that is commendable. On the other hand, music itself is dealt with somewhat naively. While the user study reflects hard work, it seems premature. The semi-plausible piano melodies here are only music in the way that LSTM Shakespeare passes as poetry. So it's analogous to conducting a user study comparing LSTM Shakespeare to n-gram Shakespeare. 

I'd caution the author's against the uncritical motivation that a problem has previously been studied. Research contains abundant dead ends (not to say this is necessarily one) and the burden to motivate research shouldn't be forgotten. This is especially true when the application is the primary thrust of a paper.

Generally the authors should be careful about describing this model as ""composing"". By analogy to a Shakespeare-LSTM, the language model is not really composing English prose. The relationship between constructing a statistical sequence model and creating art - an activity that involves communication grounded in real-world semantics should not be overstated. 

I appreciate the authors' efforts to respond to some criticisms of the problem setup and encourage them to anticipate these arguments in the paper and to better motivate the work in the future. If the main contribution is the application (the methods have been used elsewhere), then the motivation is of central importance. I also appreciate their contention that the field benefits from multiple datasets and not simply relying on language modeling. Further, they are correct in asserting that MIDI can capture all the information in a score (not merely ""Gameboy music"", and that for some musics (e.g. European classical) the score is of central importance. However, the authors may overstate the role of a score in jazz music.

Overall, for me, the application, while fun, doesn't add enough to the impact of the paper. And the methodology, while appropriate, doesn't stand on its own. 

--Update-- Thanks for your modifications and arguments. I've revised my scores to add a point.",0
"This paper proposes a relation network (RN) to model relations between input entities such as objects.  The relation network is built in two stages.  First a lower-level structure analyzes a pair of input entities.  All pairs of input entities are fed to this structure.  Next, the output of this lower-level structure is aggregated across all input pairs via a simple sum.  This is used as the input to a higher-level structure.  In the basic version, these two structures are each multi-layer perceptrons (MLPs).

Overall, this is an interesting approach to understanding relations among entities.  The core idea is clear and well-motivated -- pooling techniques that induce invariance can be used to learn relations.  The idea builds on pooling structures (e.g. spatial/temporal average/max pooling) to focus on pairwise relations.  The current pairwise approach could potentially be extended to higher-order interactions, modulo scaling issues.

Experiments on scene descriptions and images verify the efficacy of relation networks.  The MLP baselines used are incapable of modeling the structured dependencies present in these tasks.  It would be interesting to know if pooling operators (e.g. across-object max pooling in an MLP) or data augmentation via permutation would be effective for training MLPs at these tasks.  Regardless, the model proposed here is novel and effective at handling relations and shows promise for higher-level reasoning tasks.",0
"This paper proposes a relation network (RN) to model relations between input entities such as objects.  The relation network is built in two stages.  First a lower-level structure analyzes a pair of input entities.  All pairs of input entities are fed to this structure.  Next, the output of this lower-level structure is aggregated across all input pairs via a simple sum.  This is used as the input to a higher-level structure.  In the basic version, these two structures are each multi-layer perceptrons (MLPs).

Overall, this is an interesting approach to understanding relations among entities.  The core idea is clear and well-motivated -- pooling techniques that induce invariance can be used to learn relations.  The idea builds on pooling structures (e.g. spatial/temporal average/max pooling) to focus on pairwise relations.  The current pairwise approach could potentially be extended to higher-order interactions, modulo scaling issues.

Experiments on scene descriptions and images verify the efficacy of relation networks.  The MLP baselines used are incapable of modeling the structured dependencies present in these tasks.  It would be interesting to know if pooling operators (e.g. across-object max pooling in an MLP) or data augmentation via permutation would be effective for training MLPs at these tasks.  Regardless, the model proposed here is novel and effective at handling relations and shows promise for higher-level reasoning tasks.",0
"This paper presents a generative model of video sequence data where the frames are assumed to be generated by a static background with a 2d sprite composited onto it at each timestep.  The sprite itself is allowed to dynamically change its appearance and location within the image from frame to frame.  This paper follows the VAE (Variational Autoencoder) approach, where a recognition/inference network allows them to recover the latent state at each timestep.

Some results are presented on simple synthetic data (such as a moving rectangle on a black background or the “Moving MNIST” data.  However, the results are preliminary and I suspect that the assumptions used in the paper are far too strong too be useful in real videos.  On the Moving MNIST data, the numerical results are not competitive to state of the art numbers.

The model itself is also not particularly novel and the work currently misses some relevant citations.  The form of the forward model, for example, could be viewed as a variation on the DRAW paper by Gregor et al (ICML 2014).  Efficient Inference in Occlusion-Aware Generative Models of Images by Huang & Murphy (ICLR) is another relevant work, which used a variational auto-encoder with a spatial transformer and an RNN-like sequence model to model the appearance of multiple sprites on a background.

Finally, the exposition in this paper is short on many details and I don’t believe that the paper is reproducible from the text alone.  For example, it is not clear what the form of the recognition model is…  Low-level details (which are very important) are also not presented, such as initialization strategy.",0
"This paper presents a generative model of video sequence data where the frames are assumed to be generated by a static background with a 2d sprite composited onto it at each timestep.  The sprite itself is allowed to dynamically change its appearance and location within the image from frame to frame.  This paper follows the VAE (Variational Autoencoder) approach, where a recognition/inference network allows them to recover the latent state at each timestep.

Some results are presented on simple synthetic data (such as a moving rectangle on a black background or the “Moving MNIST” data.  However, the results are preliminary and I suspect that the assumptions used in the paper are far too strong too be useful in real videos.  On the Moving MNIST data, the numerical results are not competitive to state of the art numbers.

The model itself is also not particularly novel and the work currently misses some relevant citations.  The form of the forward model, for example, could be viewed as a variation on the DRAW paper by Gregor et al (ICML 2014).  Efficient Inference in Occlusion-Aware Generative Models of Images by Huang & Murphy (ICLR) is another relevant work, which used a variational auto-encoder with a spatial transformer and an RNN-like sequence model to model the appearance of multiple sprites on a background.

Finally, the exposition in this paper is short on many details and I don’t believe that the paper is reproducible from the text alone.  For example, it is not clear what the form of the recognition model is…  Low-level details (which are very important) are also not presented, such as initialization strategy.",0
"This work proposes to augment normal gradient descent algorithms with a ""Data Filter"", that acts as a curriculum teacher by selecting which examples the trained target network should see to learn optimally. Such a filter is learned simultaneously to the target network, and trained via Reinforcement Learning algorithms receiving rewards based on the state of training with respect to some pseudo-validation set.


Stylistic comment, please use the more common style of ""(Author, year)"" rather than ""Author (year)"" when the Author is *not* referred to or used in the sentence.
E.g. ""and its variants such as Adagrad Duchi et al. (2011)"" should be ""such as Adagrad (Duchi et al., 2011)"", and  ""proposed in Andrychowicz et al. (2016),"" should remain so.

I think the paragraph containing ""What we need to do is, after seeing the mini-batch Dt of M training instances, we dynamically determine which instances in Dt are used for training and which are filtered."" should be clarified. What is ""seeing""? That is, you should mention explicitly that you do the forward-pass first, then compute features from that, and then decide for which examples to perform the backwards pass.


There are a few choices in this work which I do not understand:

Why wait until the end of the episode to update your reinforce policy (algorithm 2), but train your actor critic at each step (algorithm 3)? You say REINFORCE has high variance, which is true, but does not mean it cannot be trained at each step (unless you have some experiments that suggest otherwise, and if so they should be included or mentionned in the paper).

Similarly, why not train REINFORCE with the same reward as your Actor-Critic model? And vice-versa? You claim several times that a limitation of REINFORCE is that you need to wait for the episode to be over, but considering your data is i.i.d., you can make your episode be anything from a single training step, one D_t, to the whole multi-epoch training procedure.


I have a few qualms with the experimental setting:
- is Figure 2 obtained from a single (i.e. one per setup) experiment? From different initial weights? If so, there is no proper way of knowing whether results are chance or not! This is a serious concern for me.
- with most state-of-the-art work using optimization methods such as Adam and RMSProp, is it surprising that they were not experimented with.
- it is not clear what the learning rates are; how fast should the RL part adapt to the SL part? Its not clear that this was experimented with at all.
- the environment, i.e. the target network being trained, is not stationnary at all. It would have been interesting to measure how much the policy changes as a function of time. Figure 3, could both be the result of the policy adapting, or of the policy remaining fixed and the features changing (which could indicate a failure of the policy to adapt).
- in fact it is not really adressed in the paper that the environment is non-stationary, given the current setup, the distribution of features will change as the target network progresses. This has an impact on optimization.
- how is the ""pseudo-validation"" data, target to the policy, chosen? It should be a subset of the training data. The second paragraph of section 3.2 suggests something of the sort, but then your algorithms suggest that the same data is used to train both the policies and the networks, so I am unsure of which is what.


Overall the idea is novel and interesting, the paper is well written for the most part, but the methodology has some flaws. Clearer explanations and either more justification of the experimental choices or more experiments are needed to make this paper complete. Unless the authors convince me otherwise, I think it would be worth waiting for more experiments and submitting a very strong paper rather than presenting this (potentially powerful!) idea with weak results.",0
"This work proposes to augment normal gradient descent algorithms with a ""Data Filter"", that acts as a curriculum teacher by selecting which examples the trained target network should see to learn optimally. Such a filter is learned simultaneously to the target network, and trained via Reinforcement Learning algorithms receiving rewards based on the state of training with respect to some pseudo-validation set.


Stylistic comment, please use the more common style of ""(Author, year)"" rather than ""Author (year)"" when the Author is *not* referred to or used in the sentence.
E.g. ""and its variants such as Adagrad Duchi et al. (2011)"" should be ""such as Adagrad (Duchi et al., 2011)"", and  ""proposed in Andrychowicz et al. (2016),"" should remain so.

I think the paragraph containing ""What we need to do is, after seeing the mini-batch Dt of M training instances, we dynamically determine which instances in Dt are used for training and which are filtered."" should be clarified. What is ""seeing""? That is, you should mention explicitly that you do the forward-pass first, then compute features from that, and then decide for which examples to perform the backwards pass.


There are a few choices in this work which I do not understand:

Why wait until the end of the episode to update your reinforce policy (algorithm 2), but train your actor critic at each step (algorithm 3)? You say REINFORCE has high variance, which is true, but does not mean it cannot be trained at each step (unless you have some experiments that suggest otherwise, and if so they should be included or mentionned in the paper).

Similarly, why not train REINFORCE with the same reward as your Actor-Critic model? And vice-versa? You claim several times that a limitation of REINFORCE is that you need to wait for the episode to be over, but considering your data is i.i.d., you can make your episode be anything from a single training step, one D_t, to the whole multi-epoch training procedure.


I have a few qualms with the experimental setting:
- is Figure 2 obtained from a single (i.e. one per setup) experiment? From different initial weights? If so, there is no proper way of knowing whether results are chance or not! This is a serious concern for me.
- with most state-of-the-art work using optimization methods such as Adam and RMSProp, is it surprising that they were not experimented with.
- it is not clear what the learning rates are; how fast should the RL part adapt to the SL part? Its not clear that this was experimented with at all.
- the environment, i.e. the target network being trained, is not stationnary at all. It would have been interesting to measure how much the policy changes as a function of time. Figure 3, could both be the result of the policy adapting, or of the policy remaining fixed and the features changing (which could indicate a failure of the policy to adapt).
- in fact it is not really adressed in the paper that the environment is non-stationary, given the current setup, the distribution of features will change as the target network progresses. This has an impact on optimization.
- how is the ""pseudo-validation"" data, target to the policy, chosen? It should be a subset of the training data. The second paragraph of section 3.2 suggests something of the sort, but then your algorithms suggest that the same data is used to train both the policies and the networks, so I am unsure of which is what.


Overall the idea is novel and interesting, the paper is well written for the most part, but the methodology has some flaws. Clearer explanations and either more justification of the experimental choices or more experiments are needed to make this paper complete. Unless the authors convince me otherwise, I think it would be worth waiting for more experiments and submitting a very strong paper rather than presenting this (potentially powerful!) idea with weak results.",0
"This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game “venture”, maybe).

Novelty: none of the proposed types of intrinsic motivation are novel, and it’s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).

Potential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent’s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.

Computation time: I find the paper’s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?",0
"This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game “venture”, maybe).

Novelty: none of the proposed types of intrinsic motivation are novel, and it’s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).

Potential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent’s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.

Computation time: I find the paper’s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?",0
"Thank you for an interesting read.

Given the huge interest in generative modelling nowadays, this paper is very timely and does provide very clear connections between methods that don't use maximum likelihood for training. It made a very useful observation that the generative and the discriminative loss do **not** need to be coupled with each other. I think this paper in summary provides some very useful insights to the practitioners on how to select the objective function to train the implicit generative model.

The only reason that I decided to hold back my strong acceptance recommendation is that I don't understand the acceptance criteria of ICLR. First this paper has the style very similar to the Sugiyama et al. papers that are cited (e.g. presenting in different perspectives that were all covered in those papers but in a different context), making me unsure about how to evaluate the novelty. Second this paper has no experiment nor mathematical theorem, and I'm not exactly sure what kinds of contributions the ICLR committee is looking for.",0
"Thank you for an interesting read.

Given the huge interest in generative modelling nowadays, this paper is very timely and does provide very clear connections between methods that don't use maximum likelihood for training. It made a very useful observation that the generative and the discriminative loss do **not** need to be coupled with each other. I think this paper in summary provides some very useful insights to the practitioners on how to select the objective function to train the implicit generative model.

The only reason that I decided to hold back my strong acceptance recommendation is that I don't understand the acceptance criteria of ICLR. First this paper has the style very similar to the Sugiyama et al. papers that are cited (e.g. presenting in different perspectives that were all covered in those papers but in a different context), making me unsure about how to evaluate the novelty. Second this paper has no experiment nor mathematical theorem, and I'm not exactly sure what kinds of contributions the ICLR committee is looking for.",0
"The authors apply the image captioning architecture of Xu et al. 2015 to video captioning. The model is extended to have attention over multiple layers of the ConvNet instead of just a single layer. Experiments on YouTube2Text, M-VAD and MSR-VTT show that this works better than only using one of the layers at a time.

I think this is solid work on the level of a well-executed course project or a workshop paper. The model makes sense, it is adequately described, and the experiments show that attending over multiple layers works better than attending over any one layer in isolation. Unfortunately, I don't think there is enough to get excited about here from a technical perspective and it's not clear what value the paper brings to the community. Other aspects of the paper, such as including the hard attention component, don't seem to add to the paper but take up space. 

If the authors want to contribute a detailed, focused exploration of multi-level features this could become a more valuable paper, but in that case I would expect a much more thorough exploration of the choices and tradeoffs of different schemes without too many spurious aspects such as video features, hard attention, etc.",0
"The authors apply the image captioning architecture of Xu et al. 2015 to video captioning. The model is extended to have attention over multiple layers of the ConvNet instead of just a single layer. Experiments on YouTube2Text, M-VAD and MSR-VTT show that this works better than only using one of the layers at a time.

I think this is solid work on the level of a well-executed course project or a workshop paper. The model makes sense, it is adequately described, and the experiments show that attending over multiple layers works better than attending over any one layer in isolation. Unfortunately, I don't think there is enough to get excited about here from a technical perspective and it's not clear what value the paper brings to the community. Other aspects of the paper, such as including the hard attention component, don't seem to add to the paper but take up space. 

If the authors want to contribute a detailed, focused exploration of multi-level features this could become a more valuable paper, but in that case I would expect a much more thorough exploration of the choices and tradeoffs of different schemes without too many spurious aspects such as video features, hard attention, etc.",0
"This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a ""3D model"" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.

The proposed GAN architecture could potentially be interesting.  However, I won’t champion the paper as the evaluation could be improved.

A critical missing baseline is a comparison against a generic GAN.  Without this it’s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. 

A couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):

[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.

[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.

The problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  

I found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  

Minor comments:

Fig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  

Page 3: ""chapter"" => ""section"".

In Table 2, what is the loss used for the DCNN?

Fig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?",0
"This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a ""3D model"" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.

The proposed GAN architecture could potentially be interesting.  However, I won’t champion the paper as the evaluation could be improved.

A critical missing baseline is a comparison against a generic GAN.  Without this it’s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. 

A couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):

[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.

[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.

The problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  

I found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  

Minor comments:

Fig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  

Page 3: ""chapter"" => ""section"".

In Table 2, what is the loss used for the DCNN?

Fig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?",0
"This work proposes to compute embeddings of symbolic expressions (e.g., boolean expressions, or polynomials) such that semantically equivalent expressions are near each other in the embedded space.  The proposed model uses recursive neural networks where the architecture is made to match that of the parse tree of a given symbolic expression.  To train the model parameters, the authors create a dataset of expressions where semantic equivalence relationships are known and minimize a loss function so that equivalent expressions are closer to each other than non-equivalent expressions via a max-margin loss function.  The authors also use a “subexpression forcing” mechanism which, if I understand it correctly, encourages the embeddings to respect some kind of compositionality.

Results are shown on a few symbolic expression datasets created by the authors and the proposed method is demonstrated to outperform baselines pretty convincingly.  I especially like the PCA visualization where the action of negating an expression is shown to correspond roughly to negating the embedding in its vector space — it is a lot like the man - woman + queen = king type embeddings that we see in the word2vec and glove style papers.  

The weakest part of the paper is probably that the setting seems somewhat contrived — I can’t really think of a real setting where it is easy to have a training set of known semantic equivalences, but still more worth it to use a neural network to do predictions.   The authors have also punted on dealing with variable names, assuming that distinct variables refer to different entities in the domain.  This is understandable, as variable names add a whole new layer of complexity on an already difficult problem, but also seems high limiting.  For example, the proposed methods would not be useable in an “equation search engine” unless we were able to accurately canonicalize variable names in some way.

Other miscellaneous points:
* Regarding problem hardness, I believe that the problem of determining if two expressions are equivalent is actually undecidable — see the “word problem for Thue systems”.  Related to this, I was not able to figure out how the authors determine ground truth equivalence in their training sets.  They say that expressions are simplified into a canonical form and grouped, but this seems to not be possible in general, so one question is — is it possible that equivalent expressions in the training data would have been mapped to different canonical forms?  Would it have been easier/possible to construct and compare truth tables?
* The “COMBINE” operation uses what the authors describe as a residual-like connection.  Looking at the equations, the reason why this is not actually a residual connection is because of the weight matrix that is multiplied by the lower level l_0 features.  A true residual connection would have passed the features through unchanged (identity connection) and would have also been better at fighting gradient explosion…. so is there a reason why this was used rather than an identity connection?
* In table 3, the first tf-idf entry: a + (c+a) * c seems equivalent to a + (c * (a+c))
* Vertical spacing between Figure 4 caption and body of text is very small and looks like the caption continues into the body of the text.",0
"This work proposes to compute embeddings of symbolic expressions (e.g., boolean expressions, or polynomials) such that semantically equivalent expressions are near each other in the embedded space.  The proposed model uses recursive neural networks where the architecture is made to match that of the parse tree of a given symbolic expression.  To train the model parameters, the authors create a dataset of expressions where semantic equivalence relationships are known and minimize a loss function so that equivalent expressions are closer to each other than non-equivalent expressions via a max-margin loss function.  The authors also use a “subexpression forcing” mechanism which, if I understand it correctly, encourages the embeddings to respect some kind of compositionality.

Results are shown on a few symbolic expression datasets created by the authors and the proposed method is demonstrated to outperform baselines pretty convincingly.  I especially like the PCA visualization where the action of negating an expression is shown to correspond roughly to negating the embedding in its vector space — it is a lot like the man - woman + queen = king type embeddings that we see in the word2vec and glove style papers.  

The weakest part of the paper is probably that the setting seems somewhat contrived — I can’t really think of a real setting where it is easy to have a training set of known semantic equivalences, but still more worth it to use a neural network to do predictions.   The authors have also punted on dealing with variable names, assuming that distinct variables refer to different entities in the domain.  This is understandable, as variable names add a whole new layer of complexity on an already difficult problem, but also seems high limiting.  For example, the proposed methods would not be useable in an “equation search engine” unless we were able to accurately canonicalize variable names in some way.

Other miscellaneous points:
* Regarding problem hardness, I believe that the problem of determining if two expressions are equivalent is actually undecidable — see the “word problem for Thue systems”.  Related to this, I was not able to figure out how the authors determine ground truth equivalence in their training sets.  They say that expressions are simplified into a canonical form and grouped, but this seems to not be possible in general, so one question is — is it possible that equivalent expressions in the training data would have been mapped to different canonical forms?  Would it have been easier/possible to construct and compare truth tables?
* The “COMBINE” operation uses what the authors describe as a residual-like connection.  Looking at the equations, the reason why this is not actually a residual connection is because of the weight matrix that is multiplied by the lower level l_0 features.  A true residual connection would have passed the features through unchanged (identity connection) and would have also been better at fighting gradient explosion…. so is there a reason why this was used rather than an identity connection?
* In table 3, the first tf-idf entry: a + (c+a) * c seems equivalent to a + (c * (a+c))
* Vertical spacing between Figure 4 caption and body of text is very small and looks like the caption continues into the body of the text.",0
"The paper presents an interesting incremental approach for exploring new convolutional network hierarchies in an incremental manner after a baseline network has reached a good recognition performance.

The experiments are presented for the CIFAR-100 and ImageNet benchmarks by morphing various ResNet models into better performing models with somewhat more computation.

Although the baselines are less strong than those presented in the literature, the paper claims significant error reduction for both ImageNet and CIFAR-100.

The main idea of the paper is to rewrite convolutions into multiple convolutions while expanding the number of filters. It is quite unexpected that this approach yields any improvements over the baseline model at all.

However, for some of the basic tenets of network morphing, experimental evidence is not given in the paper. Here are some fundamental questions raised by the paper:
- How does the quality of morphed networks compares to those with the same topology trained from scratch?
- How does the incremental training time after morphing relate to that of the network trained from scratch?
- Where is the extra computational cost of the morphed networks come from?
- Why is the quality of the baseline ResNet models lag behind those that are reported in the literature and github? (E.g. the github ResNet-101 model is supposed to have 6.1% top-5 recall vs 6.6 reported in the paper)
More evidence for the first three points would be necessary to evaluate the validity of the claims of the paper.

The paper is written reasonably well and can be understood quite well, but the missing evidence and weaker baselines make it looks somewhat less convincing. 
I would be inclined to revise up the score if a more experimental evidence were given for the main message of the paper (see the points above).",0
"The paper presents an interesting incremental approach for exploring new convolutional network hierarchies in an incremental manner after a baseline network has reached a good recognition performance.

The experiments are presented for the CIFAR-100 and ImageNet benchmarks by morphing various ResNet models into better performing models with somewhat more computation.

Although the baselines are less strong than those presented in the literature, the paper claims significant error reduction for both ImageNet and CIFAR-100.

The main idea of the paper is to rewrite convolutions into multiple convolutions while expanding the number of filters. It is quite unexpected that this approach yields any improvements over the baseline model at all.

However, for some of the basic tenets of network morphing, experimental evidence is not given in the paper. Here are some fundamental questions raised by the paper:
- How does the quality of morphed networks compares to those with the same topology trained from scratch?
- How does the incremental training time after morphing relate to that of the network trained from scratch?
- Where is the extra computational cost of the morphed networks come from?
- Why is the quality of the baseline ResNet models lag behind those that are reported in the literature and github? (E.g. the github ResNet-101 model is supposed to have 6.1% top-5 recall vs 6.6 reported in the paper)
More evidence for the first three points would be necessary to evaluate the validity of the claims of the paper.

The paper is written reasonably well and can be understood quite well, but the missing evidence and weaker baselines make it looks somewhat less convincing. 
I would be inclined to revise up the score if a more experimental evidence were given for the main message of the paper (see the points above).",0
"The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The approach is illustrated in two tasks: gridworld with objects and a simplified Minecraft problem).  The idea of providing symbolic descriptions of tasks and learning corresponding ""implementations"" is potentially interesting and the empirical results are promising.  However, there are two main drawbacks of the current incarnation of this work.  First, the ideas presented in the paper have all been explored in other work (symbolic specifications, actor-critic, shared representations).  While related work is discussed, it is not really clear what is new here, and what is the main contribution of this work besides providing a new implementation of existing ideas in the context of deep learning. The main contribution if the work needs to be clearly spelled out.  Secondly, the approach presented relies crucially on curriculum learning (this is quite clear from the experiments).  While the authors argue that specifying tasks in simplified language is easy, designing a curriculum may in fact be pretty complicated, depending on the task at hand.  The examples provided are fairly small, and there is no hint of how curriculum can be designed for larger problems. Because the approach is sensitive to the curriculum, this limits the potential utility of the work. It is also unclear if there is a way to provide supervision automatically, instead of doing it based on prior domain knowledge.
More minor comments:
- The experiments are not described in enough detail in the paper. It's great to provide github code, but one needs to explain in the paper why certain choices were made in the task setup (were these optimized? What's this the first thing that worked?) Even with the code, the experiments as described are not reproducible
- The description of the approach is pretty tangled with the specific algorithmic choices. Can the authors step back and think more generally of how this approach can be formalized?  I think this would help relate it to the prior work more clearly as well.",0
"The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The approach is illustrated in two tasks: gridworld with objects and a simplified Minecraft problem).  The idea of providing symbolic descriptions of tasks and learning corresponding ""implementations"" is potentially interesting and the empirical results are promising.  However, there are two main drawbacks of the current incarnation of this work.  First, the ideas presented in the paper have all been explored in other work (symbolic specifications, actor-critic, shared representations).  While related work is discussed, it is not really clear what is new here, and what is the main contribution of this work besides providing a new implementation of existing ideas in the context of deep learning. The main contribution if the work needs to be clearly spelled out.  Secondly, the approach presented relies crucially on curriculum learning (this is quite clear from the experiments).  While the authors argue that specifying tasks in simplified language is easy, designing a curriculum may in fact be pretty complicated, depending on the task at hand.  The examples provided are fairly small, and there is no hint of how curriculum can be designed for larger problems. Because the approach is sensitive to the curriculum, this limits the potential utility of the work. It is also unclear if there is a way to provide supervision automatically, instead of doing it based on prior domain knowledge.
More minor comments:
- The experiments are not described in enough detail in the paper. It's great to provide github code, but one needs to explain in the paper why certain choices were made in the task setup (were these optimized? What's this the first thing that worked?) Even with the code, the experiments as described are not reproducible
- The description of the approach is pretty tangled with the specific algorithmic choices. Can the authors step back and think more generally of how this approach can be formalized?  I think this would help relate it to the prior work more clearly as well.",0
"This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation. The approach is novel, and is motivated by being able to learn policies for robotics.

My two key reservations with the paper are as follows:
1. The method is motivated by learning policies for robotics. However, the proposed method is *only* useful for robotics if the learned policy can transfer the real world. Transferring policies from simulation to real-world is an open research problem, and is particularly challenging with less realistic simulators.
2. They key novelty/benefit of this approach over other model-based approaches is that the simulator is differentiable. However, the only empirical comparison in the paper is to a model-free approach (CMA-ES). To appropriately demonstrate the approach, it should be compared to other model-based approaches, which do not require analytic derivatives of the model.

For the reader to fully understand the pros and cons of the approach, the paper should also include quantitative comparison between the speed of the proposed simulator, and that of standard simulation platforms.

Because the idea is interesting and novel, I think it lies above the acceptance threshold. However, it would be significantly improved with the aforementioned comparisons.

Lastly, the writing of the paper could be improved, as it is rather informal and/or imprecise in a number of places. Here are some examples:
-- “we model the use of a neural network as a general controller for a robot” - can be more concisely phrased as something like “we model the robot controller using a neural network” or “the robot controller is modeled using a neural network""
-- “In previous research, finding a gradient…” - This is a run-on sentence.
-- “We basically jam this entire equation into” - This sentence is informal and imprecise.
-- “deep learning neural network” - the word “learning” should be omitted
-- “one big RNN, where we unfold over time” - should be “…RNN, which we unfold over time” or “…RNN, unfolded over time”

The writing would also be improved by making it more concise and fitting the paper into 8 pages.",0
"This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation. The approach is novel, and is motivated by being able to learn policies for robotics.

My two key reservations with the paper are as follows:
1. The method is motivated by learning policies for robotics. However, the proposed method is *only* useful for robotics if the learned policy can transfer the real world. Transferring policies from simulation to real-world is an open research problem, and is particularly challenging with less realistic simulators.
2. They key novelty/benefit of this approach over other model-based approaches is that the simulator is differentiable. However, the only empirical comparison in the paper is to a model-free approach (CMA-ES). To appropriately demonstrate the approach, it should be compared to other model-based approaches, which do not require analytic derivatives of the model.

For the reader to fully understand the pros and cons of the approach, the paper should also include quantitative comparison between the speed of the proposed simulator, and that of standard simulation platforms.

Because the idea is interesting and novel, I think it lies above the acceptance threshold. However, it would be significantly improved with the aforementioned comparisons.

Lastly, the writing of the paper could be improved, as it is rather informal and/or imprecise in a number of places. Here are some examples:
-- “we model the use of a neural network as a general controller for a robot” - can be more concisely phrased as something like “we model the robot controller using a neural network” or “the robot controller is modeled using a neural network""
-- “In previous research, finding a gradient…” - This is a run-on sentence.
-- “We basically jam this entire equation into” - This sentence is informal and imprecise.
-- “deep learning neural network” - the word “learning” should be omitted
-- “one big RNN, where we unfold over time” - should be “…RNN, which we unfold over time” or “…RNN, unfolded over time”

The writing would also be improved by making it more concise and fitting the paper into 8 pages.",0
"This paper proposes a new learning framework called ""compositional kernel machines"" (CKM). It combines two ideas: kernel methods and sum-product network (SPN). CKM first defines leaf kernels on elements of the query and training examples, then it defines kernel recursively (similar to sum-product network). This paper has shown that the evaluation CKM can be done efficiently using the same tricks in SPN.

Positive: I think the idea in this paper is interesting. Instance-based learning methods (such as SVM with kernels) have been successful in the past, but have been replaced by deep learning methods (e.g. convnet) in the past few years. This paper investigate an unexplored area of how to combine the ideas from kernel methods and deep networks (SPN in this case). 

Negative: Although the idea of this paper is interesting, this paper is clearly very preliminary. In its current form, I simply do not see any advantage of the proposed framework over convnet. I will elaborate below.

1) One of the most important claims of this paper is that CKM is faster to learn than convnet. I am not clear why that is the case. Both CKM and convnet use gradient descent during learning, why would CKM be faster?

Also during inference, the running time of convnet only depends on its network structure. But for CKM, in addition to the network structure, it also depends on the size of training set. From this perspective, it does not seem CKM is very scalable when the training size is big. That is probably why this paper has to use all kinds of specialized data structures and tricks (even on a fairly simple dataset like NORB)

2) I am having a hard time understanding what the leaf kernel is capturing. For example, if the ""elements"" correspond to raw pixel intensities, a leaf kernel essentially compares the intensity value of a pixel in the query image with that in a training image. But in this case, wouldn't you end up comparing a lot of background pixels across these two images (which does not help with recognition)?

I think it probably helps to explain Sec 3.1 a bit better. In its current form, this part is very dense and hard to understand.

3) It is also not entirely clear to me how you would design the architecture of the sum-product function. The example is Sec 3.1 seems to be fairly arbitrary.

4) The experiment section is probably the weakest part. NORB is a very small and toy-ish dataset by today's standard. Even on this small dataset, the proposed method is only slighly better than SVM (it is not clear whether ""SVM"" in Table 2 is linear SVM or kernel SVM. If it is linear SVM, I suspect the performance of ""SVM"" will be even higher when you use kernel SVM), and far worse than convnet. The proposed method only shows improvement over convnet on synthetic datasets (NORB compositions, NORM symmetries)

Overall, I think this paper has some interesting ideas. But in its current form, it is a bit too preliminary and more work is needed to show its advantage. Having said that, I acknowledge that in the machine learning history, many important ideas seem pre-mature when they were first proposed, and it took time for these ideas to develop.",0
"This paper proposes a new learning framework called ""compositional kernel machines"" (CKM). It combines two ideas: kernel methods and sum-product network (SPN). CKM first defines leaf kernels on elements of the query and training examples, then it defines kernel recursively (similar to sum-product network). This paper has shown that the evaluation CKM can be done efficiently using the same tricks in SPN.

Positive: I think the idea in this paper is interesting. Instance-based learning methods (such as SVM with kernels) have been successful in the past, but have been replaced by deep learning methods (e.g. convnet) in the past few years. This paper investigate an unexplored area of how to combine the ideas from kernel methods and deep networks (SPN in this case). 

Negative: Although the idea of this paper is interesting, this paper is clearly very preliminary. In its current form, I simply do not see any advantage of the proposed framework over convnet. I will elaborate below.

1) One of the most important claims of this paper is that CKM is faster to learn than convnet. I am not clear why that is the case. Both CKM and convnet use gradient descent during learning, why would CKM be faster?

Also during inference, the running time of convnet only depends on its network structure. But for CKM, in addition to the network structure, it also depends on the size of training set. From this perspective, it does not seem CKM is very scalable when the training size is big. That is probably why this paper has to use all kinds of specialized data structures and tricks (even on a fairly simple dataset like NORB)

2) I am having a hard time understanding what the leaf kernel is capturing. For example, if the ""elements"" correspond to raw pixel intensities, a leaf kernel essentially compares the intensity value of a pixel in the query image with that in a training image. But in this case, wouldn't you end up comparing a lot of background pixels across these two images (which does not help with recognition)?

I think it probably helps to explain Sec 3.1 a bit better. In its current form, this part is very dense and hard to understand.

3) It is also not entirely clear to me how you would design the architecture of the sum-product function. The example is Sec 3.1 seems to be fairly arbitrary.

4) The experiment section is probably the weakest part. NORB is a very small and toy-ish dataset by today's standard. Even on this small dataset, the proposed method is only slighly better than SVM (it is not clear whether ""SVM"" in Table 2 is linear SVM or kernel SVM. If it is linear SVM, I suspect the performance of ""SVM"" will be even higher when you use kernel SVM), and far worse than convnet. The proposed method only shows improvement over convnet on synthetic datasets (NORB compositions, NORM symmetries)

Overall, I think this paper has some interesting ideas. But in its current form, it is a bit too preliminary and more work is needed to show its advantage. Having said that, I acknowledge that in the machine learning history, many important ideas seem pre-mature when they were first proposed, and it took time for these ideas to develop.",0
"Thank you for an interesting read. 

To my knowledge, very few papers have looked at transfer learning with **no** target domain data (the authors called this task as ""extrapolation""). This paper clearly shows that the knowledge of the underlying system dynamics is crucial in this case. The experiments clearly showed the promising potential of the proposed EQL model. I think EQL is very interesting also from the perspective of interpretability, which is crucial for data analysis in scientific domains.

Quesions and comments:

1. Multiplication units. By the universal approximation theorem, multiplication can also be represented by a neural network in the usual sense. I agree with the authors' explanation of interpolation and extrapolation, but I still don't quite understand why multiplication unit is crucial here. I guess is it because this representation generalises better when training data is not that representative for the future?

2. Fitting an EQL vs. fitting a polynomial. It seems to me that the number of layers in EQL has some connections to the degree of the polynomial. Assume we know the underlying dynamics we want to learn can be represented by a polynomial. Then what's the difference between fitting a polynomial (with model selection techniques to determine the degree) and fitting an EQL (with model selection techniques to determine the number of layers)? Also your experiments showed that the selection of basis functions (specific to the underlying dynamics you want to learn) is crucial for the performance. This means you need to have some prior knowledge on the form of the equation anyway!

3. Ben-David et al. 2010 has presented some error bounds for the hypothesis that is trained on source data but tested on the target data. I wonder if your EQL model can achieve better error bounds?

4. Can you comment on the comparison of your method to those who modelled the extrapolation data with **uncertainty**?",0
"Thank you for an interesting read. 

To my knowledge, very few papers have looked at transfer learning with **no** target domain data (the authors called this task as ""extrapolation""). This paper clearly shows that the knowledge of the underlying system dynamics is crucial in this case. The experiments clearly showed the promising potential of the proposed EQL model. I think EQL is very interesting also from the perspective of interpretability, which is crucial for data analysis in scientific domains.

Quesions and comments:

1. Multiplication units. By the universal approximation theorem, multiplication can also be represented by a neural network in the usual sense. I agree with the authors' explanation of interpolation and extrapolation, but I still don't quite understand why multiplication unit is crucial here. I guess is it because this representation generalises better when training data is not that representative for the future?

2. Fitting an EQL vs. fitting a polynomial. It seems to me that the number of layers in EQL has some connections to the degree of the polynomial. Assume we know the underlying dynamics we want to learn can be represented by a polynomial. Then what's the difference between fitting a polynomial (with model selection techniques to determine the degree) and fitting an EQL (with model selection techniques to determine the number of layers)? Also your experiments showed that the selection of basis functions (specific to the underlying dynamics you want to learn) is crucial for the performance. This means you need to have some prior knowledge on the form of the equation anyway!

3. Ben-David et al. 2010 has presented some error bounds for the hypothesis that is trained on source data but tested on the target data. I wonder if your EQL model can achieve better error bounds?

4. Can you comment on the comparison of your method to those who modelled the extrapolation data with **uncertainty**?",0
"The research direction taken by this paper is of great interest. 
However, the empirical results are not great enough to pay for the weaknesses of the proposed approach (see Section 6). 
""Throughout this paper the selection of hyper-parameters was kept rather simple."" but the momentum term of CPN is set to 0.95 
and not 0.9 as in all/most optimizers CPN is compared to. I suppose that the positive effect of CPN (if any) is mostly due to its momentum term.",0
"The research direction taken by this paper is of great interest. 
However, the empirical results are not great enough to pay for the weaknesses of the proposed approach (see Section 6). 
""Throughout this paper the selection of hyper-parameters was kept rather simple."" but the momentum term of CPN is set to 0.95 
and not 0.9 as in all/most optimizers CPN is compared to. I suppose that the positive effect of CPN (if any) is mostly due to its momentum term.",0
"First, I'd like to thank the authors for their answers and clarifications.
I find, the presentation of the multi-stage version of the model much clearer now.

Pros:

+ The paper states a sparse coding problem using cosine loss, which allows to solve the problem in a single pass.

+ The energy-based formulation allows bi-directional coding that incorporates top-down and bottom-up information in the feature extraction process. 

Cons:

+ The cost of running the evaluation could be large in the  multi-class setting, rendering the approach less attractive and the computational cost comparable to recurrent architectures.

+ While the model is competitive and improves over the baseline, the paper would be more convincing with other comparisons (see text). The experimental evaluation is limited (a single database and a single baseline)

------

The motivation of the sparse coding scheme is to perform inference in a feed forward manner. This property does not hold in the multi stage setting, thus optimization would be required (as clarified by the authors).

Having an efficient way of performing a bi-directional coding scheme is very interesting. As the authors clarified, this could not necessarily be the case, as the model needs to be evaluated many times for performing a classification.

Maybe an interesting combination would be to run the model without any class-specific bias, and evaluation only the top K predictions with the energy-based setting.

Having said this, it would be good to include a discussion (if not direct comparisons) of the trade-offs of using a model as the one proposed by Cao et al. Eg. computational costs, performance.

Using the bidirectional coding only on the top layers seems reasonable: one can get a good low level representation in a class agnostic way. This, however could be studied in more detail, for instance showing empirically the trade offs. If I understand correctly, now only one setting is being reported.

Finally, the authors mention that one benefit of using the architecture derived from the proposed coding method is the spherical normalization scheme, which can lead to smoother optimization dynamics. Does the baseline (or model) use batch-normalization? If not, seems relevant to test.


Minor comments:

I find figure 2 (d) confusing. I would not plot this setting as it does not lead to a function (as the authors state in the text).",0
"First, I'd like to thank the authors for their answers and clarifications.
I find, the presentation of the multi-stage version of the model much clearer now.

Pros:

+ The paper states a sparse coding problem using cosine loss, which allows to solve the problem in a single pass.

+ The energy-based formulation allows bi-directional coding that incorporates top-down and bottom-up information in the feature extraction process. 

Cons:

+ The cost of running the evaluation could be large in the  multi-class setting, rendering the approach less attractive and the computational cost comparable to recurrent architectures.

+ While the model is competitive and improves over the baseline, the paper would be more convincing with other comparisons (see text). The experimental evaluation is limited (a single database and a single baseline)

------

The motivation of the sparse coding scheme is to perform inference in a feed forward manner. This property does not hold in the multi stage setting, thus optimization would be required (as clarified by the authors).

Having an efficient way of performing a bi-directional coding scheme is very interesting. As the authors clarified, this could not necessarily be the case, as the model needs to be evaluated many times for performing a classification.

Maybe an interesting combination would be to run the model without any class-specific bias, and evaluation only the top K predictions with the energy-based setting.

Having said this, it would be good to include a discussion (if not direct comparisons) of the trade-offs of using a model as the one proposed by Cao et al. Eg. computational costs, performance.

Using the bidirectional coding only on the top layers seems reasonable: one can get a good low level representation in a class agnostic way. This, however could be studied in more detail, for instance showing empirically the trade offs. If I understand correctly, now only one setting is being reported.

Finally, the authors mention that one benefit of using the architecture derived from the proposed coding method is the spherical normalization scheme, which can lead to smoother optimization dynamics. Does the baseline (or model) use batch-normalization? If not, seems relevant to test.


Minor comments:

I find figure 2 (d) confusing. I would not plot this setting as it does not lead to a function (as the authors state in the text).",0
"The paper proposes to add an additional term to the denoising-autoencoder objective. The new term is well motivated, it introduces an asymmetry between the encoder and decoder, forcing the encoder to represent a compressed, denoised version of the input. The authors propose to avoid the trivial solution introduced by the new term by using tied weights or normalized Euclidean distance error (the trivial solution occurs by scaling the magnitude of the code down in the encoder, and back up in the decoder). The proposed auto-encoder scheme is very similar to a host of other auto-encoders that have been out in the literature for some time. The authors evaluate the proposed scheme on toy-data distributions in 2D as well as MNIST. Although the work is well motivated, it certainly seems like an empirically unproven and incremental improvement to an old idea.",0
"The paper proposes to add an additional term to the denoising-autoencoder objective. The new term is well motivated, it introduces an asymmetry between the encoder and decoder, forcing the encoder to represent a compressed, denoised version of the input. The authors propose to avoid the trivial solution introduced by the new term by using tied weights or normalized Euclidean distance error (the trivial solution occurs by scaling the magnitude of the code down in the encoder, and back up in the decoder). The proposed auto-encoder scheme is very similar to a host of other auto-encoders that have been out in the literature for some time. The authors evaluate the proposed scheme on toy-data distributions in 2D as well as MNIST. Although the work is well motivated, it certainly seems like an empirically unproven and incremental improvement to an old idea.",0
"The authors propose a time-series model with discrete states for robotics applications. I think the proposed method is too simplistic to be useful in the presented form, eg. 1) the state space (dimensionality & topology) is exactly matched to the experiments 2) displacements in the transition model are linear in the actions 3) observations are one-dimensional. This seems to be quite behind the current state of the art, eg “Embed to Control” by Watter et al 2015, where a state representation is learned directly from pixels.
Furthermore the authors do not compare to any other method except for an out-of-the-box LSTM model. Also, I feel like there must be a lot of prior work for combining HMMs + NNs out there, I think it would be necessary for the authors to relate their work to this literature.",0
"The authors propose a time-series model with discrete states for robotics applications. I think the proposed method is too simplistic to be useful in the presented form, eg. 1) the state space (dimensionality & topology) is exactly matched to the experiments 2) displacements in the transition model are linear in the actions 3) observations are one-dimensional. This seems to be quite behind the current state of the art, eg “Embed to Control” by Watter et al 2015, where a state representation is learned directly from pixels.
Furthermore the authors do not compare to any other method except for an out-of-the-box LSTM model. Also, I feel like there must be a lot of prior work for combining HMMs + NNs out there, I think it would be necessary for the authors to relate their work to this literature.",0
"This is a nice proposal, and could lead to more efficient training of
recurrent nets. I would really love to see a bit more experimental evidence.
I asked a few questions already but didn't get any answer so far.
Here are a few other questions/concerns I have:

- Is the resulting model still a universal approximator? (providing large enough hidden dimensions and number of layers)
- More generally, can one compare the expressiveness of the model with the equivalent model without the orthogonal matrices? with the same number of parameters for instance?
- The experiments are a bit disappointing as the number of distinct input/output
sequences were in fact very small and as noted by the authr, training
becomes unstable (I didn't understand what ""success"" meant in this case).
The authors point that the experiment section need to be expanded, but as
far as I can tell they still haven't unfortunately.",0
"This is a nice proposal, and could lead to more efficient training of
recurrent nets. I would really love to see a bit more experimental evidence.
I asked a few questions already but didn't get any answer so far.
Here are a few other questions/concerns I have:

- Is the resulting model still a universal approximator? (providing large enough hidden dimensions and number of layers)
- More generally, can one compare the expressiveness of the model with the equivalent model without the orthogonal matrices? with the same number of parameters for instance?
- The experiments are a bit disappointing as the number of distinct input/output
sequences were in fact very small and as noted by the authr, training
becomes unstable (I didn't understand what ""success"" meant in this case).
The authors point that the experiment section need to be expanded, but as
far as I can tell they still haven't unfortunately.",0
"Combining storage and processing capabilities is an interesting research topic because data transfer is a major issue for many machine learning tasks.
The paper itself is well-written, but unfortunately addresses a lot of things only to medium depth (probably due length constraints).
My opinion is that a journal with an in-depth discussion of the technical details would be a better target for this paper.

Even though the researchers took an interesting approach to evaluate the performance of the system, it's difficult for me to grasp the expected practical improvements of this approach.
With such a big focus on GPU (and more specialized hardware such as TPUs), the one question that comes to mind: By how much does this - or do you expect it to - beat the latest and greatest GPU on a real task?

I don't consider myself an expert on this topic even though I have some experience with SystemC.",0
"Combining storage and processing capabilities is an interesting research topic because data transfer is a major issue for many machine learning tasks.
The paper itself is well-written, but unfortunately addresses a lot of things only to medium depth (probably due length constraints).
My opinion is that a journal with an in-depth discussion of the technical details would be a better target for this paper.

Even though the researchers took an interesting approach to evaluate the performance of the system, it's difficult for me to grasp the expected practical improvements of this approach.
With such a big focus on GPU (and more specialized hardware such as TPUs), the one question that comes to mind: By how much does this - or do you expect it to - beat the latest and greatest GPU on a real task?

I don't consider myself an expert on this topic even though I have some experience with SystemC.",0
"This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.

The paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.

As pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.

The other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.

[1] ""Asynchronous Methods for Deep Reinforcement Learning"", ICML 2016.",0
"This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.

The paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.

As pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.

The other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.

[1] ""Asynchronous Methods for Deep Reinforcement Learning"", ICML 2016.",0
"This paper presents an intriguing study of how one can pose architecture search as a meta learning problem. By collecting features from networks trained on various datasets and training a “ranking classifier” (the actual details of the classifier do not seem to be described in detail) one can potentially infer what a good architecture for a new problem could be by simply running the ranker on the extracted features for a new problem setup.

One notable comment from the paper is that the authors fix some important hyper-parameters for all the networks. I am of the opinion that optimizing the learning rate (and its decay schedule) is actually quite important. I hypothesize that a lot of the conclusions of this paper may change quite a bit if the authors did an actual search over the rates instead. I suspect that instead of training 11k nets, one can train 2k nets with 5 learning rates each and get a much better result that is actually compelling.

I am not convinced that the protocol for generating the various architectures is doing a good job at creating a diversity of architecture (simply because of the max depth of 8 layers and 14 components overall). I suspect that most of these generated architectures are actually almost identical performance-wise and that it’s a waste to train so many of them on so many tasks. Unless the authors are already doing this, they should define a pruning mechanism that filters out nets that are too similar to already existing ones.

The batch normalization experiments in Table 2 seem odd and under-explained. It is also well-known that the optimal learning rates when using batch norm vs. not using batch norm can differ by an order of magnitude so given the fixed learning rate throughout all experiments, I take these results with some grain of salt.

I am not sure we got many insights into the kinds of architectures that ended up being at the top. Either visualizations, or trends (or both), would be great.

This work seems to conflate the study of parallel vs. serial architectures with the study of meta learning, which are somewhat distinct issues. I take issue with the table that compares parallel vs. serial performance (table 2) simply because the right way would be to filter the architectures by the same number of parameters / capacity.

Ultimately the conclusion seems to be that when applying deep nets in a new domain, it is difficult to come up with a good architecture in advance. In that sense, it is hard to see the paper as a constructive result, because it’s conclusions are that while the ranker may do a good job often-times, it’s not that reliable. Thus I am not convinced that this particular result will be of practical use to folks who are intending to use deep nets for a new domain.",0
"This paper presents an intriguing study of how one can pose architecture search as a meta learning problem. By collecting features from networks trained on various datasets and training a “ranking classifier” (the actual details of the classifier do not seem to be described in detail) one can potentially infer what a good architecture for a new problem could be by simply running the ranker on the extracted features for a new problem setup.

One notable comment from the paper is that the authors fix some important hyper-parameters for all the networks. I am of the opinion that optimizing the learning rate (and its decay schedule) is actually quite important. I hypothesize that a lot of the conclusions of this paper may change quite a bit if the authors did an actual search over the rates instead. I suspect that instead of training 11k nets, one can train 2k nets with 5 learning rates each and get a much better result that is actually compelling.

I am not convinced that the protocol for generating the various architectures is doing a good job at creating a diversity of architecture (simply because of the max depth of 8 layers and 14 components overall). I suspect that most of these generated architectures are actually almost identical performance-wise and that it’s a waste to train so many of them on so many tasks. Unless the authors are already doing this, they should define a pruning mechanism that filters out nets that are too similar to already existing ones.

The batch normalization experiments in Table 2 seem odd and under-explained. It is also well-known that the optimal learning rates when using batch norm vs. not using batch norm can differ by an order of magnitude so given the fixed learning rate throughout all experiments, I take these results with some grain of salt.

I am not sure we got many insights into the kinds of architectures that ended up being at the top. Either visualizations, or trends (or both), would be great.

This work seems to conflate the study of parallel vs. serial architectures with the study of meta learning, which are somewhat distinct issues. I take issue with the table that compares parallel vs. serial performance (table 2) simply because the right way would be to filter the architectures by the same number of parameters / capacity.

Ultimately the conclusion seems to be that when applying deep nets in a new domain, it is difficult to come up with a good architecture in advance. In that sense, it is hard to see the paper as a constructive result, because it’s conclusions are that while the ranker may do a good job often-times, it’s not that reliable. Thus I am not convinced that this particular result will be of practical use to folks who are intending to use deep nets for a new domain.",0
"This paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.

Pros:

- Important analysis
- Good visualizations

Cons:

- The paper describes mostly the observation that the optima vary for different methods, however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)
- Some fonts are very small (e.g. Fig. 5)",0
"This paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.

Pros:

- Important analysis
- Good visualizations

Cons:

- The paper describes mostly the observation that the optima vary for different methods, however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)
- Some fonts are very small (e.g. Fig. 5)",0
"This paper proposed to use a simple count-based exploration technique in high-dimensional RL application (e.g., Atari Games). The counting is based on state hash, which implicitly groups (quantizes) similar state together. The hash is computed either via hand-designed features or learned features (unsupervisedly with auto-encoder). The new state to be explored receives a bonus similar to UCB (to encourage further exploration).

Overall the paper is solid with quite extensive experiments. I wonder how it generalizes to more Atari games. Montezuma’s Revenge may be particularly suitable for approaches that implicitly/explicitly cluster states together (like the proposed one), as it has multiple distinct scenarios, each with small variations in terms of visual appearance, showing clustering structures. On the other hand, such approaches might not work as well if the state space is fully continuous (e.g. in RLLab experiments). 

The authors did not answer my question about why the hash code needs to be updated during training. I think it is mainly because the code still needs to be adaptive for a particular game (to achieve lower reconstruction error) in the first few iterations . After that stabilization is the most important. Sec. 2.3 (Learned embedding) is quite confusing (but very important). I hope that the authors could make it more clear (e.g., by writing an algorithm block) in the next version.",0
"This paper proposed to use a simple count-based exploration technique in high-dimensional RL application (e.g., Atari Games). The counting is based on state hash, which implicitly groups (quantizes) similar state together. The hash is computed either via hand-designed features or learned features (unsupervisedly with auto-encoder). The new state to be explored receives a bonus similar to UCB (to encourage further exploration).

Overall the paper is solid with quite extensive experiments. I wonder how it generalizes to more Atari games. Montezuma’s Revenge may be particularly suitable for approaches that implicitly/explicitly cluster states together (like the proposed one), as it has multiple distinct scenarios, each with small variations in terms of visual appearance, showing clustering structures. On the other hand, such approaches might not work as well if the state space is fully continuous (e.g. in RLLab experiments). 

The authors did not answer my question about why the hash code needs to be updated during training. I think it is mainly because the code still needs to be adaptive for a particular game (to achieve lower reconstruction error) in the first few iterations . After that stabilization is the most important. Sec. 2.3 (Learned embedding) is quite confusing (but very important). I hope that the authors could make it more clear (e.g., by writing an algorithm block) in the next version.",0
"*** Paper Summary ***

This paper simplify matching network by considering only a single prototype per class which is obtained as the average of the embedding of the training class samples. Empirical comparisons with matching networks are reported.

*** Review ***

The paper reads well and clearly motivate the work. This work of learning metric learning propose to simplify an earlier work (matching network) which is a great objective. However, I am not sure it achieve better results than matching networks. The space of learning embeddings to optimize nearest neighbor classification has been explored before, but the idea of averaging the propotypes is interesting (as a non-linear extension of Mensink et al 2013). I would suggest to improve the discussion of related work and to consolidate the results section to help distinguish between the methods you outperform and the one you do not. 

The related work section can be extended to include work on learning distance metric to optimize a nearest neighbor classification, see Weinberger et al, 2005 and subsequent work. Extensions to perform the same task with neural networks can be found in Min et al, 09 that purse a goal very close to yours. Regarding approaches pursuing similar goals with a different learning objective, you cite siamese network with pairwise supervision. The learning to rank (for websearch) litterature with triplet supervision or global ranking losses is also highly relevant, ie. one example ""the query"" defines the class and the embedding space need to be such that positive/relevant document are closer to the query than the others. I would suggest to start with Chris Burges 2010 tutorial. One learning class 

I am not sure the reported results correctly reflect the state of the art for all tasks. The results are positive on Omniglot but I feel that you should also report the better results of matching networks on miniImageNet with fine tuning and full contextual embeddings. It can be considered misleading not to report it. On Cub 200, I thought that the state-of-the-art was 50.1%, when using features from GoogLeNet (Akata et al 2015), could you comment on this?

Overall, paper could greatly be improved, both in the discussion of related work and with a less partial reporting of prior empirical results.

*** References ***

Large Margin Nearest Neighbors. Weinberger et al, 2005
From RankNet to LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010
A Deep Non-linear Feature Mapping for Large-Margin kNN Classification, Min et al, 09",0
"*** Paper Summary ***

This paper simplify matching network by considering only a single prototype per class which is obtained as the average of the embedding of the training class samples. Empirical comparisons with matching networks are reported.

*** Review ***

The paper reads well and clearly motivate the work. This work of learning metric learning propose to simplify an earlier work (matching network) which is a great objective. However, I am not sure it achieve better results than matching networks. The space of learning embeddings to optimize nearest neighbor classification has been explored before, but the idea of averaging the propotypes is interesting (as a non-linear extension of Mensink et al 2013). I would suggest to improve the discussion of related work and to consolidate the results section to help distinguish between the methods you outperform and the one you do not. 

The related work section can be extended to include work on learning distance metric to optimize a nearest neighbor classification, see Weinberger et al, 2005 and subsequent work. Extensions to perform the same task with neural networks can be found in Min et al, 09 that purse a goal very close to yours. Regarding approaches pursuing similar goals with a different learning objective, you cite siamese network with pairwise supervision. The learning to rank (for websearch) litterature with triplet supervision or global ranking losses is also highly relevant, ie. one example ""the query"" defines the class and the embedding space need to be such that positive/relevant document are closer to the query than the others. I would suggest to start with Chris Burges 2010 tutorial. One learning class 

I am not sure the reported results correctly reflect the state of the art for all tasks. The results are positive on Omniglot but I feel that you should also report the better results of matching networks on miniImageNet with fine tuning and full contextual embeddings. It can be considered misleading not to report it. On Cub 200, I thought that the state-of-the-art was 50.1%, when using features from GoogLeNet (Akata et al 2015), could you comment on this?

Overall, paper could greatly be improved, both in the discussion of related work and with a less partial reporting of prior empirical results.

*** References ***

Large Margin Nearest Neighbors. Weinberger et al, 2005
From RankNet to LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010
A Deep Non-linear Feature Mapping for Large-Margin kNN Classification, Min et al, 09",0
"Vanishing and exploding gradients makes the optimization of RNNs very challenging. The issue becomes worse on tasks with long term dependencies that requires longer RNNs. One of the suggested approaches to improve the optimization is to optimize in a way that the transfer matrix is almost orthogonal. This paper investigate the role of orthogonality on the optimization and learning which is very important. The writing is sound and clear and arguments are easy to follow. The suggested optimization method is very interesting. The main shortcoming of this paper is the experiments which I find very important and I hope authors can update the experiment section significantly. Below I mention some comments on the experiment section:

1- I think the experiments are not enough. At the very least, report the result on the adding problem and language modeling task on Penn Treebank.

2- I understand that the copying task becomes difficult with non-lineary. However, removing non-linearity makes the optimization very different and therefore, it is very hard to conclude anything from the results on the copying task.

3- I was not able to find the number of hidden units used for RNNs in different tasks.

4- Please report the running time of your method in the paper for different numbers of hidden units, compare it with the SGD and mention the NN package you have used.

5- The results on Table 1 and Table 2 might also suggest that the orthogonality is not really helpful since even without a margin, the numbers are very close compare to the case when you find the optimal margin. Am I right?

6- What do we learn from Figure 2? It is left without any discussion.",0
"Vanishing and exploding gradients makes the optimization of RNNs very challenging. The issue becomes worse on tasks with long term dependencies that requires longer RNNs. One of the suggested approaches to improve the optimization is to optimize in a way that the transfer matrix is almost orthogonal. This paper investigate the role of orthogonality on the optimization and learning which is very important. The writing is sound and clear and arguments are easy to follow. The suggested optimization method is very interesting. The main shortcoming of this paper is the experiments which I find very important and I hope authors can update the experiment section significantly. Below I mention some comments on the experiment section:

1- I think the experiments are not enough. At the very least, report the result on the adding problem and language modeling task on Penn Treebank.

2- I understand that the copying task becomes difficult with non-lineary. However, removing non-linearity makes the optimization very different and therefore, it is very hard to conclude anything from the results on the copying task.

3- I was not able to find the number of hidden units used for RNNs in different tasks.

4- Please report the running time of your method in the paper for different numbers of hidden units, compare it with the SGD and mention the NN package you have used.

5- The results on Table 1 and Table 2 might also suggest that the orthogonality is not really helpful since even without a margin, the numbers are very close compare to the case when you find the optimal margin. Am I right?

6- What do we learn from Figure 2? It is left without any discussion.",0
"This paper introduces a hierarchical clustering method using learned CNN features to build 'the tree of life'. The assumption is that the feature similarity indicates the distance in the tree. The authors tried three different ways to construct the tree: 1) approximation central point 2) minimum spanning tree and 3) multidimensional scaling based method. Out of them, MDS works the best. It is a nice application of using deep features. However, I lean toward rejecting the paper because the following reasons:

1) All experiments are conducted in very small scale. The experiments include 6 fish species, 11 canine species, 8 vehicle classes. There are no quantitative results, only by visualizing the generated tree versus the wordNet tree. Moreover, the assumption of using wordNet is not quite valid. WordNet is not designed for biology purpose and it might not reflect the true evolutionary relationship between species. 
2) Limited technical novelty. Most parts of the pipeline are standard, e.g. use pretrained model for feature extraction, use previous methods to construct hierarchical clustering. I think the technical contribution of this paper is very limited.",0
"This paper introduces a hierarchical clustering method using learned CNN features to build 'the tree of life'. The assumption is that the feature similarity indicates the distance in the tree. The authors tried three different ways to construct the tree: 1) approximation central point 2) minimum spanning tree and 3) multidimensional scaling based method. Out of them, MDS works the best. It is a nice application of using deep features. However, I lean toward rejecting the paper because the following reasons:

1) All experiments are conducted in very small scale. The experiments include 6 fish species, 11 canine species, 8 vehicle classes. There are no quantitative results, only by visualizing the generated tree versus the wordNet tree. Moreover, the assumption of using wordNet is not quite valid. WordNet is not designed for biology purpose and it might not reflect the true evolutionary relationship between species. 
2) Limited technical novelty. Most parts of the pipeline are standard, e.g. use pretrained model for feature extraction, use previous methods to construct hierarchical clustering. I think the technical contribution of this paper is very limited.",0
"This paper proposes Generative Adversarial Parallelization (GAP), one schedule to train N Generative Adversarial Networks (GANs) in parallel. GAP proceeds by shuffling the assignments between the N generators and the N discriminators at play every few epochs. Therefore, GAP forces each generator to compete with multiple discriminators at random. The authors claim that such randomization reduces undesired ""mode collapsing behaviour"", typical of GANs.

I have three concerns with this submission.

1) After training the N GANs for a sufficient amount of time, the authors propose to choose the best generator using the GAM metric. I oppose to this because of two reasons. First, a single GAN will most likely be unable to express the full richness of the true data begin modeled. Said differently, a single generator with limited power will either describe a mode well, or describe many modes poorly. Second, GAM relies on the scores given by the discriminators, which can be ill-posed (focus on artifacts). Since there is There is nothing wrong with mode collapsing when this happens under control. Thus, I believe that a better strategy would be to not choose and combine all generators into a mixture. Of course, this would require a way to decide on mixture weights. This can be done, for instance, using rejection sampling based on discriminator scores.

2) The authors should provide a theoretical (or at least conceptual) comparison to dropout. In essence, this paper has a very similar flavour: every generator is competing against all N discriminators, but at each epoch we drop N-1 for every generator. Related to the previous point, after training dropout keeps all the neurons, effectively approximating a large ensemble of neural networks.

3) The qualitative results are not convincing. Most of the figures show only results about GAP. How do the baseline samples look like? The GAN and LAPGAN papers show very similar samples. On the other hand, I do not find Figures 3 and 4 convincing: for instance, the generator in Figure 3 was most likely under-parametrized.

As a minor comment, I would remove Figure 2. This is because of three reasons: it may be protected by copyright, it occupies a lot of space, and it does not add much value to the explanation. Also, the indices (i_t) are undefined in Algorithm 1.

Overall, this paper shows good ideas, but it needs further work in terms of conceptual development and experimental evaluation.",0
"This paper proposes Generative Adversarial Parallelization (GAP), one schedule to train N Generative Adversarial Networks (GANs) in parallel. GAP proceeds by shuffling the assignments between the N generators and the N discriminators at play every few epochs. Therefore, GAP forces each generator to compete with multiple discriminators at random. The authors claim that such randomization reduces undesired ""mode collapsing behaviour"", typical of GANs.

I have three concerns with this submission.

1) After training the N GANs for a sufficient amount of time, the authors propose to choose the best generator using the GAM metric. I oppose to this because of two reasons. First, a single GAN will most likely be unable to express the full richness of the true data begin modeled. Said differently, a single generator with limited power will either describe a mode well, or describe many modes poorly. Second, GAM relies on the scores given by the discriminators, which can be ill-posed (focus on artifacts). Since there is There is nothing wrong with mode collapsing when this happens under control. Thus, I believe that a better strategy would be to not choose and combine all generators into a mixture. Of course, this would require a way to decide on mixture weights. This can be done, for instance, using rejection sampling based on discriminator scores.

2) The authors should provide a theoretical (or at least conceptual) comparison to dropout. In essence, this paper has a very similar flavour: every generator is competing against all N discriminators, but at each epoch we drop N-1 for every generator. Related to the previous point, after training dropout keeps all the neurons, effectively approximating a large ensemble of neural networks.

3) The qualitative results are not convincing. Most of the figures show only results about GAP. How do the baseline samples look like? The GAN and LAPGAN papers show very similar samples. On the other hand, I do not find Figures 3 and 4 convincing: for instance, the generator in Figure 3 was most likely under-parametrized.

As a minor comment, I would remove Figure 2. This is because of three reasons: it may be protected by copyright, it occupies a lot of space, and it does not add much value to the explanation. Also, the indices (i_t) are undefined in Algorithm 1.

Overall, this paper shows good ideas, but it needs further work in terms of conceptual development and experimental evaluation.",0
"This submission introduces a formulation of Generative Adversarial Networks (GANs) under the lens of density ratio estimation, when using Bregman divergences. Even thought GANs already perform density estimation, the motivation of using Bregman divergences is to obtain an objective function with stronger gradients. I have three concerns with this submission.

First, the exposition of the paper must be significantly improved. The current version of the manuscript is at some points unreadable, and does a poor job at motivating, describing, and justifying the contributions.

Second, the authors scatter a variety of alternatives and heuristics throughout the description of the proposed b-GAN. This introduces a great amount of complexity when it comes to understanding, implementing, and using b-GAN. Further work is necessary to rule out (in a principled manner!) many of the proposed variants of the algorithm.

Third, it is next to impossible to interpret the experimental results, in particular Figures 2, 3, 4. The authors claim that these figures show that ""learning does not stop"", but such behavior can also be attributed to the typical chaotic dynamics of GANs. Even after reading Appendix A, I am left unconvinced on whether the proposed approach provides with any practical advantage (even no comparison is offered to other GAN approaches with similar architectures).

Overall, I believe this submission calls for significant improvements before being considered for publication.",0
"This submission introduces a formulation of Generative Adversarial Networks (GANs) under the lens of density ratio estimation, when using Bregman divergences. Even thought GANs already perform density estimation, the motivation of using Bregman divergences is to obtain an objective function with stronger gradients. I have three concerns with this submission.

First, the exposition of the paper must be significantly improved. The current version of the manuscript is at some points unreadable, and does a poor job at motivating, describing, and justifying the contributions.

Second, the authors scatter a variety of alternatives and heuristics throughout the description of the proposed b-GAN. This introduces a great amount of complexity when it comes to understanding, implementing, and using b-GAN. Further work is necessary to rule out (in a principled manner!) many of the proposed variants of the algorithm.

Third, it is next to impossible to interpret the experimental results, in particular Figures 2, 3, 4. The authors claim that these figures show that ""learning does not stop"", but such behavior can also be attributed to the typical chaotic dynamics of GANs. Even after reading Appendix A, I am left unconvinced on whether the proposed approach provides with any practical advantage (even no comparison is offered to other GAN approaches with similar architectures).

Overall, I believe this submission calls for significant improvements before being considered for publication.",0
"This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs. The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) steps, where k=1,2,3,4 ). The paper also proposes a few different ways to aggregate multiple hidden states from the past.


The reviewer can see few issues with this paper.

Firstly, the writing of this paper requires improvement. The introduction and abstract are wasting too much space just to explain unrelated facts or to describe already well-known things in the literature. Some of the statements written in the paper are misleading. For instance, it explains, “Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback” and then it says RNNs cannot actually capture long-term dependencies that well. RNNs are appealing in the first place because they can handle variable length sequences and can model temporal relationships between each symbol in a sequence. The criticism against LSTMs is hard to accept when it says: LSTMs are slow and because of the slowness, they are hard to scale at larger tasks. But we all know that some companies are already using gigantic seq2seq models for their production (LSTMs are used as building blocks in their systems). This indicates that the LSTMs can be practically used in a very large-scale setting.


Secondly, the idea proposed in the paper is incremental and not new to the field. There are other previous works that propose to use direct connections to the previous hidden states [1]. However, the previous works do not use aggregation of multiple number of previous hidden states. Most importantly, the paper fails to deliver a proper analysis on whether its main contribution is actually helpful to improve the problem posed in the paper. The new architecture is said that it handles the long-term dependencies better, however, there is no rigorous proof or intuitive design in the architecture that help us to understand why it should work better. By the design of the architecture, and speaking in very high-level, it seems like the model maybe helpful to mitigate the vanishing gradients issue by a linear factor. It is always a good practice to have at least one page to analyze the empirical findings in the paper.


Thirdly, the baseline models used in this paper are very weak. Their are plenty of other models that are trained and tested on word-level language modelling task using Penn Treebank corpus, but the paper only contains a few of outdated models. I cannot fully agree on the statement “To the best of our knowledge, this is the best performance on PTB under the same training condition”, these days, RNN-based methods usually score below 80 in terms of the test perplexity, which are far lower than 100 achieved in this paper.


[1] Zhang et al., “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS’16",0
"This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs. The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) steps, where k=1,2,3,4 ). The paper also proposes a few different ways to aggregate multiple hidden states from the past.


The reviewer can see few issues with this paper.

Firstly, the writing of this paper requires improvement. The introduction and abstract are wasting too much space just to explain unrelated facts or to describe already well-known things in the literature. Some of the statements written in the paper are misleading. For instance, it explains, “Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback” and then it says RNNs cannot actually capture long-term dependencies that well. RNNs are appealing in the first place because they can handle variable length sequences and can model temporal relationships between each symbol in a sequence. The criticism against LSTMs is hard to accept when it says: LSTMs are slow and because of the slowness, they are hard to scale at larger tasks. But we all know that some companies are already using gigantic seq2seq models for their production (LSTMs are used as building blocks in their systems). This indicates that the LSTMs can be practically used in a very large-scale setting.


Secondly, the idea proposed in the paper is incremental and not new to the field. There are other previous works that propose to use direct connections to the previous hidden states [1]. However, the previous works do not use aggregation of multiple number of previous hidden states. Most importantly, the paper fails to deliver a proper analysis on whether its main contribution is actually helpful to improve the problem posed in the paper. The new architecture is said that it handles the long-term dependencies better, however, there is no rigorous proof or intuitive design in the architecture that help us to understand why it should work better. By the design of the architecture, and speaking in very high-level, it seems like the model maybe helpful to mitigate the vanishing gradients issue by a linear factor. It is always a good practice to have at least one page to analyze the empirical findings in the paper.


Thirdly, the baseline models used in this paper are very weak. Their are plenty of other models that are trained and tested on word-level language modelling task using Penn Treebank corpus, but the paper only contains a few of outdated models. I cannot fully agree on the statement “To the best of our knowledge, this is the best performance on PTB under the same training condition”, these days, RNN-based methods usually score below 80 in terms of the test perplexity, which are far lower than 100 achieved in this paper.


[1] Zhang et al., “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS’16",0
"This paper introduces a mechanism for active learning with convolutional neural networks (CNNs). I would not go as far as the authors in calling these ""deep"", seeing that they seem to have only 2 hidden layers with only 20 filters each. The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.

The paper is sometimes hard to read, due to (a) many grammatical errors and (b) sloppy notation in some places (e.g., on page 5, line 1, f is used but never introduced before). Overall, I give an accepting score, but a weak one because of the grammatical errors. If the paper is accepted, these should be fixed for the final version, optimally by a native speaker. 

The paper's topic is interesting, and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets). I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS? In particular, uncertainty sampling did very well (in fact, better than the authors' new method) on MNIST, but apparently horribly on USPS; some explanation for this would be useful.

I have one more question: why is it necessary to first sample a larger subset D \subset U, from which we select using active learning? Is this merely done for reasons of computational efficiency, or can it actually somehow improve results? (If so, it would be instrumental to see the worse results when this is not done.)",0
"This paper introduces a mechanism for active learning with convolutional neural networks (CNNs). I would not go as far as the authors in calling these ""deep"", seeing that they seem to have only 2 hidden layers with only 20 filters each. The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.

The paper is sometimes hard to read, due to (a) many grammatical errors and (b) sloppy notation in some places (e.g., on page 5, line 1, f is used but never introduced before). Overall, I give an accepting score, but a weak one because of the grammatical errors. If the paper is accepted, these should be fixed for the final version, optimally by a native speaker. 

The paper's topic is interesting, and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets). I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS? In particular, uncertainty sampling did very well (in fact, better than the authors' new method) on MNIST, but apparently horribly on USPS; some explanation for this would be useful.

I have one more question: why is it necessary to first sample a larger subset D \subset U, from which we select using active learning? Is this merely done for reasons of computational efficiency, or can it actually somehow improve results? (If so, it would be instrumental to see the worse results when this is not done.)",0
"The authors develop a way learn subspaces of multiple views such that data point neighborhoods are similar in all of the views.  This similarity is measured between distributions of neighbors in pairs of views. The motivation is that this is a natural criterion for information retrieval.

I like the idea of preserving neighborhood relationships across views for retrieval tasks. And it is nice that the learned spaces can have different dimensionalities for different views.  However, the empirical validation seems preliminary.

The paper has been revised from the authors' ICLR 2016 submission, and the revisions are welcome, but I think the paper still needs more work in order to be publishable.  In its current form it could be a good match for the workshop track.

The experiments are all on very small data sets (e.g. 2000 examples in each of train/test on the MNIST task) and not on real tasks.  The authors point out that they are not focusing on efficiency, and presumably computation requirements keep them from considering larger data sets.  However, it is not clear that there is any conclusion that can be drawn that would apply to more realistic data sets.  Considering the wealth of work that's been done on multi-view subspace learning, with application to real tasks, it is very hard to see this as a contribution without showing that it is applicable in such realistic settings.

On a more minor point, the authors claim that no other information retrieval based approaches exist, and I think this is a bit overstated.  For example, the contrastive loss of Hermann & Blunsom ""Multilingual models for compositional distributed semantics"" ACL 2014 is related to information retrieval and would be a natural one to compare against.

The presentation is a bit sloppy, with a number of vague points and confusing wordings.  Examples:
- the term ""dependency"" gets used in the paper a lot in a rather colloquial way.  This gets confusing at times since it is used in a technical context but not using its technical definition.
- ""an information retrieval task of the analyst"": vague and not quite grammatical
- ""the probability that an analyst who inspected item i will next pick j for inspection"" is not well-defined
- In the discussion of KL divergence, I do not quite follow the reasoning about its relationship to the ""cost of misses"" etc.  It would help to make this more precise (or perhaps drop it?  KL divergence is pretty well motivated here anyway).
- Does C_{Penalty} (7) get added to C (6), or is it used instead?  I was a bit confused here.
- It is stated that CCA ""iteratively finds component pairs"".  Note that while CCA can be defined as an iterative operation, it need not (and typically is not) solved that way, but rather all projections are found at once.
- How is PCA done ""between X_i^1 and X_i^2""?
- ""We apply nonlinear dimensionality algorithm"": what is this algorithm?
- I do not quite follow what the task is in the case of the image patches and stock prices.

Other minor comments, typos, etc.:
- The figure fonts are too small.
- ""difference measures"" --> ""different measures""
- ""...since, hence any two..."": not grammatical
- ""between feature-based views and views external neighborhoods"": ?",0
"The authors develop a way learn subspaces of multiple views such that data point neighborhoods are similar in all of the views.  This similarity is measured between distributions of neighbors in pairs of views. The motivation is that this is a natural criterion for information retrieval.

I like the idea of preserving neighborhood relationships across views for retrieval tasks. And it is nice that the learned spaces can have different dimensionalities for different views.  However, the empirical validation seems preliminary.

The paper has been revised from the authors' ICLR 2016 submission, and the revisions are welcome, but I think the paper still needs more work in order to be publishable.  In its current form it could be a good match for the workshop track.

The experiments are all on very small data sets (e.g. 2000 examples in each of train/test on the MNIST task) and not on real tasks.  The authors point out that they are not focusing on efficiency, and presumably computation requirements keep them from considering larger data sets.  However, it is not clear that there is any conclusion that can be drawn that would apply to more realistic data sets.  Considering the wealth of work that's been done on multi-view subspace learning, with application to real tasks, it is very hard to see this as a contribution without showing that it is applicable in such realistic settings.

On a more minor point, the authors claim that no other information retrieval based approaches exist, and I think this is a bit overstated.  For example, the contrastive loss of Hermann & Blunsom ""Multilingual models for compositional distributed semantics"" ACL 2014 is related to information retrieval and would be a natural one to compare against.

The presentation is a bit sloppy, with a number of vague points and confusing wordings.  Examples:
- the term ""dependency"" gets used in the paper a lot in a rather colloquial way.  This gets confusing at times since it is used in a technical context but not using its technical definition.
- ""an information retrieval task of the analyst"": vague and not quite grammatical
- ""the probability that an analyst who inspected item i will next pick j for inspection"" is not well-defined
- In the discussion of KL divergence, I do not quite follow the reasoning about its relationship to the ""cost of misses"" etc.  It would help to make this more precise (or perhaps drop it?  KL divergence is pretty well motivated here anyway).
- Does C_{Penalty} (7) get added to C (6), or is it used instead?  I was a bit confused here.
- It is stated that CCA ""iteratively finds component pairs"".  Note that while CCA can be defined as an iterative operation, it need not (and typically is not) solved that way, but rather all projections are found at once.
- How is PCA done ""between X_i^1 and X_i^2""?
- ""We apply nonlinear dimensionality algorithm"": what is this algorithm?
- I do not quite follow what the task is in the case of the image patches and stock prices.

Other minor comments, typos, etc.:
- The figure fonts are too small.
- ""difference measures"" --> ""different measures""
- ""...since, hence any two..."": not grammatical
- ""between feature-based views and views external neighborhoods"": ?",0
"This paper proposes a character-aware attention residual network for sentence embedding. Several text classification tasks are used to evaluate the effectiveness of the proposed model. On two of the three tasks, the residual network outforms a few baselines, but couldn't beat the simple TFIDF-SVM on the last one.

This work is not novel enough. Character information has been applied in many previously published work, as cited by the authors. Residual network is also not new.

Why not testing the model on a few more widely used datasets for short text classification, such as TREC? More competitive baselines can be compared to. Also, it's not clear how the ""Question"" dataset was created and which domain it is.

Last, it is surprising that the format of citations throughout the paper is all wrong. 

For example:
like Word2Vec Mikolov et al. (2013)
->
like Word2Vec (Mikolov et al., 2013)

The citations can't just mix with the normal text. Please refer to other published papers.",0
"This paper proposes a character-aware attention residual network for sentence embedding. Several text classification tasks are used to evaluate the effectiveness of the proposed model. On two of the three tasks, the residual network outforms a few baselines, but couldn't beat the simple TFIDF-SVM on the last one.

This work is not novel enough. Character information has been applied in many previously published work, as cited by the authors. Residual network is also not new.

Why not testing the model on a few more widely used datasets for short text classification, such as TREC? More competitive baselines can be compared to. Also, it's not clear how the ""Question"" dataset was created and which domain it is.

Last, it is surprising that the format of citations throughout the paper is all wrong. 

For example:
like Word2Vec Mikolov et al. (2013)
->
like Word2Vec (Mikolov et al., 2013)

The citations can't just mix with the normal text. Please refer to other published papers.",0
"This paper describes a method that estimates the similarity between a set of images by alternatively attend each image with a recurrent manner. The idea of the paper is interesting, which mimic the human's behavior. However, there are several cons of the paper:

1. The paper is now well written. There are too many 'TODO', 'CITE' in the final version of the paper, which indicates that the paper is submitted in a rush or the authors did not take much care about the paper. I think the paper is not suitable to be published with the current version.

2. The missing of the experimental results. The paper mentioned the LFW dataset. However, the paper did not provide the results on LFW dataset. (At least I did not find it in the version of Dec. 13th)

3. The experiments of Omniglot dataset are not sufficient. I suggest that the paper provides some illustrations about how the model the attend two images (e.g. the trajectory of attend).",0
"This paper describes a method that estimates the similarity between a set of images by alternatively attend each image with a recurrent manner. The idea of the paper is interesting, which mimic the human's behavior. However, there are several cons of the paper:

1. The paper is now well written. There are too many 'TODO', 'CITE' in the final version of the paper, which indicates that the paper is submitted in a rush or the authors did not take much care about the paper. I think the paper is not suitable to be published with the current version.

2. The missing of the experimental results. The paper mentioned the LFW dataset. However, the paper did not provide the results on LFW dataset. (At least I did not find it in the version of Dec. 13th)

3. The experiments of Omniglot dataset are not sufficient. I suggest that the paper provides some illustrations about how the model the attend two images (e.g. the trajectory of attend).",0
"This paper proposes a neural architecture for answering non-factoid questions. The author's model improves over previous neural models for answer sentence selection. Experiments are conducted on a Japanese love advice corpus; the coolest part of the paper for me was that the model was actually rolled out to the public and its answers were rated twice as good as actual human contributors! 

It was hard for me to determine the novelty of the contribution. The authors mention that their model ""fills the gap
between answer selection and generation""; however, no generation is actually performed by the model! Instead, the model appears to be very similar to the QA-LSTM of Tan et al., 2015 except that there are additional terms in the objective to handle conclusion and supplementary sentences. The structure of the answer is fixed to a predefined template (e.g., conclusion --> supplementary), so the model is not really learning how to order the sentences. The other contribution is the ""word embedding with semantics"" portion described in sec 4.1, which is essentially just the paragraph vector model except with ""titles"" and ""categories"" instead of paragraphs. 

While the result of the paper is a model that has actually demonstrated real-life usefulness, the technical contributions do not strike me as novel enough for publication at ICLR.

Other comments:
- One major issue with the reliance of the model on the template is that you can't evaluate on commonly-used non-factoid QA datasets such as InsuranceQA. If the template were not fixed beforehand (but possibly learned by the model), you could conceivably evaluate on different datasets. 
- The examples in Table 4 don't show a clear edge in answer quality to your model; QA-LSTM seems to choose good answers as well.
- Doesn't the construction model have an advantage over the vanilla QA-LSTM in that it knows which sentences are conclusions and which are supplementary? Or does QA-LSTM also get this distinction?",0
"This paper proposes a neural architecture for answering non-factoid questions. The author's model improves over previous neural models for answer sentence selection. Experiments are conducted on a Japanese love advice corpus; the coolest part of the paper for me was that the model was actually rolled out to the public and its answers were rated twice as good as actual human contributors! 

It was hard for me to determine the novelty of the contribution. The authors mention that their model ""fills the gap
between answer selection and generation""; however, no generation is actually performed by the model! Instead, the model appears to be very similar to the QA-LSTM of Tan et al., 2015 except that there are additional terms in the objective to handle conclusion and supplementary sentences. The structure of the answer is fixed to a predefined template (e.g., conclusion --> supplementary), so the model is not really learning how to order the sentences. The other contribution is the ""word embedding with semantics"" portion described in sec 4.1, which is essentially just the paragraph vector model except with ""titles"" and ""categories"" instead of paragraphs. 

While the result of the paper is a model that has actually demonstrated real-life usefulness, the technical contributions do not strike me as novel enough for publication at ICLR.

Other comments:
- One major issue with the reliance of the model on the template is that you can't evaluate on commonly-used non-factoid QA datasets such as InsuranceQA. If the template were not fixed beforehand (but possibly learned by the model), you could conceivably evaluate on different datasets. 
- The examples in Table 4 don't show a clear edge in answer quality to your model; QA-LSTM seems to choose good answers as well.
- Doesn't the construction model have an advantage over the vanilla QA-LSTM in that it knows which sentences are conclusions and which are supplementary? Or does QA-LSTM also get this distinction?",0
"This paper extends boosting to the task of learning generative models of data. The strong learner is obtained as a geometric average of “weak learners”, which can themselves be normalized (e.g. VAE) or un-normalized (e.g. RBMs) generative models (genBGM), or a classifier trained to discriminate between the strong learner at iteration T-1 and the true data distribution (discBGM). This latter method is closely related to Noise Contrastive Estimation, GANs, etc.

The approach benefits from strong theoretical guarantees, with strict conditions under which each boosting iteration is guaranteed to improve the log-likelihood. The downside of the method appears to be the lack of normalization constant for the resulting strong learner and the use of heuristics to weight each weak learner (which seems to matter in practice, from Sec. 3.2). The discriminative approach further suffers from an expensive training procedure: each round of boosting first requires generating a “training set” worth of samples from the previous strong learner, where samples are obtained via MCMC.

The experimental section is clearly the weak point of the paper. The method is evaluated on a synthetic dataset, and a single real-world dataset, MNIST: both for generation and as a feature extraction mechanism for classification. Of these, the synthetic experiments were the clearest in showcasing the method. On MNIST, the baseline models are much too weak for the results to be convincing. A modestly sized VAE can obtain 90 nats within hours on a single GPU, clearly an achievable goal. Furthermore, despite arguments to the contrary, I firmly believe that mixing base learners is an academic exercise, if only because of the burden of implementing K different models & training algorithms. This section fails to answer a more fundamental question: is it better to train a large VAE by maximizing the elbow, or e.g. train 10 iterations of boosting, using VAEs 1/10th the size of the baseline model ? Experimental details are also lacking, especially with respect to the sampling procedure used to draw samples from the BGM. The paper would also benefit from likelihood estimates obtained via AIS.

With regards to novelty and prior work, there is also a missing reference to “Self Supervised Boosting” by Welling et al [R1]. After a cursory read through, there seems to be strong similarities to the GenBGM approach which ought to be discussed.

Overall, I am on the fence. The idea of boosting generative models is intriguing, seems well motivated and has potential for impact. For this reason, and given the theoretical contributions, I am willing to overlook some of the issues highlighted above, and hope the authors can address some of them in time for the rebuttal.

[R1]",0
"This paper extends boosting to the task of learning generative models of data. The strong learner is obtained as a geometric average of “weak learners”, which can themselves be normalized (e.g. VAE) or un-normalized (e.g. RBMs) generative models (genBGM), or a classifier trained to discriminate between the strong learner at iteration T-1 and the true data distribution (discBGM). This latter method is closely related to Noise Contrastive Estimation, GANs, etc.

The approach benefits from strong theoretical guarantees, with strict conditions under which each boosting iteration is guaranteed to improve the log-likelihood. The downside of the method appears to be the lack of normalization constant for the resulting strong learner and the use of heuristics to weight each weak learner (which seems to matter in practice, from Sec. 3.2). The discriminative approach further suffers from an expensive training procedure: each round of boosting first requires generating a “training set” worth of samples from the previous strong learner, where samples are obtained via MCMC.

The experimental section is clearly the weak point of the paper. The method is evaluated on a synthetic dataset, and a single real-world dataset, MNIST: both for generation and as a feature extraction mechanism for classification. Of these, the synthetic experiments were the clearest in showcasing the method. On MNIST, the baseline models are much too weak for the results to be convincing. A modestly sized VAE can obtain 90 nats within hours on a single GPU, clearly an achievable goal. Furthermore, despite arguments to the contrary, I firmly believe that mixing base learners is an academic exercise, if only because of the burden of implementing K different models & training algorithms. This section fails to answer a more fundamental question: is it better to train a large VAE by maximizing the elbow, or e.g. train 10 iterations of boosting, using VAEs 1/10th the size of the baseline model ? Experimental details are also lacking, especially with respect to the sampling procedure used to draw samples from the BGM. The paper would also benefit from likelihood estimates obtained via AIS.

With regards to novelty and prior work, there is also a missing reference to “Self Supervised Boosting” by Welling et al [R1]. After a cursory read through, there seems to be strong similarities to the GenBGM approach which ought to be discussed.

Overall, I am on the fence. The idea of boosting generative models is intriguing, seems well motivated and has potential for impact. For this reason, and given the theoretical contributions, I am willing to overlook some of the issues highlighted above, and hope the authors can address some of them in time for the rebuttal.

[R1]",0
"After the rebuttal:

The paper contains an interesting set of results (mainly produced after the initial submission), but novelty is limited, and presentation is suboptimal. 

For me now the biggest problem now is that the title and the content do not correspond. The authors clearly attack deterministic encoder-decoder models (as described in 3.2), which are not at all the same as generative models, even though many generative models make use of this architecture. A small experiment with sampling is interesting, but does not change the overall focus of the paper. This inconsistency in not acceptable. The whole issue could be resolved for example by simply replacing ""generative models"" by ""encoder-decoder networks"" in the title. Then I would tend towards recommending acceptance.

------
Initial review:

The paper describes three approaches to generating adversarial examples for deep encoder-decoder generative networks (trained as VAE or VAE-GAN), and shows a comparative analysis of these. While the phenomenon of adversarial examples in discriminative models is widely known and relatively well studied, I am not aware of previous work on adversarial examples for generative networks, so this work is novel (there is a concurrent work by Tabacof et al. which should be cited, though).  The paper has significantly improved since the initial submission; still, I have a number of remarks on presentation and experimental evaluation. I am in the borderline mode, and may change my rating during the discussion phase.

Detailed comments:

1) The paper is 13 pages long - significantly over the recommended page limit of 8 pages. Reviewers have to read multiple papers, multiple versions of each, it is a lot of work. Large portions of the paper should be shortened and/or moved to the appendix. It is job of the authors to make the paper concise and readable. ""in our attempts to be thorough, we have had a hard time keeping the length down"" is a bad excuse - it may be hard, but has to be done.

2) I intentionally avoided term ""generative model"" above because it is not obvious to me if the attacks described by the authors indeed attack generative models. To clarify, the authors train encoder-decoders as generative models (VAE or VAE-GAN), but then remove all stochasticity (sampling) and prior on the latent variables: that is, they treat the models as deterministic encoders-decoders. It is not a big surprise that a deterministic deep network can be easily tricked; it would be much more interesting to see if the probabilistic aspect of generative models makes them more robust to such attacks. Am I missing something? I would like the authors to clarify their view on this and adjust the claims in the paper if necessary.

3) The paper is motivated by possible attacks on a data channel which uses a generative network for compressing information. Description of the attack scenario in 3.1 does not look convincing to me. It takes a huge amount of space and I do not think it adds much to the paper. First, experiments on natural images are necessary to judge if the proposed attack could succeed in a realistic scenario and second, I am not aware of any existing practical applications of VAEs to image compression: attacking JPEG would be much more practical. 

4) Experiments are limited to MNIST and, in the latest version, SVHN (which is very nice). While no good generative models of general natural images exist, it is common to evaluate generative models on datasets of faces, so this would be another very natural domain for testing the proposed approach. 

Smaller remarks:
1) Usage of ""Oracle"" in 3.2 does not look appropriate - oracle typically has access to (part of) ground truth, which is not the case here as far as I understand.
2) Beginning of section 4: ""All three methods work for any generative architecture that relies on a learned latent representation z"" - ""are technically applicable to"" would be more correct than ""work for"". 
3) 4.1: ""confidentally""",0
"After the rebuttal:

The paper contains an interesting set of results (mainly produced after the initial submission), but novelty is limited, and presentation is suboptimal. 

For me now the biggest problem now is that the title and the content do not correspond. The authors clearly attack deterministic encoder-decoder models (as described in 3.2), which are not at all the same as generative models, even though many generative models make use of this architecture. A small experiment with sampling is interesting, but does not change the overall focus of the paper. This inconsistency in not acceptable. The whole issue could be resolved for example by simply replacing ""generative models"" by ""encoder-decoder networks"" in the title. Then I would tend towards recommending acceptance.

------
Initial review:

The paper describes three approaches to generating adversarial examples for deep encoder-decoder generative networks (trained as VAE or VAE-GAN), and shows a comparative analysis of these. While the phenomenon of adversarial examples in discriminative models is widely known and relatively well studied, I am not aware of previous work on adversarial examples for generative networks, so this work is novel (there is a concurrent work by Tabacof et al. which should be cited, though).  The paper has significantly improved since the initial submission; still, I have a number of remarks on presentation and experimental evaluation. I am in the borderline mode, and may change my rating during the discussion phase.

Detailed comments:

1) The paper is 13 pages long - significantly over the recommended page limit of 8 pages. Reviewers have to read multiple papers, multiple versions of each, it is a lot of work. Large portions of the paper should be shortened and/or moved to the appendix. It is job of the authors to make the paper concise and readable. ""in our attempts to be thorough, we have had a hard time keeping the length down"" is a bad excuse - it may be hard, but has to be done.

2) I intentionally avoided term ""generative model"" above because it is not obvious to me if the attacks described by the authors indeed attack generative models. To clarify, the authors train encoder-decoders as generative models (VAE or VAE-GAN), but then remove all stochasticity (sampling) and prior on the latent variables: that is, they treat the models as deterministic encoders-decoders. It is not a big surprise that a deterministic deep network can be easily tricked; it would be much more interesting to see if the probabilistic aspect of generative models makes them more robust to such attacks. Am I missing something? I would like the authors to clarify their view on this and adjust the claims in the paper if necessary.

3) The paper is motivated by possible attacks on a data channel which uses a generative network for compressing information. Description of the attack scenario in 3.1 does not look convincing to me. It takes a huge amount of space and I do not think it adds much to the paper. First, experiments on natural images are necessary to judge if the proposed attack could succeed in a realistic scenario and second, I am not aware of any existing practical applications of VAEs to image compression: attacking JPEG would be much more practical. 

4) Experiments are limited to MNIST and, in the latest version, SVHN (which is very nice). While no good generative models of general natural images exist, it is common to evaluate generative models on datasets of faces, so this would be another very natural domain for testing the proposed approach. 

Smaller remarks:
1) Usage of ""Oracle"" in 3.2 does not look appropriate - oracle typically has access to (part of) ground truth, which is not the case here as far as I understand.
2) Beginning of section 4: ""All three methods work for any generative architecture that relies on a learned latent representation z"" - ""are technically applicable to"" would be more correct than ""work for"". 
3) 4.1: ""confidentally""",0
"This paper proposed to use Generalized Advantage Estimation (GAE) to optimize DNNs for information seeking tasks. The task is posed as a reinforcement learning problem and the proposed method explicitly promotes information gain to encourage exploration.

Both GAE and DNN have been used for RL before. The novelty in this paper seems to be the explicit modeling of information gain. However, there is insufficient empirical evidence to demonstrate the benefit and generality of the proposed method. An apple to apple comparison to previous RL framework that doesn't model information gain is missing. For example, the cluttered MNIST experiment tried to compare against Mnih et al. (2014) (which is a little out dated) with two settings. But in both setting the input to the two methods are different. Thus it is unclear what contributed to the performance difference.

The experiment section is cluttered and hard to read. A table that summarizes the numbers would be much better.",0
"This paper proposed to use Generalized Advantage Estimation (GAE) to optimize DNNs for information seeking tasks. The task is posed as a reinforcement learning problem and the proposed method explicitly promotes information gain to encourage exploration.

Both GAE and DNN have been used for RL before. The novelty in this paper seems to be the explicit modeling of information gain. However, there is insufficient empirical evidence to demonstrate the benefit and generality of the proposed method. An apple to apple comparison to previous RL framework that doesn't model information gain is missing. For example, the cluttered MNIST experiment tried to compare against Mnih et al. (2014) (which is a little out dated) with two settings. But in both setting the input to the two methods are different. Thus it is unclear what contributed to the performance difference.

The experiment section is cluttered and hard to read. A table that summarizes the numbers would be much better.",0
"The manuscript is a bit scattered and hard to follow. There is technical depth but the paper doesn't do a good job explaining what shortcoming the proposed methods are overcoming and what baselines they are outperforming. 

The writing could be improved. There are numerous grammatical errors.

The experiments in 3.1 are interesting, but you need to be clearer about the relationship of your ResCeption method to the state-of-the-art. The use of extensive footnotes on page 5 is a bit odd. ""That is a competitive result"" is vague. A footnote links to """,0
"The manuscript is a bit scattered and hard to follow. There is technical depth but the paper doesn't do a good job explaining what shortcoming the proposed methods are overcoming and what baselines they are outperforming. 

The writing could be improved. There are numerous grammatical errors.

The experiments in 3.1 are interesting, but you need to be clearer about the relationship of your ResCeption method to the state-of-the-art. The use of extensive footnotes on page 5 is a bit odd. ""That is a competitive result"" is vague. A footnote links to """,0
"The authors propose to combine a CCA objective with a downstream loss.  This is a really nice and natural idea.  However, both the execution and presentation leave a lot to be desired in the current version of the paper.

It is not clear what the overall objective is.  This was asked in a pre-review question but the answer did not fully clarify it for me.  Is it the sum of the CCA objective and the final (top-layer) objective, including the CCA constraints?  Is there some interpolation of the two objectives?  

By saying that the top-layer objective is ""cosine distance"" or ""squared cosine distance"", do you really mean you are just minimizing this distance between the matched pairs in the two views?  If so, then of course that does not work out of the box without the intervening CCA layer:  You could minimize it by setting all of the projections to a single point.  A better comparison would be against a contrastive loss like the Hermann & Blunsom one mentioned in the reviewer question, which aims to both minimize the distance for matched pairs and separate mismatched ones (where ""mismatched"" ones can be uniformly drawn, or picked in some cleverer way).  But other discriminative top-layer objectives that are tailored to a downstream task could make sense.

There is some loose terminology in the paper.  The authors refer to the ""correlation"" and ""cross-correlation"" between two vectors.  ""Correlation"" normally applies to scalars, so you need to define what you mean here.  ""Cross-correlation"" typically refers to time series.  In eq. (2) you are taking the max of a matrix.  Finally I am not too sure in what way this approach is ""fully differentiable"" while regular CCA is not -- perhaps it is worth revisiting this term as well.

Also just a small note about the relationship between cosine distance and correlation:  they are related when we view the dimensions of each of the two vectors as samples of a single random variable.  In that case the cosine distance of the (mean-normalized) vectors is the same as the correlation between the two corresponding random variables.  In CCA we are viewing each dimension of the vectors as its own random variable.  So I fear the claim about cosine distance and correlation is a bit of a red herring here.

A couple of typos:

""prosed"" --> ""proposed""
""allong"" --> ""along""",0
"The authors propose to combine a CCA objective with a downstream loss.  This is a really nice and natural idea.  However, both the execution and presentation leave a lot to be desired in the current version of the paper.

It is not clear what the overall objective is.  This was asked in a pre-review question but the answer did not fully clarify it for me.  Is it the sum of the CCA objective and the final (top-layer) objective, including the CCA constraints?  Is there some interpolation of the two objectives?  

By saying that the top-layer objective is ""cosine distance"" or ""squared cosine distance"", do you really mean you are just minimizing this distance between the matched pairs in the two views?  If so, then of course that does not work out of the box without the intervening CCA layer:  You could minimize it by setting all of the projections to a single point.  A better comparison would be against a contrastive loss like the Hermann & Blunsom one mentioned in the reviewer question, which aims to both minimize the distance for matched pairs and separate mismatched ones (where ""mismatched"" ones can be uniformly drawn, or picked in some cleverer way).  But other discriminative top-layer objectives that are tailored to a downstream task could make sense.

There is some loose terminology in the paper.  The authors refer to the ""correlation"" and ""cross-correlation"" between two vectors.  ""Correlation"" normally applies to scalars, so you need to define what you mean here.  ""Cross-correlation"" typically refers to time series.  In eq. (2) you are taking the max of a matrix.  Finally I am not too sure in what way this approach is ""fully differentiable"" while regular CCA is not -- perhaps it is worth revisiting this term as well.

Also just a small note about the relationship between cosine distance and correlation:  they are related when we view the dimensions of each of the two vectors as samples of a single random variable.  In that case the cosine distance of the (mean-normalized) vectors is the same as the correlation between the two corresponding random variables.  In CCA we are viewing each dimension of the vectors as its own random variable.  So I fear the claim about cosine distance and correlation is a bit of a red herring here.

A couple of typos:

""prosed"" --> ""proposed""
""allong"" --> ""along""",0
"The authors propose a method to investigate the predictiveness of intermediate layer activations. To do so, they propose training linear classifiers and evaluate the error on the test set.

The paper is well motivated and aims to shed some light onto the progress of model training and hopes to provide insights into deep learning architecture design.

The two main reasons for why the authors decided to use linear probes seem to be:
- convexity
- The last layer in the network is (usually) linear

In the second to last paragraph of page 4 the authors point out that it could happen that the intermediate features are useless for a linear classifier. This is correct and what I consider the main flaw of the paper. I am missing any motivation as to the usefulness of the suggested analysis to architecture design. In fact, the example with the skip connection (Figure 8) seems to suggest that skip connections shouldn't be used. Doesn't that contradict the recent successes of ResNet?

While the results are interesting, they aren't particularly surprising and I am failing to see direct applicability to understanding deep models as the authors suggest.",0
"The authors propose a method to investigate the predictiveness of intermediate layer activations. To do so, they propose training linear classifiers and evaluate the error on the test set.

The paper is well motivated and aims to shed some light onto the progress of model training and hopes to provide insights into deep learning architecture design.

The two main reasons for why the authors decided to use linear probes seem to be:
- convexity
- The last layer in the network is (usually) linear

In the second to last paragraph of page 4 the authors point out that it could happen that the intermediate features are useless for a linear classifier. This is correct and what I consider the main flaw of the paper. I am missing any motivation as to the usefulness of the suggested analysis to architecture design. In fact, the example with the skip connection (Figure 8) seems to suggest that skip connections shouldn't be used. Doesn't that contradict the recent successes of ResNet?

While the results are interesting, they aren't particularly surprising and I am failing to see direct applicability to understanding deep models as the authors suggest.",0
"This paper is an extension of Lenc&Vedaldi15 paper, showing CNN representations at FC7 layer are to certain extent equivariant to various classes of transformations and that training with a certain group of transformation makes the representations more equivalent.

Authors performed a large amount of experiments, training over 30 networks with different forms of jitter, which is quite impressive. However it is rather difficult to find a main message of this work. Yes, authors measured the properties on a different layer than the Lenc&Vedaldi15, however it is hard to find some novel insights other than the known fact that jitter helps to achieve invariance. The evaluation seems to be mostly correct, however the paper does not seem to be solving the task advertised in its title really well.

Major issues are in the experiments with the representation distances:
* The selection of only FC7 is a bit controversial - it is followed only by a single classification layer (FC8) to the common output - class likelyhoods. Because the FC8 is just a linear projections, what the equivalence map does is just to re-project the FC8 weights of the attached network to the weights of the original network. Probably performing similar experiments but on more layers may be more useful (as the networks are already trained).
* The experiment with representation distance is missing what is the classification error on the testing dataset. This would answer whether the representations are actually compatible up to linear transformation at all...
* It is not clear for the experiment with K-NN whether this is measured per each test set example? After training the equivalence map? More clear would be to show that networks trained on similar group of jitter transformations are more compatible on the target task.
* The proposed method does not seem to improve equivariance consistently on all tasks. Especially with \lambda_1 and \lambda_2 having such small values, the loss is basically equal to simple data jitter as it just adds up the loss of the original and transformed image. Maybe the issue is in the selection of the FC7 layer?

In general, this paper shows some interesting results on the FC7 equivariance, but it does not seem to be drawing many interesting new observations out of these experiments. Due to some issues with the equivalence experiments and the finetuning of equivariance, I would not recommend acceptance of this manuscript. However, refining the experiments on already trained networks and restructuring this manuscript into more investigative work may lead to interesting contribution to the field.

There are also few minor issues:
* It is not experimentally verified that the new criterion for equivariance mapping helps to gain better results.
* The angles on page 1 and 5 are missing units (degrees?).
* On page three, ""In practice, it is difficult... "", it is not M_g which is maximised/minimised, but the loss over the M_g
* Page 4, footnote 2 - if you are just halving the activations, it is hard to call it a dropout as this constant factor can be passed to the following/preceding weights
* Is the network for RVL-CDIP the same architecture as Alexnet?
* On page 7, Figure 3a+3b - in my opinion, turning the diagonal elements to white is really misleading, and probably even incorrect, as the distance between the same representations should be zero (which is also a way how to verify that the experiments are performed correctly).",0
"This paper is an extension of Lenc&Vedaldi15 paper, showing CNN representations at FC7 layer are to certain extent equivariant to various classes of transformations and that training with a certain group of transformation makes the representations more equivalent.

Authors performed a large amount of experiments, training over 30 networks with different forms of jitter, which is quite impressive. However it is rather difficult to find a main message of this work. Yes, authors measured the properties on a different layer than the Lenc&Vedaldi15, however it is hard to find some novel insights other than the known fact that jitter helps to achieve invariance. The evaluation seems to be mostly correct, however the paper does not seem to be solving the task advertised in its title really well.

Major issues are in the experiments with the representation distances:
* The selection of only FC7 is a bit controversial - it is followed only by a single classification layer (FC8) to the common output - class likelyhoods. Because the FC8 is just a linear projections, what the equivalence map does is just to re-project the FC8 weights of the attached network to the weights of the original network. Probably performing similar experiments but on more layers may be more useful (as the networks are already trained).
* The experiment with representation distance is missing what is the classification error on the testing dataset. This would answer whether the representations are actually compatible up to linear transformation at all...
* It is not clear for the experiment with K-NN whether this is measured per each test set example? After training the equivalence map? More clear would be to show that networks trained on similar group of jitter transformations are more compatible on the target task.
* The proposed method does not seem to improve equivariance consistently on all tasks. Especially with \lambda_1 and \lambda_2 having such small values, the loss is basically equal to simple data jitter as it just adds up the loss of the original and transformed image. Maybe the issue is in the selection of the FC7 layer?

In general, this paper shows some interesting results on the FC7 equivariance, but it does not seem to be drawing many interesting new observations out of these experiments. Due to some issues with the equivalence experiments and the finetuning of equivariance, I would not recommend acceptance of this manuscript. However, refining the experiments on already trained networks and restructuring this manuscript into more investigative work may lead to interesting contribution to the field.

There are also few minor issues:
* It is not experimentally verified that the new criterion for equivariance mapping helps to gain better results.
* The angles on page 1 and 5 are missing units (degrees?).
* On page three, ""In practice, it is difficult... "", it is not M_g which is maximised/minimised, but the loss over the M_g
* Page 4, footnote 2 - if you are just halving the activations, it is hard to call it a dropout as this constant factor can be passed to the following/preceding weights
* Is the network for RVL-CDIP the same architecture as Alexnet?
* On page 7, Figure 3a+3b - in my opinion, turning the diagonal elements to white is really misleading, and probably even incorrect, as the distance between the same representations should be zero (which is also a way how to verify that the experiments are performed correctly).",0
"1) Summary

This paper proposes an end-to-end hybrid architecture to predict the local linear trends of time series. A temporal convnet on raw data extracts short-term features. In parallel, long term representations are learned via a LSTM on piecewise linear approximations of the time series. Both representations are combined using a MLP with one hidden layer (in two parts, one for each stream), and the entire architecture is trained end-to-end by minimizing (using Adam) the (l2-regularized) euclidean loss w.r.t. ground truth local trend durations and slopes.
 
2) Contributions

+ Interesting end-to-end architecture decoupling short-term and long-term representation learning in two separate streams in the first part of the architecture.
+ Comparison to deep and shallow baselines.

3) Suggestions for improvement

Add a LRCN baseline and discussion:
The benefits of decoupling short-term and long-term representation learning need to be assessed by comparing to the popular ""long-term recurrent convolutional network"" (LRCN) of Donahue et al (",0
"1) Summary

This paper proposes an end-to-end hybrid architecture to predict the local linear trends of time series. A temporal convnet on raw data extracts short-term features. In parallel, long term representations are learned via a LSTM on piecewise linear approximations of the time series. Both representations are combined using a MLP with one hidden layer (in two parts, one for each stream), and the entire architecture is trained end-to-end by minimizing (using Adam) the (l2-regularized) euclidean loss w.r.t. ground truth local trend durations and slopes.
 
2) Contributions

+ Interesting end-to-end architecture decoupling short-term and long-term representation learning in two separate streams in the first part of the architecture.
+ Comparison to deep and shallow baselines.

3) Suggestions for improvement

Add a LRCN baseline and discussion:
The benefits of decoupling short-term and long-term representation learning need to be assessed by comparing to the popular ""long-term recurrent convolutional network"" (LRCN) of Donahue et al (",0
"This paper takes a model based on that of Graves and retrofits it with a representation derived from the work of Plamondon. 
part of the goal of deep learning has been to avoid the use of hand-crafted features and have the network learn from raw feature representations, so this paper is somewhat against the grain. 

The paper relies on some qualitative examples as demonstration of the system, and doesn't seem to provide a strong motivation for there being any progress here. 
The paper does not provide true text-conditional handwriting synthesis as shown in Graves' original work. 

Be more consistent about your bibliography (e.g. variants of Plamondon's own name, use of ""et al."" in the bibliography etc.)",0
"This paper takes a model based on that of Graves and retrofits it with a representation derived from the work of Plamondon. 
part of the goal of deep learning has been to avoid the use of hand-crafted features and have the network learn from raw feature representations, so this paper is somewhat against the grain. 

The paper relies on some qualitative examples as demonstration of the system, and doesn't seem to provide a strong motivation for there being any progress here. 
The paper does not provide true text-conditional handwriting synthesis as shown in Graves' original work. 

Be more consistent about your bibliography (e.g. variants of Plamondon's own name, use of ""et al."" in the bibliography etc.)",0
"This paper proposed a quantitative metric for evaluating out-of-class novelty of samples from generative models. The authors evaluated the proposed metric on over 1000 models with different hyperparameters and performed human subject study on a subset of them.

The authors mentioned difficulties in human subject studies, but did not provide details of their own setting. An ""in-house"" annotation tool was used but it's unclear how many subjects were involved, who they are, and how many samples were presented to each subject. I'm worried about the diversity in the subjects because there may be too few subjects who are shown too many samples and/or are experts in this field.

This paper aims at proposing a general metric for novelty but the experiments only used one setting, namely generating Arabic digits and English letters. There is insufficient evidence to prove the generality of the proposed metric.

Moreover, defining English letters as ""novel"" compared to Arabic digits is questionable. What if the model generates Arabic or Indian letters? Can a human who has never seen Arabic handwriting tell it from random doodle? What makes English letters more ""novel"" than random doodle? In my opinion these questions are best answered through large scale human subject study on tasks that has clear real world meanings. For example, do you prefer painting A (generated) or B (painted by artist).",0
"This paper proposed a quantitative metric for evaluating out-of-class novelty of samples from generative models. The authors evaluated the proposed metric on over 1000 models with different hyperparameters and performed human subject study on a subset of them.

The authors mentioned difficulties in human subject studies, but did not provide details of their own setting. An ""in-house"" annotation tool was used but it's unclear how many subjects were involved, who they are, and how many samples were presented to each subject. I'm worried about the diversity in the subjects because there may be too few subjects who are shown too many samples and/or are experts in this field.

This paper aims at proposing a general metric for novelty but the experiments only used one setting, namely generating Arabic digits and English letters. There is insufficient evidence to prove the generality of the proposed metric.

Moreover, defining English letters as ""novel"" compared to Arabic digits is questionable. What if the model generates Arabic or Indian letters? Can a human who has never seen Arabic handwriting tell it from random doodle? What makes English letters more ""novel"" than random doodle? In my opinion these questions are best answered through large scale human subject study on tasks that has clear real world meanings. For example, do you prefer painting A (generated) or B (painted by artist).",0
"this work investigates a joint learning setup where tasks are stacked based on their complexity. to this end, experimental evaluation is done on pos tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. the end-to-end model improves over models trained solely on target tasks.

although the hypothesis of this work is an important one, the experimental evaluation lacks thoroughness:

first, a very simple multi-task learning baseline [1] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity.

second, since the test set of chunking is included in training data of dependency parsing, the results related to chunking with JMT_all are not informative. 

third, since the model does not guarantee well-formed dependency trees, thus, results in table 4 are not fair. 

minor issue:
- chunking is not a word-level task although the annotation is word-level. chunking is a structured prediction task where we would like to learn a structured annotation over a sequence [2].

[1]",0
"this work investigates a joint learning setup where tasks are stacked based on their complexity. to this end, experimental evaluation is done on pos tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. the end-to-end model improves over models trained solely on target tasks.

although the hypothesis of this work is an important one, the experimental evaluation lacks thoroughness:

first, a very simple multi-task learning baseline [1] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity.

second, since the test set of chunking is included in training data of dependency parsing, the results related to chunking with JMT_all are not informative. 

third, since the model does not guarantee well-formed dependency trees, thus, results in table 4 are not fair. 

minor issue:
- chunking is not a word-level task although the annotation is word-level. chunking is a structured prediction task where we would like to learn a structured annotation over a sequence [2].

[1]",0
"The paper investigates on better training strategies for the Neural GPU models as well as studies the limitations of the model.

Pros:
* Well written.
* Many investigations.
* Available source code.

Cons:
* Misleading title, there is no extension to the Neural GPU model, just to its training strategies.
* No comparisons to similar architectures (e.g. Grid LSTM, NTM, Adaptive Computation Time).
* More experiments on other tasks would be nice, it is only tested on some toy tasks.
* No positive results, only negative results. To really understand the negative results, it would be good to know what is missing to make it work. This has not been studied further.
* Some details remain unclear or missing, e.g. if gradient noise was used in all experiments, or the length of sequences e.g. in Figure 3.
* Misleading number of NTM computation steps. You write O(n) but it is actually variable.

After the results from the paper, the limitations still remain unclear because it is not clear exactly why the model fails. Despite showing some examples which make it fail, it was not studied in more detail why it failed for those examples, and how you could fix the problem.",0
"The paper investigates on better training strategies for the Neural GPU models as well as studies the limitations of the model.

Pros:
* Well written.
* Many investigations.
* Available source code.

Cons:
* Misleading title, there is no extension to the Neural GPU model, just to its training strategies.
* No comparisons to similar architectures (e.g. Grid LSTM, NTM, Adaptive Computation Time).
* More experiments on other tasks would be nice, it is only tested on some toy tasks.
* No positive results, only negative results. To really understand the negative results, it would be good to know what is missing to make it work. This has not been studied further.
* Some details remain unclear or missing, e.g. if gradient noise was used in all experiments, or the length of sequences e.g. in Figure 3.
* Misleading number of NTM computation steps. You write O(n) but it is actually variable.

After the results from the paper, the limitations still remain unclear because it is not clear exactly why the model fails. Despite showing some examples which make it fail, it was not studied in more detail why it failed for those examples, and how you could fix the problem.",0
"This paper proposes to reduce model size and evaluation time of deep CNN models on mobile devices by converting multiple layers into single layer and then retraining the converted model. The paper showed that the computation time can be reduced by 3x to 5x with only 0.4% accuracy loss on a specific model.

Reducing model sizes and speeding up model evaluation are important in many applications. I have several concerns:

1. There are many techniques that can reduce model sizes. For example, it has been shown by several groups that using the teacher-student approach, people can achieve the same and sometimes even better accuracy than the teacher (big model) using a much smaller model. However, this paper does not compare any one of them.
2. The technique proposed in this paper is limited in its applicability since it's designed specifically for the models discussed in the paper. 
3. Replacing several layers with single layer is a relatively standard procedure. For example, the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy.

BTW, the DNN low-rank approximation technique was first proposed in speech recognition. e.g., 

Xue, J., Li, J. and Gong, Y., 2013, August. Restructuring of deep neural network acoustic models with singular value decomposition. In INTERSPEECH (pp. 2365-2369).",0
"This paper proposes to reduce model size and evaluation time of deep CNN models on mobile devices by converting multiple layers into single layer and then retraining the converted model. The paper showed that the computation time can be reduced by 3x to 5x with only 0.4% accuracy loss on a specific model.

Reducing model sizes and speeding up model evaluation are important in many applications. I have several concerns:

1. There are many techniques that can reduce model sizes. For example, it has been shown by several groups that using the teacher-student approach, people can achieve the same and sometimes even better accuracy than the teacher (big model) using a much smaller model. However, this paper does not compare any one of them.
2. The technique proposed in this paper is limited in its applicability since it's designed specifically for the models discussed in the paper. 
3. Replacing several layers with single layer is a relatively standard procedure. For example, the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy.

BTW, the DNN low-rank approximation technique was first proposed in speech recognition. e.g., 

Xue, J., Li, J. and Gong, Y., 2013, August. Restructuring of deep neural network acoustic models with singular value decomposition. In INTERSPEECH (pp. 2365-2369).",0
"This work presents a novel 3D CNN architecture for climate event detection that combines an unsupervised auto-encoder reconstruction loss with YOLO like bounding box prediction. The approach is trained and evaluated on a large-scale, simulated climate dataset labeled by a costly heuristic approach called TECA. 

For the most part, the paper is nicely written (minor comments below) and addresses an important and well motivated problem. The authors provide sufficient model details to allow reproduction (although public code would be preferred). I find the experiments a bit unconvincing (see below) but appreciate the attention to model capacity (via number of parameter) when comparing the 2D and 3D model variants.

I am concerned that the evaluation may be insufficient to assess the effectiveness of this method. An IoU threshold of 0.1 allows for many rather poor detections to count as true positives. If the goal of this work is to count the number of such instances this is defensible, but for localization this seems overly loose.Furthermore, the 3D CNN architecture (which is one of the the core novelties of this work compared to past efforts) does not seem capable of producing variable sized boxes (as noted in the last paragraph of page 7), which I imagine results in poorer performance at higher IoU thresholds (as many of the weather events appear small). 
The experiments also feel inconclusive about the effect of temporal modeling and semi-supervision. The temporal component does not seem to matter in the supervised settings (2D 51.45 mAP - 3D 51.00 mAP) but improves somewhat in the semi-supervised case (2D 51.11 mAP - 3D 52.92 mAP). Whereas the additional unlabeled data seems to hurt in the 2D case but improve results for the 3D model. Could the authors provide confidence intervals for these numbers? I would like to see further discussion of these trends especially with respect to the effect of the loss weights (alpha, beta, and gamma). 

I also note that it is not clear if both the 2D and 3D models were trained for equivalent time periods (seems like no from last paragraph of page 7). Could a plot of training and validation accuracy for each model be presented for comparison? 

Finally, is there any baseline approach the authors could report or compare too? Without one, it is difficult to evaluate the performance of the approach with respect to the difficulty of the problem.

Preliminary Rating:
I think this is an interesting paper that is well motivated but feel the experiments as presented do not seem adequate to support any conclusive trends. I would like to see the mAP trends across a wider range of IoU values and further discussion of training procedure, loss weight settings, and reasons for lack of bounding box variability in the 3D model (as stated above). 

Clarification:
In the paper you say ""While climate models are run on a 3D grid, with the vertical dimension corresponding to 30 levels; we only consider surface quantities (i.e. 2D data) in this study."" Could you elaborate on what the surface quantities correspond to? Is it the highest cloud level?

Minor notes:
	Please provide years for Prabhat et al. references rather than a and b.
	Footnote in 4.2 could be inline text with similar space.
	4.3 second paragraph the word table is not capitalized like elsewhere.
	4.3 4th paragraph the word section is not capitalized like elsewhere.

Edit: I appreciate the authors responding to my questions but still feel the relatively poor localization performance at stricter IoU thresholds fails to justify the complexity of the approach. I encourage the authors to continue pursuing this line of research.",0
"This work presents a novel 3D CNN architecture for climate event detection that combines an unsupervised auto-encoder reconstruction loss with YOLO like bounding box prediction. The approach is trained and evaluated on a large-scale, simulated climate dataset labeled by a costly heuristic approach called TECA. 

For the most part, the paper is nicely written (minor comments below) and addresses an important and well motivated problem. The authors provide sufficient model details to allow reproduction (although public code would be preferred). I find the experiments a bit unconvincing (see below) but appreciate the attention to model capacity (via number of parameter) when comparing the 2D and 3D model variants.

I am concerned that the evaluation may be insufficient to assess the effectiveness of this method. An IoU threshold of 0.1 allows for many rather poor detections to count as true positives. If the goal of this work is to count the number of such instances this is defensible, but for localization this seems overly loose.Furthermore, the 3D CNN architecture (which is one of the the core novelties of this work compared to past efforts) does not seem capable of producing variable sized boxes (as noted in the last paragraph of page 7), which I imagine results in poorer performance at higher IoU thresholds (as many of the weather events appear small). 
The experiments also feel inconclusive about the effect of temporal modeling and semi-supervision. The temporal component does not seem to matter in the supervised settings (2D 51.45 mAP - 3D 51.00 mAP) but improves somewhat in the semi-supervised case (2D 51.11 mAP - 3D 52.92 mAP). Whereas the additional unlabeled data seems to hurt in the 2D case but improve results for the 3D model. Could the authors provide confidence intervals for these numbers? I would like to see further discussion of these trends especially with respect to the effect of the loss weights (alpha, beta, and gamma). 

I also note that it is not clear if both the 2D and 3D models were trained for equivalent time periods (seems like no from last paragraph of page 7). Could a plot of training and validation accuracy for each model be presented for comparison? 

Finally, is there any baseline approach the authors could report or compare too? Without one, it is difficult to evaluate the performance of the approach with respect to the difficulty of the problem.

Preliminary Rating:
I think this is an interesting paper that is well motivated but feel the experiments as presented do not seem adequate to support any conclusive trends. I would like to see the mAP trends across a wider range of IoU values and further discussion of training procedure, loss weight settings, and reasons for lack of bounding box variability in the 3D model (as stated above). 

Clarification:
In the paper you say ""While climate models are run on a 3D grid, with the vertical dimension corresponding to 30 levels; we only consider surface quantities (i.e. 2D data) in this study."" Could you elaborate on what the surface quantities correspond to? Is it the highest cloud level?

Minor notes:
	Please provide years for Prabhat et al. references rather than a and b.
	Footnote in 4.2 could be inline text with similar space.
	4.3 second paragraph the word table is not capitalized like elsewhere.
	4.3 4th paragraph the word section is not capitalized like elsewhere.

Edit: I appreciate the authors responding to my questions but still feel the relatively poor localization performance at stricter IoU thresholds fails to justify the complexity of the approach. I encourage the authors to continue pursuing this line of research.",0
"The authors address the problem of modeling temporally-changing signal on a graph, where the signal at one node changes as a function of the inputs and the hidden states of its neighborhood, the size of which is a hyperparameter. The approach follows closely that of Shi et al. 2015, but it is generalized to arbitrary graph structures rather than a fixed grid by using graph convolutions of Defferrard et al. 2016. This is not a strict generalization because the graph formulation treats all edges equally, while the conv kernels in Shi et al. have a built in directionality. The authors show results on a moving MNIST and on the Penn Tree Bank Language Modeling task.

The paper, model and experiments are decent but I have some concerns:

1. The proposed model is not exceptionally novel from a technical perspective. I usually don't mind if this is the case provided that the authors make up for the deficiency with thorough experimental evaluation, clear write up, and interesting insights into the pros/cons of the approach with respect to previous models. In this case I lean towards this not being the case.

2. The experiment results section is rather terse and light on interpretation. I'm not fully up to date on the latest of Penn Tree Bank language modeling results but I do know that it is a hotly contested and well-known dataset. I am surprised to see a comparison only to Zaremba et al 2014 where I would expect to see multiple other results.

3. The writing is not very clear and the authors don't make sufficiently strong attempt to compare the models or provide insight or comparisons into why the proposed model works better. In particular, unless I'm mistaken the word probabilities are a function of the neighborhood in the graph. What is the width of this graph? For example, suppose I sample a word in one part of the graph, doesn't this information have to propagate to the other parts of the graph along the edges? Also, it's not clear to me how the model can achieve reasonable results on moving MNIST when it cannot distinguish the direction of the moving edges. The authors state this but do not provide satisfying insight into how this can work. How does a pixel know that it should turn on in the next frame? I wish the authors thought about this more and presented it more clearly.

In summary, the paper has somewhat weak technical contribution, the experiments section is not very thorough, and the insights are sparse.",0
"The authors address the problem of modeling temporally-changing signal on a graph, where the signal at one node changes as a function of the inputs and the hidden states of its neighborhood, the size of which is a hyperparameter. The approach follows closely that of Shi et al. 2015, but it is generalized to arbitrary graph structures rather than a fixed grid by using graph convolutions of Defferrard et al. 2016. This is not a strict generalization because the graph formulation treats all edges equally, while the conv kernels in Shi et al. have a built in directionality. The authors show results on a moving MNIST and on the Penn Tree Bank Language Modeling task.

The paper, model and experiments are decent but I have some concerns:

1. The proposed model is not exceptionally novel from a technical perspective. I usually don't mind if this is the case provided that the authors make up for the deficiency with thorough experimental evaluation, clear write up, and interesting insights into the pros/cons of the approach with respect to previous models. In this case I lean towards this not being the case.

2. The experiment results section is rather terse and light on interpretation. I'm not fully up to date on the latest of Penn Tree Bank language modeling results but I do know that it is a hotly contested and well-known dataset. I am surprised to see a comparison only to Zaremba et al 2014 where I would expect to see multiple other results.

3. The writing is not very clear and the authors don't make sufficiently strong attempt to compare the models or provide insight or comparisons into why the proposed model works better. In particular, unless I'm mistaken the word probabilities are a function of the neighborhood in the graph. What is the width of this graph? For example, suppose I sample a word in one part of the graph, doesn't this information have to propagate to the other parts of the graph along the edges? Also, it's not clear to me how the model can achieve reasonable results on moving MNIST when it cannot distinguish the direction of the moving edges. The authors state this but do not provide satisfying insight into how this can work. How does a pixel know that it should turn on in the next frame? I wish the authors thought about this more and presented it more clearly.

In summary, the paper has somewhat weak technical contribution, the experiments section is not very thorough, and the insights are sparse.",0
"SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard.

THOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of ""learning end-to-end "" an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an ""end-to-end trained"" system.

The paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models.

Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (",0
"SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard.

THOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of ""learning end-to-end "" an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an ""end-to-end trained"" system.

The paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models.

Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (",0
"(paper summary) The authors introduce the notion of “sample importance”, meant to measure the influence of a particular training example on the training of a deep neural network. This quantity is closely related to the squared L2 norm of the gradient, where the summation is performed over (i) parameters of a given layer or (ii) across all parameters. Summing this quantity across time gives the “overall importance”, used to tease apart easy from hard examples. From this quantity, the authors illustrate the impact of [easy,hard] example during training, and their impact on layer depth.

(detailed review)
I have several objections to this paper. First and foremost, I am not convinced of the “sample importance” as a meaningful metric. As previously mentioned, the magnitude of gradients will change significantly during learning, and I am not sure what conclusions one can draw from \sum_t g_i^t vs \sum_t g_j^t. For example, gradients tend to have higher norms early in training than at convergence, in which case weighting each gradient equally seems problematic. I tried illustrating the above with a small thought experiment during the question period: “if” the learning rate were too high, training may not even converge in which case sample importance would be ill-defined.  Having a measure which depends on the learning rate seems problematic to me, as does the use of the L2 norm. The “input Fisher” norm, \mathbb{E} \frac{\partial \log p} {\partial x} (for a given time-step) may be better suited, as it speaks directly to the sensitivity of the classifier to the input x (and is insensitive to changes in the mean gradient norm). But again summing Fisher norms across time may not be meaningful.

The experimental analysis also seems problematic. The authors claim from Fig. 2 that output layers are primarily learnt in the early stage of training. However, this is definitely not the case for CIFAR-10 and is debatable for MNIST: sample importance remains high for all layers during training, despite a small spike early on the output layer. Fig 2. (lower, middle) and Fig. 6 also seems to highlight an issue with the SI measure: the SI is dominated by the input layer which has the most parameters, and can thus more readily impact the gradient norm. Different model architectures may have yielded different conclusions. Had the authors managed to use the SI to craft a better curriculum, this would have given significant weight to the measure. Unfortunately, these results are negative.

PROS:
+ extensive experiments

CONS:
- sample importance is a heuristic, not entirely well justified
- SI yields limited insight into training of neural nets
- SI does not inform curriculum learning",0
"(paper summary) The authors introduce the notion of “sample importance”, meant to measure the influence of a particular training example on the training of a deep neural network. This quantity is closely related to the squared L2 norm of the gradient, where the summation is performed over (i) parameters of a given layer or (ii) across all parameters. Summing this quantity across time gives the “overall importance”, used to tease apart easy from hard examples. From this quantity, the authors illustrate the impact of [easy,hard] example during training, and their impact on layer depth.

(detailed review)
I have several objections to this paper. First and foremost, I am not convinced of the “sample importance” as a meaningful metric. As previously mentioned, the magnitude of gradients will change significantly during learning, and I am not sure what conclusions one can draw from \sum_t g_i^t vs \sum_t g_j^t. For example, gradients tend to have higher norms early in training than at convergence, in which case weighting each gradient equally seems problematic. I tried illustrating the above with a small thought experiment during the question period: “if” the learning rate were too high, training may not even converge in which case sample importance would be ill-defined.  Having a measure which depends on the learning rate seems problematic to me, as does the use of the L2 norm. The “input Fisher” norm, \mathbb{E} \frac{\partial \log p} {\partial x} (for a given time-step) may be better suited, as it speaks directly to the sensitivity of the classifier to the input x (and is insensitive to changes in the mean gradient norm). But again summing Fisher norms across time may not be meaningful.

The experimental analysis also seems problematic. The authors claim from Fig. 2 that output layers are primarily learnt in the early stage of training. However, this is definitely not the case for CIFAR-10 and is debatable for MNIST: sample importance remains high for all layers during training, despite a small spike early on the output layer. Fig 2. (lower, middle) and Fig. 6 also seems to highlight an issue with the SI measure: the SI is dominated by the input layer which has the most parameters, and can thus more readily impact the gradient norm. Different model architectures may have yielded different conclusions. Had the authors managed to use the SI to craft a better curriculum, this would have given significant weight to the measure. Unfortunately, these results are negative.

PROS:
+ extensive experiments

CONS:
- sample importance is a heuristic, not entirely well justified
- SI yields limited insight into training of neural nets
- SI does not inform curriculum learning",0
"This paper is refreshing and elegant in its handling of ""over-sampling"" in VAE. Problem is that good reconstruction requires more nodes in the latent layers of the VAE. Not all of them can or should be sampled from at the ""creative"" regime of the VAE. Which ones to choose? The paper offers and sensible solution. Problem is that real-life data-sets like CIFAR have not being tried, so the reader is hard-pressed to choose between many other, just as natural, solutions. One can e.g. run in parallel a classifier and let it choose the best epitome, in the spirit of spatial transformers, ACE, reference [1]. The list can go on. We hope that the paper finds its way to the conference because it addresses an important problem in an elegant way, and papers like this are few and far between!

On a secondary note, regarding terminology: Pls avoid using ""the KL term"" as in section 2.1, there are so many ""KL terms"" related to VAE-s, it ultimately gets out of control. ""Generative error"" is a more descriptive term, because minimizing it is indispensable for the generative qualities of the net. The variational error for example is also a ""KL term"" (equation (3.4) in reference [1]), as is the upper bound commonly used in VAE-s (your formula (5) and its equivalent - the KL expression as in formula (3.8) in reference [1]). The latter expression is frequently used and is handy for, say, importance sampling, as in reference [2].

[1]",0
"This paper is refreshing and elegant in its handling of ""over-sampling"" in VAE. Problem is that good reconstruction requires more nodes in the latent layers of the VAE. Not all of them can or should be sampled from at the ""creative"" regime of the VAE. Which ones to choose? The paper offers and sensible solution. Problem is that real-life data-sets like CIFAR have not being tried, so the reader is hard-pressed to choose between many other, just as natural, solutions. One can e.g. run in parallel a classifier and let it choose the best epitome, in the spirit of spatial transformers, ACE, reference [1]. The list can go on. We hope that the paper finds its way to the conference because it addresses an important problem in an elegant way, and papers like this are few and far between!

On a secondary note, regarding terminology: Pls avoid using ""the KL term"" as in section 2.1, there are so many ""KL terms"" related to VAE-s, it ultimately gets out of control. ""Generative error"" is a more descriptive term, because minimizing it is indispensable for the generative qualities of the net. The variational error for example is also a ""KL term"" (equation (3.4) in reference [1]), as is the upper bound commonly used in VAE-s (your formula (5) and its equivalent - the KL expression as in formula (3.8) in reference [1]). The latter expression is frequently used and is handy for, say, importance sampling, as in reference [2].

[1]",0
"This paper introduces a variant of the semi-supervised variational auto-encoder (VAE) framework. The authors present a way of introducing structure (observed variables) inside the recognition network.

I find that the presentation of the inference with auxiliary variables could be avoided, as it actually makes the presentation unnecessarily complicated. Specifically, the expressions with auxiliary variables are helpful for devising a unified implementation, but modeling-wise one can get the same model without these auxiliary variables and recover a minimal extension of VAE where part of the generating space is actually observed. The observed variables mean that the posterior needs to also condition on those, so as to incorporate the information they convey. The way this is done in this paper is actually not very different from Kingma et al. 2014, and I am surprised that the experiments show a large deviation in these two methods' results. Given the similarity of the models, it'd be useful if the authors could give a possible explanation on the superiority of their method compared to Kingma et al. 2014. By the way, I was wondering if the experimental setup is the same as in Kingma et al. 2014 for the results of Fig. 5 (bottom) - the authors mention that they use CNNs for feature extraction but from the paper it's not clear if Kingma et al. do the same. 

On a related note, I was wondering the same for the comparison with Jampani et al. 2015. In particular, is that model also using the same rate of supervision for a fair comparison?

The experiment in section 4.3 is interesting and demonstrates a useful property of the approach.

The discussion of the supervision rate (and the pre-review answer) is helpful in giving some insight about what is a successful training protocol to use in semi-supervised learning.

Overall, the paper is interesting but the title and introduction made me expect something more from it. From the title I expected a method for interpreting general deep generative models, instead the described approach was about a semi-supervised variant of VAE - naturally including labelled examples disentangles the latent space, but this is a general property of any semi-supervised probabilistic model and not unique to the approach described here. Moreover, from the intro I expected to see a more general approximation scheme for the variational posterior (similar to Ranganath et al. 2015  which trully allows very flexible distributions), however this is not the case here.

Given the above, the contributions of this paper are in defining a slight variant of the semi-supervised VAE, and (perhaps more importantly) formulating it in a way that is amendable to easier automation in terms of software. But methodologically there is not much contribution to the current literature. The authors mention that they plan to extend the framework in the probabilistic programming setting. It seems indeed that this would be a very promising and useful extension. 

Minor note: three of Kingma's papers are all cited in the main text as Kingma et al. 2014, causing confusion. I suggest using Kingma et al. 2014a etc.",0
"This paper introduces a variant of the semi-supervised variational auto-encoder (VAE) framework. The authors present a way of introducing structure (observed variables) inside the recognition network.

I find that the presentation of the inference with auxiliary variables could be avoided, as it actually makes the presentation unnecessarily complicated. Specifically, the expressions with auxiliary variables are helpful for devising a unified implementation, but modeling-wise one can get the same model without these auxiliary variables and recover a minimal extension of VAE where part of the generating space is actually observed. The observed variables mean that the posterior needs to also condition on those, so as to incorporate the information they convey. The way this is done in this paper is actually not very different from Kingma et al. 2014, and I am surprised that the experiments show a large deviation in these two methods' results. Given the similarity of the models, it'd be useful if the authors could give a possible explanation on the superiority of their method compared to Kingma et al. 2014. By the way, I was wondering if the experimental setup is the same as in Kingma et al. 2014 for the results of Fig. 5 (bottom) - the authors mention that they use CNNs for feature extraction but from the paper it's not clear if Kingma et al. do the same. 

On a related note, I was wondering the same for the comparison with Jampani et al. 2015. In particular, is that model also using the same rate of supervision for a fair comparison?

The experiment in section 4.3 is interesting and demonstrates a useful property of the approach.

The discussion of the supervision rate (and the pre-review answer) is helpful in giving some insight about what is a successful training protocol to use in semi-supervised learning.

Overall, the paper is interesting but the title and introduction made me expect something more from it. From the title I expected a method for interpreting general deep generative models, instead the described approach was about a semi-supervised variant of VAE - naturally including labelled examples disentangles the latent space, but this is a general property of any semi-supervised probabilistic model and not unique to the approach described here. Moreover, from the intro I expected to see a more general approximation scheme for the variational posterior (similar to Ranganath et al. 2015  which trully allows very flexible distributions), however this is not the case here.

Given the above, the contributions of this paper are in defining a slight variant of the semi-supervised VAE, and (perhaps more importantly) formulating it in a way that is amendable to easier automation in terms of software. But methodologically there is not much contribution to the current literature. The authors mention that they plan to extend the framework in the probabilistic programming setting. It seems indeed that this would be a very promising and useful extension. 

Minor note: three of Kingma's papers are all cited in the main text as Kingma et al. 2014, causing confusion. I suggest using Kingma et al. 2014a etc.",0
"[Summary]
This paper proposes a new way for knowledge base completion which highlights: 1) adopting an implicit shared memory, which makes no assumption about its structure and is completely learned during training; 2) modeling a multi-step search process that can decide when to terminate.

The experimental results on WN18 and FB15k seem pretty good. The authors also perform an analysis on a shortest path synthetic task, and demonstrate that this model is better than standard seq2seq.

The paper is well-written and it is easy to follow.

[Major comments]
I actually do like the idea and am also impressed that this model can work well.
The main concern is that this paper presents too little analysis about how it works and whether it is sensitive to the hyper-parameters, besides that only reporting a final model on WN18 and FB15k.

One key hyper-parameter I believe is the size of shared memory (using 64 for the experiments). I don’t think that this number should be fixed for all tasks, at least it should depend on the KB scale. Could you verify this in your experiments? Would it be even possible to make a memory structure with dynamic size?

The RL setting (stochastic search process) is also one highlight of the paper, but could you demonstrate that how much it does really help? I think it is necessary to compare to the following: remove the termination gate and fix the number of inference steps and see how well the model does? Also show how the performance varies on # of steps?

I appreciate your attempts on the shortest path synthetic task. However, I think it would be much better if you can demonstrate that under a real KB setting. You can still perform the shortest path analysis, but using KB  (e.g., Freebase) entities and relations.

[Minor comments]
I am afraid that the output gate illustrated in Figure 1 is a bit confusing. There should be only one output, depending on when the search process is terminated.",0
"[Summary]
This paper proposes a new way for knowledge base completion which highlights: 1) adopting an implicit shared memory, which makes no assumption about its structure and is completely learned during training; 2) modeling a multi-step search process that can decide when to terminate.

The experimental results on WN18 and FB15k seem pretty good. The authors also perform an analysis on a shortest path synthetic task, and demonstrate that this model is better than standard seq2seq.

The paper is well-written and it is easy to follow.

[Major comments]
I actually do like the idea and am also impressed that this model can work well.
The main concern is that this paper presents too little analysis about how it works and whether it is sensitive to the hyper-parameters, besides that only reporting a final model on WN18 and FB15k.

One key hyper-parameter I believe is the size of shared memory (using 64 for the experiments). I don’t think that this number should be fixed for all tasks, at least it should depend on the KB scale. Could you verify this in your experiments? Would it be even possible to make a memory structure with dynamic size?

The RL setting (stochastic search process) is also one highlight of the paper, but could you demonstrate that how much it does really help? I think it is necessary to compare to the following: remove the termination gate and fix the number of inference steps and see how well the model does? Also show how the performance varies on # of steps?

I appreciate your attempts on the shortest path synthetic task. However, I think it would be much better if you can demonstrate that under a real KB setting. You can still perform the shortest path analysis, but using KB  (e.g., Freebase) entities and relations.

[Minor comments]
I am afraid that the output gate illustrated in Figure 1 is a bit confusing. There should be only one output, depending on when the search process is terminated.",0
"This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also ""fintunes"" on test examples with active search to achieve better performance.

The proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.

However, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.

Further more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms.",0
"This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also ""fintunes"" on test examples with active search to achieve better performance.

The proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.

However, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.

Further more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms.",0
"This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. 

The approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.

You are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.

The submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.

Prior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.

What do you mean by transition ""scalars""?

I do not repeat further comments here, which were already given in the pre-review period.

Minor comments:
 - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly
   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)
 - Sec. 2.3: Bayse -> Bayes
 - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).
 - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)
 - Sec. 2.4, first line: threholding -> thresholding (spell check..)
 - Figure 4: mention the corpus used here - dev?",0
"This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. 

The approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.

You are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.

The submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.

Prior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.

What do you mean by transition ""scalars""?

I do not repeat further comments here, which were already given in the pre-review period.

Minor comments:
 - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly
   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)
 - Sec. 2.3: Bayse -> Bayes
 - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).
 - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)
 - Sec. 2.4, first line: threholding -> thresholding (spell check..)
 - Figure 4: mention the corpus used here - dev?",0
"The authors propose a RNN-method for time-series classification with missing values, that can make use of potential information in missing values. It is based on a simple linear imputation of missing values with learnable parameters. Furthermore, time-intervals between missing values are computed and used to scale the RNN computation downstream. The authors demonstrate that their method outperforms reasonable baselines on (small to mid-sized) real world datasets. The paper is clearly written.
IMO the authors propose a reasonable approach for dealing with missing values for their intended application domain, where data is not abundant and requires smallish models. I’m somewhat sceptical if the benefits would carry over to big datasets, where more general, less handcrafted multi-layer RNNs are an option.",0
"The authors propose a RNN-method for time-series classification with missing values, that can make use of potential information in missing values. It is based on a simple linear imputation of missing values with learnable parameters. Furthermore, time-intervals between missing values are computed and used to scale the RNN computation downstream. The authors demonstrate that their method outperforms reasonable baselines on (small to mid-sized) real world datasets. The paper is clearly written.
IMO the authors propose a reasonable approach for dealing with missing values for their intended application domain, where data is not abundant and requires smallish models. I’m somewhat sceptical if the benefits would carry over to big datasets, where more general, less handcrafted multi-layer RNNs are an option.",0
"This paper proposed the group sparse auto-encoder for feature extraction. The author then stack the group sparse auto-encoders on top of CNNs to extract better question sentence representation for QA tasks. 

Pros: 
- group-sparse auto-encoder seems new to me.
- extensive experiments on QA tasks. 

Cons:
- The idea is somewhat incremental.
- Writing need to be improved. 
- Lack of ablation studies to show the effectiveness of the proposed approach. 

Moreover, I am not convinced by the author's answer regarding the baseline. A separate training stages of CNN+SGL for comparison is fine. The purpose is to validate and analyze why the proposed SGA is preferred rather than group lasso, e.g. joint training could improve, or the proposed group-sparse regularization outperforms l_21 norm, etc. However, we can't see it from the current experiments.",0
"This paper proposed the group sparse auto-encoder for feature extraction. The author then stack the group sparse auto-encoders on top of CNNs to extract better question sentence representation for QA tasks. 

Pros: 
- group-sparse auto-encoder seems new to me.
- extensive experiments on QA tasks. 

Cons:
- The idea is somewhat incremental.
- Writing need to be improved. 
- Lack of ablation studies to show the effectiveness of the proposed approach. 

Moreover, I am not convinced by the author's answer regarding the baseline. A separate training stages of CNN+SGL for comparison is fine. The purpose is to validate and analyze why the proposed SGA is preferred rather than group lasso, e.g. joint training could improve, or the proposed group-sparse regularization outperforms l_21 norm, etc. However, we can't see it from the current experiments.",0
"Paper Summary: 
This paper presents a new comprehension dataset called NewsQA dataset, containing 100,000 question-answer pairs from over 10,000 news articles from CNN. The dataset is collected through a four-stage process -- article filtering, question collection, answer collection and answer validation. Examples from the dataset are divided into different types based on answer types and reasoning required to answer questions. Human and machine performances on NewsQA are reported and compared with SQuAD.

Paper Strengths: 
-- I agree that models can benefit from diverse set of datasets. This dataset is collected from news articles, hence might pose different sets of problems from current popular datasets such as SQuAD.
-- The proposed dataset is sufficiently large for data hungry deep learning models to train. 
-- The inclusion of questions with null answers is a nice property to have.
-- A good amount of thought has gone into formulating the four-stage data collection process.
-- The proposed BARB model is performing as good as a published state-of-the-art model, while being much faster.    

Paper Weaknesses: 
-- Human evaluation is weak. Two near-native English speakers' performance on 100 examples each can hardly be a representative of the complete dataset. Also, what is the model performance on these 200 examples?
-- Not that it is necessary for this paper, but to clearly demonstrate that this dataset is harder than SQuAD, the authors should either calculate the human performance the same way as SQuAD or calculate human performances on both NewsQA and SQuAD in some other consistent manner on large enough subsets which are good representatives of the complete datasets. Dataset from other communities such as VQA dataset (Antol et al., ICCV 2015) also use the same method as SQuAD to compute human performance. 
-- Section 3.5 says that 86% of questions have answers agreed upon by atleast 2 workers. Why is this number inconsistent with the 4.5% of questions which have answers without agreement after validation (last line in Section 4.1)?
-- Is the same article shown to multiple Questioners? If yes, is it ensured that the Questioners asking questions about the same article are not asking the same/similar questions?
-- Authors mention that they keep the same hyperparameters as SQuAD. What are the accuracies if the hyperparameters are tuned using a validation set from NewsQA?
-- 500 examples which are labeled for reasoning types do not seem enough to represent the complete dataset. Also, what is the model performance on these 500 examples?
-- Which model's performance has been shown in Figure 1?
-- Are the two ""students"" graduate/undergraduate students or researchers?
-- Test set seems to be very small.
-- Suggestion: Answer validation step is nice, but maybe the dataset can be released in 2 versions -- one with all the answers collected in 3rd stage (without the validation step), and one in the current format with the validation step. 

Preliminary Evaluation: 
The proposed dataset is a large scale machine comprehension dataset collected from news articles, which in my suggestion, is diverse enough from existing datasets that state-of-the-art models can definitely benefit from it. With a better human evaluation, I think this paper will make a good poster.",0
"Paper Summary: 
This paper presents a new comprehension dataset called NewsQA dataset, containing 100,000 question-answer pairs from over 10,000 news articles from CNN. The dataset is collected through a four-stage process -- article filtering, question collection, answer collection and answer validation. Examples from the dataset are divided into different types based on answer types and reasoning required to answer questions. Human and machine performances on NewsQA are reported and compared with SQuAD.

Paper Strengths: 
-- I agree that models can benefit from diverse set of datasets. This dataset is collected from news articles, hence might pose different sets of problems from current popular datasets such as SQuAD.
-- The proposed dataset is sufficiently large for data hungry deep learning models to train. 
-- The inclusion of questions with null answers is a nice property to have.
-- A good amount of thought has gone into formulating the four-stage data collection process.
-- The proposed BARB model is performing as good as a published state-of-the-art model, while being much faster.    

Paper Weaknesses: 
-- Human evaluation is weak. Two near-native English speakers' performance on 100 examples each can hardly be a representative of the complete dataset. Also, what is the model performance on these 200 examples?
-- Not that it is necessary for this paper, but to clearly demonstrate that this dataset is harder than SQuAD, the authors should either calculate the human performance the same way as SQuAD or calculate human performances on both NewsQA and SQuAD in some other consistent manner on large enough subsets which are good representatives of the complete datasets. Dataset from other communities such as VQA dataset (Antol et al., ICCV 2015) also use the same method as SQuAD to compute human performance. 
-- Section 3.5 says that 86% of questions have answers agreed upon by atleast 2 workers. Why is this number inconsistent with the 4.5% of questions which have answers without agreement after validation (last line in Section 4.1)?
-- Is the same article shown to multiple Questioners? If yes, is it ensured that the Questioners asking questions about the same article are not asking the same/similar questions?
-- Authors mention that they keep the same hyperparameters as SQuAD. What are the accuracies if the hyperparameters are tuned using a validation set from NewsQA?
-- 500 examples which are labeled for reasoning types do not seem enough to represent the complete dataset. Also, what is the model performance on these 500 examples?
-- Which model's performance has been shown in Figure 1?
-- Are the two ""students"" graduate/undergraduate students or researchers?
-- Test set seems to be very small.
-- Suggestion: Answer validation step is nice, but maybe the dataset can be released in 2 versions -- one with all the answers collected in 3rd stage (without the validation step), and one in the current format with the validation step. 

Preliminary Evaluation: 
The proposed dataset is a large scale machine comprehension dataset collected from news articles, which in my suggestion, is diverse enough from existing datasets that state-of-the-art models can definitely benefit from it. With a better human evaluation, I think this paper will make a good poster.",0
"SUMMARY.

The paper proposes a machine reading approach for cloze-style question answering.
The proposed system first encodes the query and the document using a bidirectional gru. These two representations are combined together using a Gated Attention (GA).
GA calculates the compatibility of each word in the document and the query as a probability distribution.
For each word in the document a gate is calculated weighting the query representation according to the word compatibility.
Ultimately, the gate is applied to the gru-encoded document word.
The resulting word vectors are re-encoded with a bidirectional GRU.
This process is performed for multiple hops. After k hops, the probability of a word to be part of the answer is calculated by a log-linear model that take as input the last word representations, and the concatenation of the last query representation before and after the cloze token.
The probability of a candidate being the answer to the question is given by a linear combination of the single word probabilities.

The proposed model is tested on 4 different dataset. 
The authors shown that the proposed model works well (state-of-the-art performance) for 3 out of 4 benchmarks.


----------

OVERALL JUDGMENT
The main contribution of the paper is the gated attention mechanism, that in my opinion, is a simple and interesting idea.
The paper is well thought, and the ablation study on the benefits given by the gated attention are convincing.
The GA reader as whole model outperforms previous state-of-the-art models on 3 benchmarks and seems very promising also on the CBT dataset.
I would have liked to see some discussion on why the model works less well on the CBT dataset, though.


----------

DETAILED COMMENTS

minor. In the introduction, Weston et al., 2014 do not use any attention mechanism.",0
"SUMMARY.

The paper proposes a machine reading approach for cloze-style question answering.
The proposed system first encodes the query and the document using a bidirectional gru. These two representations are combined together using a Gated Attention (GA).
GA calculates the compatibility of each word in the document and the query as a probability distribution.
For each word in the document a gate is calculated weighting the query representation according to the word compatibility.
Ultimately, the gate is applied to the gru-encoded document word.
The resulting word vectors are re-encoded with a bidirectional GRU.
This process is performed for multiple hops. After k hops, the probability of a word to be part of the answer is calculated by a log-linear model that take as input the last word representations, and the concatenation of the last query representation before and after the cloze token.
The probability of a candidate being the answer to the question is given by a linear combination of the single word probabilities.

The proposed model is tested on 4 different dataset. 
The authors shown that the proposed model works well (state-of-the-art performance) for 3 out of 4 benchmarks.


----------

OVERALL JUDGMENT
The main contribution of the paper is the gated attention mechanism, that in my opinion, is a simple and interesting idea.
The paper is well thought, and the ablation study on the benefits given by the gated attention are convincing.
The GA reader as whole model outperforms previous state-of-the-art models on 3 benchmarks and seems very promising also on the CBT dataset.
I would have liked to see some discussion on why the model works less well on the CBT dataset, though.


----------

DETAILED COMMENTS

minor. In the introduction, Weston et al., 2014 do not use any attention mechanism.",0
"While the overall direction is promising, there are several serious issues with the paper which affect the novelty and validity of the results:

1. Incorrect claims about related work affecting novelty:

  - This work is not the first to explore a deep learning approach to automatic code completion: “Toward Deep Learning Software Repositories”, MSR’15 also uses deep learning for code completion, and is not cited.

  - “Code Completion with Statistical Language Models”, PLDI’14 is cited incorrectly -- it also does code completion with recurrent neural networks.

  - PHOG is independent of JavaScript -- it does representation learning and has been applied to other languages (e.g., Python, see OOPSLA’16 below). 

  - This submission is not the only one that “can automatically extract features”. Some high-precision (cited) baselines do it.

  - “Structured generative models of natural source code” is an incorrect citation. It is from ICML’14 and has more authors. It is also a log-linear model and conditions on more context than claimed in this submission.


2. Uses a non-comparable prediction task for non-terminal symbols: The type of prediction made here is simpler than the one used in PHOG and state-of-the-art (see OOPSLA’16 paper below) and thus the claimed 11 point improvement is not substantiated. In particular, in JavaScript there are 44 types of nodes. However, a PHOG and OOPSLA’16 predictions considers not only these 44 types, but also whether there are right siblings and children of a node. This is necessary for predicting tree fragments instead of a sequence of nodes. It however makes the prediction harder than the one considered here (it leads to 150+ labels, a >3x increase).


3. Not comparing to state-of-the-art: the state-of-the-art however is not the basic PHOG cited here, but “Probabilistic Model for Code with Decision Trees”, (OOPSLA 2016) which appeared before the submission deadline for ICLR’17:",0
"While the overall direction is promising, there are several serious issues with the paper which affect the novelty and validity of the results:

1. Incorrect claims about related work affecting novelty:

  - This work is not the first to explore a deep learning approach to automatic code completion: “Toward Deep Learning Software Repositories”, MSR’15 also uses deep learning for code completion, and is not cited.

  - “Code Completion with Statistical Language Models”, PLDI’14 is cited incorrectly -- it also does code completion with recurrent neural networks.

  - PHOG is independent of JavaScript -- it does representation learning and has been applied to other languages (e.g., Python, see OOPSLA’16 below). 

  - This submission is not the only one that “can automatically extract features”. Some high-precision (cited) baselines do it.

  - “Structured generative models of natural source code” is an incorrect citation. It is from ICML’14 and has more authors. It is also a log-linear model and conditions on more context than claimed in this submission.


2. Uses a non-comparable prediction task for non-terminal symbols: The type of prediction made here is simpler than the one used in PHOG and state-of-the-art (see OOPSLA’16 paper below) and thus the claimed 11 point improvement is not substantiated. In particular, in JavaScript there are 44 types of nodes. However, a PHOG and OOPSLA’16 predictions considers not only these 44 types, but also whether there are right siblings and children of a node. This is necessary for predicting tree fragments instead of a sequence of nodes. It however makes the prediction harder than the one considered here (it leads to 150+ labels, a >3x increase).


3. Not comparing to state-of-the-art: the state-of-the-art however is not the basic PHOG cited here, but “Probabilistic Model for Code with Decision Trees”, (OOPSLA 2016) which appeared before the submission deadline for ICLR’17:",0
"This paper proposes a model for the task of argumentation mining (labeling the set of relationships between statements expressed as sentence-sized spans in a short text). The model combines a pointer network component that identifies links between statements and a classifier that predicts the roles of these statements. The resulting model works well: It outperforms strong baselines, even on datasets with fewer than 100 training examples.

I don't see any major technical issues with this paper, and the results are strong. I am concerned, though, that the paper doesn't make a substantial novel contribution to representation learning. It focuses on ways to adapt reasonably mature techniques to a novel NLP problem. I think that one of the ACL conferences would be a better fit for this work.

The choice of a pointer network for this problem seems reasonable, though (as noted by other commenters) the paper does not make any substantial comparison with other possible ways of producing trees. The paper does a solid job at breaking down the results quantitatively, but I would appreciate some examples of model output and some qualitative error analysis.

Detail notes: 

- Figure 2 appears to have an error. You report that the decoder produces a distribution over input indices only, but you show an example of the network pointing to an output index in one case.
- I don't think ""Wei12"" is a name.",0
"This paper proposes a model for the task of argumentation mining (labeling the set of relationships between statements expressed as sentence-sized spans in a short text). The model combines a pointer network component that identifies links between statements and a classifier that predicts the roles of these statements. The resulting model works well: It outperforms strong baselines, even on datasets with fewer than 100 training examples.

I don't see any major technical issues with this paper, and the results are strong. I am concerned, though, that the paper doesn't make a substantial novel contribution to representation learning. It focuses on ways to adapt reasonably mature techniques to a novel NLP problem. I think that one of the ACL conferences would be a better fit for this work.

The choice of a pointer network for this problem seems reasonable, though (as noted by other commenters) the paper does not make any substantial comparison with other possible ways of producing trees. The paper does a solid job at breaking down the results quantitatively, but I would appreciate some examples of model output and some qualitative error analysis.

Detail notes: 

- Figure 2 appears to have an error. You report that the decoder produces a distribution over input indices only, but you show an example of the network pointing to an output index in one case.
- I don't think ""Wei12"" is a name.",0
"This paper introduces a novel extension of the variational autoencoder to arbitrary tree-structured outputs. Experiments are conducted on a synthetic arithmetic expression dataset and a first-order logic proof clause dataset in order to evaluate its density modeling performance.

Pros:
+ The paper is clear and well-written.
+ The tree-structure definition is sufficiently complete to capture a wide variety of tree types found in real-world situations.
+ The tree generation and encoding procedure is elegant and well-articulated.
+ The experiments, though limited in scope, are relatively thorough. The use of IWAE to obtain a better estimate of log likelihoods is a particularly nice touch.

Cons:
- The performance gain over a baseline sequential model is marginal.
- The experiments are limited in scope, both in the datasets considered and in the evaluation metrics used to compare the model with other approaches. Specifically: (a) there is only one set of results on a real-world dataset and in that case the proposed model performs worse than the baseline, and (b) there is no evaluation of the learned latent representation with respect to other tasks such as classification.
- The ability of the model to generate trees in time proportional to the depth of the tree is proposed as a benefit of the approach, though this is not empirically validated in the experiments.

The procedures to generate and encode trees are clever in their repeated use of common operations. The weight sharing and gating operations seem important for this model to perform well but it is difficult to assess their utility without an ablation (in Table 1 and 2 these modifications are not evaluated side-by-side). Experiments in another domain (such as modeling source code, or parse trees conditioned on a sentence) would help in demonstrating the utility of this model. Overall the model seems promising and applicable to a variety of data but the lack of breadth in the experiments is a concern.

* Section 3.1: ""We distinguish three types"" => two
* Section 3.6: The exposition of the variable-sized latent state is slightly confusing because the issue of how many z's to generate is not discussed.
* Section 4.2-4.3: When generating the datasets, did you verify that the test set is disjoint from the training set?
* Table 1: Is there a particular reason why the variable latent results are missing for the depth 11 trees?",0
"This paper introduces a novel extension of the variational autoencoder to arbitrary tree-structured outputs. Experiments are conducted on a synthetic arithmetic expression dataset and a first-order logic proof clause dataset in order to evaluate its density modeling performance.

Pros:
+ The paper is clear and well-written.
+ The tree-structure definition is sufficiently complete to capture a wide variety of tree types found in real-world situations.
+ The tree generation and encoding procedure is elegant and well-articulated.
+ The experiments, though limited in scope, are relatively thorough. The use of IWAE to obtain a better estimate of log likelihoods is a particularly nice touch.

Cons:
- The performance gain over a baseline sequential model is marginal.
- The experiments are limited in scope, both in the datasets considered and in the evaluation metrics used to compare the model with other approaches. Specifically: (a) there is only one set of results on a real-world dataset and in that case the proposed model performs worse than the baseline, and (b) there is no evaluation of the learned latent representation with respect to other tasks such as classification.
- The ability of the model to generate trees in time proportional to the depth of the tree is proposed as a benefit of the approach, though this is not empirically validated in the experiments.

The procedures to generate and encode trees are clever in their repeated use of common operations. The weight sharing and gating operations seem important for this model to perform well but it is difficult to assess their utility without an ablation (in Table 1 and 2 these modifications are not evaluated side-by-side). Experiments in another domain (such as modeling source code, or parse trees conditioned on a sentence) would help in demonstrating the utility of this model. Overall the model seems promising and applicable to a variety of data but the lack of breadth in the experiments is a concern.

* Section 3.1: ""We distinguish three types"" => two
* Section 3.6: The exposition of the variable-sized latent state is slightly confusing because the issue of how many z's to generate is not discussed.
* Section 4.2-4.3: When generating the datasets, did you verify that the test set is disjoint from the training set?
* Table 1: Is there a particular reason why the variable latent results are missing for the depth 11 trees?",0
"The paper presents a way to ""learn"" approximate data structures. They train neural networks (ConvNets here) to perform as an approximate abstract data structure by having an L2 loss (for the unrolled NN) on respecting the axioms of the data structure they want the NN to learn. E.g. you NN.push(8), NN.push(6), NN.push(4), the loss is proportional to the distance with what is NN.pop()ed three times and 4, 6, 8 (this example is the one of Figure 1).

There are several flaws:
 - In the case of the stack: I do not see a difference between this and a seq-to-seq RNN trained with e.g. 8, 6, 4 as input sequence, to predict 4, 6, 8.
 - While some of the previous work is adequately cited, there is an important body of previous work (some from the 90s) on learning Peano's axioms, stacks, queues, etc. that is not cited nor compared to. For instance [Das et al. 1992], [Wiles & Elman 1995], and more recently [Graves et al. 2014], [Joulin & Mikolov 2015], [Kaiser & Sutskever 2016]...
 - Using MNIST digits, and not e.g. a categorical distribution on numbers, is adding complexity for no reason.
 - (Probably the biggest flaw) The experimental section is too weak to support the claims. The figures are adequate, but there is no comparison to anything. There is also no description nor attempt to quantify a form of ""success rate"" of learning such data structures, for instance w.r.t the number of examples, or w.r.t to the size of the input sequences. The current version of the paper (December 9th 2016) provides, at best, anecdotal experimental evidence to support the claims of the rest of the paper.

While an interesting direction of research, I think that this paper is not experimentally sound enough for ICLR.",0
"The paper presents a way to ""learn"" approximate data structures. They train neural networks (ConvNets here) to perform as an approximate abstract data structure by having an L2 loss (for the unrolled NN) on respecting the axioms of the data structure they want the NN to learn. E.g. you NN.push(8), NN.push(6), NN.push(4), the loss is proportional to the distance with what is NN.pop()ed three times and 4, 6, 8 (this example is the one of Figure 1).

There are several flaws:
 - In the case of the stack: I do not see a difference between this and a seq-to-seq RNN trained with e.g. 8, 6, 4 as input sequence, to predict 4, 6, 8.
 - While some of the previous work is adequately cited, there is an important body of previous work (some from the 90s) on learning Peano's axioms, stacks, queues, etc. that is not cited nor compared to. For instance [Das et al. 1992], [Wiles & Elman 1995], and more recently [Graves et al. 2014], [Joulin & Mikolov 2015], [Kaiser & Sutskever 2016]...
 - Using MNIST digits, and not e.g. a categorical distribution on numbers, is adding complexity for no reason.
 - (Probably the biggest flaw) The experimental section is too weak to support the claims. The figures are adequate, but there is no comparison to anything. There is also no description nor attempt to quantify a form of ""success rate"" of learning such data structures, for instance w.r.t the number of examples, or w.r.t to the size of the input sequences. The current version of the paper (December 9th 2016) provides, at best, anecdotal experimental evidence to support the claims of the rest of the paper.

While an interesting direction of research, I think that this paper is not experimentally sound enough for ICLR.",0
"This paper addresses video captioning with a TEM-HAM architecture, where a HAM module attends over attended outputs of the TEM module when generating the description. This gives a kind of 2-level attention. The model is evaluated on the Charades and MSVD datasets.

1. Quality/Clarify: I found this paper to be poorly written and relatively hard to understand. As far as I can tell the TEM module of Section 3.1 is a straight-forward attention frame encoder of Bahdanau et al. 2015 or Xu et al. 2015. The decoder of Section 3.3 is a standard LSTM with log likelihood. The HAM module of Section 3.2 is the novel module but is not very well described. It looks to be an attention LSTM where the attention is over the TEM LSTM outputs, but the attention weights are additionally conditioned on the decoder state. There are a lot of small problems with the description, such as notational discrepancy in using \textbf in equations and then not using it in the text. Also, I spent a long time trying to understand what f_m is. The authors say: 

""In order to let the network remember what has been attended before and the temporal
structure of a video, we propose f_m to memorize the previous attention and encoded version of an
input video with language model. Using f_m not only enables the network to memorize previous
attention and frames, but also to learn multi-layer attention over an input video and corresponding
language.""

Where one f_m is bold and the other f_m is not. Due to words such as ""we propose f_m"" assumed this was some kind of a novel technical contribution I couldn't find any details about but it is specified later in Section 3.3 at the end that f_m is in fact just an LSTM. It's not clear why this piece of information is in Section 3.3, which discusses the decoder. The paper is sloppy in other parts. For example in Table 1 some numbers have 1 significant digit and some have 2. The semantics of the horizontal line in Table 2 are not explained in text. 

2. Experimental results: The ablation study shows mixed results when adding TEM and HAM to the model. Looking at METEOR which was shown to have the highest correlation to humans in the COCO paper compared to the other evaluation criteria, adding TEM+HAM improves the model from 31.20 to 31.70. It is not clear how significant this improvement is, especially given that the test set is only 670 videos. I have doubts over this result. In Table 2, the METEOR score of Pan et al. 2016a is higher [33.10 vs. 31.80], but this discrepancy is not addressed in text. This is surprising because the authors explicitly claim ""state of the art results"".

3. Originality/Significance: The paper introduces an additional layer of attention over a more standard sequence to sequence setup, which is argued to alleviate the burden on the LSTM's memory. This is moderately novel but I don't believe that the experimental results make it sufficiently clear that it is also worth doing. If the paper made the standard model somehow simpler instead of more complex I would be more inclined to judge it favorably.


Minor:
In response to the author's comment ""not sure what causes to think of RBM. We don't model any part of our architecture using RBM. We'd be appreciated if you please elaborate more about your confusion about figure 1 so we can address it accordingly."", I created a diagram to hopefully make this more clear:",0
"This paper addresses video captioning with a TEM-HAM architecture, where a HAM module attends over attended outputs of the TEM module when generating the description. This gives a kind of 2-level attention. The model is evaluated on the Charades and MSVD datasets.

1. Quality/Clarify: I found this paper to be poorly written and relatively hard to understand. As far as I can tell the TEM module of Section 3.1 is a straight-forward attention frame encoder of Bahdanau et al. 2015 or Xu et al. 2015. The decoder of Section 3.3 is a standard LSTM with log likelihood. The HAM module of Section 3.2 is the novel module but is not very well described. It looks to be an attention LSTM where the attention is over the TEM LSTM outputs, but the attention weights are additionally conditioned on the decoder state. There are a lot of small problems with the description, such as notational discrepancy in using \textbf in equations and then not using it in the text. Also, I spent a long time trying to understand what f_m is. The authors say: 

""In order to let the network remember what has been attended before and the temporal
structure of a video, we propose f_m to memorize the previous attention and encoded version of an
input video with language model. Using f_m not only enables the network to memorize previous
attention and frames, but also to learn multi-layer attention over an input video and corresponding
language.""

Where one f_m is bold and the other f_m is not. Due to words such as ""we propose f_m"" assumed this was some kind of a novel technical contribution I couldn't find any details about but it is specified later in Section 3.3 at the end that f_m is in fact just an LSTM. It's not clear why this piece of information is in Section 3.3, which discusses the decoder. The paper is sloppy in other parts. For example in Table 1 some numbers have 1 significant digit and some have 2. The semantics of the horizontal line in Table 2 are not explained in text. 

2. Experimental results: The ablation study shows mixed results when adding TEM and HAM to the model. Looking at METEOR which was shown to have the highest correlation to humans in the COCO paper compared to the other evaluation criteria, adding TEM+HAM improves the model from 31.20 to 31.70. It is not clear how significant this improvement is, especially given that the test set is only 670 videos. I have doubts over this result. In Table 2, the METEOR score of Pan et al. 2016a is higher [33.10 vs. 31.80], but this discrepancy is not addressed in text. This is surprising because the authors explicitly claim ""state of the art results"".

3. Originality/Significance: The paper introduces an additional layer of attention over a more standard sequence to sequence setup, which is argued to alleviate the burden on the LSTM's memory. This is moderately novel but I don't believe that the experimental results make it sufficiently clear that it is also worth doing. If the paper made the standard model somehow simpler instead of more complex I would be more inclined to judge it favorably.


Minor:
In response to the author's comment ""not sure what causes to think of RBM. We don't model any part of our architecture using RBM. We'd be appreciated if you please elaborate more about your confusion about figure 1 so we can address it accordingly."", I created a diagram to hopefully make this more clear:",0
An interesting study of using Sine as activation function showing successful training of models using Sine. However the scope of tasks this is applied to is a bit too limited to be convincing. Maybe showing good results on more important tasks in addition to current toy tasks would make a stronger case?,0
An interesting study of using Sine as activation function showing successful training of models using Sine. However the scope of tasks this is applied to is a bit too limited to be convincing. Maybe showing good results on more important tasks in addition to current toy tasks would make a stronger case?,0
"This work proposes a convolutional architecture for any graph-like input data (where the structure is example-dependent), or more generally, any data where the input dimensions that are related by a similarity matrix. If instead each input example is associated with a transition matrix, then a random walk algorithm is used generate a similarity matrix.

Developing convolutional or recurrent architectures for graph-like data is an important problem because we would like to develop neural networks that can handle inputs such as molecule structures or social networks. However, I don't think this work contributes anything significant to the work that has already been done in this area. 

The two main proposals I see in this paper are:
1) For data associated with a transition matrix, this paper proposes that the transition matrix be converted to a similarity matrix. This seems obvious.
2) For data associated with a similarity matrix, the k nearest neighbors of each node are computed and supply the context information for that node. This also seems obvious.

Perhaps I have misunderstood the contribution, but the presentation also lacks clarity, and I cannot recommend this paper for publication. 

Specific Comments:
1) On page 4: ""An interesting attribute of this convolution, as compared to other convolutions on graphs is that, it preserves locality while still being applicable over different graphs with different structures.""  This is false; the other proposed architectures can be applied to inputs with different structures (e.g. Duvenaud et. al., Lusci et. al. for NN architectures on molecules specifically).",0
"This work proposes a convolutional architecture for any graph-like input data (where the structure is example-dependent), or more generally, any data where the input dimensions that are related by a similarity matrix. If instead each input example is associated with a transition matrix, then a random walk algorithm is used generate a similarity matrix.

Developing convolutional or recurrent architectures for graph-like data is an important problem because we would like to develop neural networks that can handle inputs such as molecule structures or social networks. However, I don't think this work contributes anything significant to the work that has already been done in this area. 

The two main proposals I see in this paper are:
1) For data associated with a transition matrix, this paper proposes that the transition matrix be converted to a similarity matrix. This seems obvious.
2) For data associated with a similarity matrix, the k nearest neighbors of each node are computed and supply the context information for that node. This also seems obvious.

Perhaps I have misunderstood the contribution, but the presentation also lacks clarity, and I cannot recommend this paper for publication. 

Specific Comments:
1) On page 4: ""An interesting attribute of this convolution, as compared to other convolutions on graphs is that, it preserves locality while still being applicable over different graphs with different structures.""  This is false; the other proposed architectures can be applied to inputs with different structures (e.g. Duvenaud et. al., Lusci et. al. for NN architectures on molecules specifically).",0
"I reviewed the manuscript as of December 6th.

The authors perform a systematic investigation of various retraining methods for making a classification network robust to adversarial examples. The authors achieve lower error rates using their RAD and IAEC methods perform better then previously introduced distillation methods for retraining networks to be robust to adversarial examples. This method suggests a promising direction for building a defense for adversarial examples.

Major Comments:
I find the paper to not be lacking in exposition and clarity. The paper has a laundry list of related results (page 2) but no clear message. I *think* one larger point is the superior performance of their retraining techniques but it is not clear how well these techniques perform compared to other retraining techniques, nor are the details of the retraining techniques clear. The paper requires more discussion and a clear exposition about the methods the authors introduced (i.e. RAD, IAEC). What follow are some more detailed comments along this theme of improving the exposition and clarity:

- The authors should provide more details about how they constructed the auto-encoder in the IAEC method (diagram?). The same needs to be said for the RAD method. The authors point to a previous workshop submission (",0
"I reviewed the manuscript as of December 6th.

The authors perform a systematic investigation of various retraining methods for making a classification network robust to adversarial examples. The authors achieve lower error rates using their RAD and IAEC methods perform better then previously introduced distillation methods for retraining networks to be robust to adversarial examples. This method suggests a promising direction for building a defense for adversarial examples.

Major Comments:
I find the paper to not be lacking in exposition and clarity. The paper has a laundry list of related results (page 2) but no clear message. I *think* one larger point is the superior performance of their retraining techniques but it is not clear how well these techniques perform compared to other retraining techniques, nor are the details of the retraining techniques clear. The paper requires more discussion and a clear exposition about the methods the authors introduced (i.e. RAD, IAEC). What follow are some more detailed comments along this theme of improving the exposition and clarity:

- The authors should provide more details about how they constructed the auto-encoder in the IAEC method (diagram?). The same needs to be said for the RAD method. The authors point to a previous workshop submission (",0
"The responses to the pre-review questions are not strong; especially w.r.t. the question about dataset density and why the dataset had to be subsampled, the authors responded that subsampling is common in recommender systems work, including the papers cited. This isn't a particularly strong justification of why subsampling is a good idea, and in particular doesn't answer the question of ""how would the results look without subsampling,"" which I think is a question that could easily have been answered directly.

Especially given that the goal of dealing with the cold-start issue is so heavily emphasized in the paper, in seems odd to sample the data to reduce sparsity.

Other than that, the pre-review questions seem to have been answered satisfactorily.

The contribution of the paper is to propose user and item embedding methods, as a means of learning complex non-linear interactions between users and items. This is fairly similar to recent work on deep RS, though the network formulation has some differences.

Overall this is an reasonably put together paper that makes a contribution in an important area, though there are still some shortcomings that should be addressed, namely:
1) The evaluation is unusual. Recall@M is the only result reported, though this is not usually an evaluation seen in recommender systems research. At the very least other performance measures (rmse or AUC) should be reported for completeness, even if the results are not strong
2) Given that the contribution is fairly simple (i.e., the ""standard"" recommender systems task, but with a new model) it's a shame that unusual data samples have to be taken. This should be a case where it's possible to report results against competing methods using *exactly* the same data they used, and exactly the same error measure, for the fairest comparison possible.

Without the above it's hard to tell how much the performance improvements are really due to the method being better, versus the choice of datasets and the choice of loss functions.",0
"The responses to the pre-review questions are not strong; especially w.r.t. the question about dataset density and why the dataset had to be subsampled, the authors responded that subsampling is common in recommender systems work, including the papers cited. This isn't a particularly strong justification of why subsampling is a good idea, and in particular doesn't answer the question of ""how would the results look without subsampling,"" which I think is a question that could easily have been answered directly.

Especially given that the goal of dealing with the cold-start issue is so heavily emphasized in the paper, in seems odd to sample the data to reduce sparsity.

Other than that, the pre-review questions seem to have been answered satisfactorily.

The contribution of the paper is to propose user and item embedding methods, as a means of learning complex non-linear interactions between users and items. This is fairly similar to recent work on deep RS, though the network formulation has some differences.

Overall this is an reasonably put together paper that makes a contribution in an important area, though there are still some shortcomings that should be addressed, namely:
1) The evaluation is unusual. Recall@M is the only result reported, though this is not usually an evaluation seen in recommender systems research. At the very least other performance measures (rmse or AUC) should be reported for completeness, even if the results are not strong
2) Given that the contribution is fairly simple (i.e., the ""standard"" recommender systems task, but with a new model) it's a shame that unusual data samples have to be taken. This should be a case where it's possible to report results against competing methods using *exactly* the same data they used, and exactly the same error measure, for the fairest comparison possible.

Without the above it's hard to tell how much the performance improvements are really due to the method being better, versus the choice of datasets and the choice of loss functions.",0
"This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.

To that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see ""Locally affine sparse-to-dense matching for motion and occlusion estimation"" by Leordeanu et al., ""EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow"" by Revaud et al., ""Optical Flow With Semantic Segmentation and Localized Layers"" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. 
In addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.

While I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:

  - the choice of not comparing with previous approaches in term of pixel prediction error seems very ""convenient"", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.
  
  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.
  
  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.
  
  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. 
  
  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to ""SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY"" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.

  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.

  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at",0
"This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.

To that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see ""Locally affine sparse-to-dense matching for motion and occlusion estimation"" by Leordeanu et al., ""EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow"" by Revaud et al., ""Optical Flow With Semantic Segmentation and Localized Layers"" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. 
In addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.

While I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:

  - the choice of not comparing with previous approaches in term of pixel prediction error seems very ""convenient"", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.
  
  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.
  
  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.
  
  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. 
  
  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to ""SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY"" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.

  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.

  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at",0
"The present submission discusses a ""causal regularizer"", which promotes the use of causal dependencies (X -> Y, where X is a feature of the learning problem, and Y is the target variable) in predictive models. Similarly, such causal regularizer penalizes the use of non-causal dependencies, which can arise due to reverse causation (Y -> X) or confounding (X <- Z -> Y, where Z is a hidden confounder).

+ Overall, this submission tackles one of the most important problems in machine learning, which is to build causal models. The paper discusses and addresses this issue effectively when applied to a dataset in heart disease. In their experiments, the authors correctly identify some of the common causes of heart disease by virtue of their causal regularizer.

- The authors do not discuss the robustness of their approach with respect to choice of hyper-parameters (both describing the neural network architecture and the generative model that synthesizes artificial causal data). This seems like a crucial issue, in particular when dealing with medical data.

- The conclusions of the experimental evaluation should be discussed in greater length. On the one hand, Figure 4.a shows that there are no differences between L1 and causal regularization in terms of predictive performance, but it is difficult to conclude if this result is statistically significant without access to error-bars. On the other hand, Table 3 describes the qualitative differences between L1 and causal regularization. However, this table is hard to read: How were the 30 rows selected? What does the red highlighting mean? Are these red rows some true causal features that were missed? If so, this is related to precision. What about recall? Did the causal regularization pick up many non-causal features as causal?

- Regarding causal classifiers, this paper should do a much better job at reviewing previous work. For instance, the paper ""Towards a Learning Theory of Cause-Effect Inference"" from Lopez-Paz et al. is missing from the references. However, this prior work studies many of the aspects that are hinted as novel in this submission. In particular, the prior work of Lopez-Paz 1) introduces the concept of Mother distribution (referred as Nature hyper-prior in this submission) which explicitly factorizes the distribution over causes and mechanisms, 2) circumvented intractable likelihoods by synthesizing and training on causal data, 3) tackled the confounding case (compare Figure 1 of this submission and Appendix C of Lopez-Paz), and 4) dealt with discrete data seamlessly (such as the ChaLearn data from Section 5.3 in Lopez-Paz).

On a positive note, this is a well-written paper that addresses the important, under-appreciated problem of incorporating causal reasoning into machine learning. On a negative note, the novelty of the technical contributions is modest and the qualitative evaluation of the results could be greatly extended. In short, I am leaning slightly towards acceptance.",0
"The present submission discusses a ""causal regularizer"", which promotes the use of causal dependencies (X -> Y, where X is a feature of the learning problem, and Y is the target variable) in predictive models. Similarly, such causal regularizer penalizes the use of non-causal dependencies, which can arise due to reverse causation (Y -> X) or confounding (X <- Z -> Y, where Z is a hidden confounder).

+ Overall, this submission tackles one of the most important problems in machine learning, which is to build causal models. The paper discusses and addresses this issue effectively when applied to a dataset in heart disease. In their experiments, the authors correctly identify some of the common causes of heart disease by virtue of their causal regularizer.

- The authors do not discuss the robustness of their approach with respect to choice of hyper-parameters (both describing the neural network architecture and the generative model that synthesizes artificial causal data). This seems like a crucial issue, in particular when dealing with medical data.

- The conclusions of the experimental evaluation should be discussed in greater length. On the one hand, Figure 4.a shows that there are no differences between L1 and causal regularization in terms of predictive performance, but it is difficult to conclude if this result is statistically significant without access to error-bars. On the other hand, Table 3 describes the qualitative differences between L1 and causal regularization. However, this table is hard to read: How were the 30 rows selected? What does the red highlighting mean? Are these red rows some true causal features that were missed? If so, this is related to precision. What about recall? Did the causal regularization pick up many non-causal features as causal?

- Regarding causal classifiers, this paper should do a much better job at reviewing previous work. For instance, the paper ""Towards a Learning Theory of Cause-Effect Inference"" from Lopez-Paz et al. is missing from the references. However, this prior work studies many of the aspects that are hinted as novel in this submission. In particular, the prior work of Lopez-Paz 1) introduces the concept of Mother distribution (referred as Nature hyper-prior in this submission) which explicitly factorizes the distribution over causes and mechanisms, 2) circumvented intractable likelihoods by synthesizing and training on causal data, 3) tackled the confounding case (compare Figure 1 of this submission and Appendix C of Lopez-Paz), and 4) dealt with discrete data seamlessly (such as the ChaLearn data from Section 5.3 in Lopez-Paz).

On a positive note, this is a well-written paper that addresses the important, under-appreciated problem of incorporating causal reasoning into machine learning. On a negative note, the novelty of the technical contributions is modest and the qualitative evaluation of the results could be greatly extended. In short, I am leaning slightly towards acceptance.",0
"The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled – autoregressive processes – i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines.
There are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation.
The paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers.
Concerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation?
A comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods.
Overall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors.  I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets.",0
"The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled – autoregressive processes – i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines.
There are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation.
The paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers.
Concerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation?
A comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods.
Overall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors.  I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets.",0
"It is an interesting idea to go after saddle points in the optimization with an SR1 update and a good start in experiments, but missing important comparisons to recent 2nd order optimizations such as Adam, other Hessian free methods (Martens 2012), Pearlmutter fast exact multiplication by the Hessian. From the mnist/cifar curves it is not really showing an advantage to AdaDelta/Nag (although this is stated), and much more experimentation is needed to make a claim about mini-batch insensitivity to performance, can you show error rates on a larger scale task?",0
"It is an interesting idea to go after saddle points in the optimization with an SR1 update and a good start in experiments, but missing important comparisons to recent 2nd order optimizations such as Adam, other Hessian free methods (Martens 2012), Pearlmutter fast exact multiplication by the Hessian. From the mnist/cifar curves it is not really showing an advantage to AdaDelta/Nag (although this is stated), and much more experimentation is needed to make a claim about mini-batch insensitivity to performance, can you show error rates on a larger scale task?",0
"The paper describes a network architecture for inverse problems in computer vision. Example inverse problems considered are image inpainting, computing intrinsic image decomposition and foreground/background separation.
The architecture is composed of (i) a generator that produces target (latent) output (such as foreground / background regions), 
(ii) renderer that composes that latent output back to the image that can be compared with the input to measure reconstruction error, 
and (iii) adversarial prior that ensures the target output (latent) image respects a certain image statistics.

Strong  points.
- The proposed architecture with memory database is interesting and appears to be novel. 

Weak points:
- Experimental results are only proof-of-concept in toy set-ups and do not clearly demonstrate benefits of the proposed architecture.
- It is unclear whether the memory retrieval engine that retrieves images based on L2 distance on pixel values is going generalize to other more realistic scenarios. 
- Clarity. The clarity of explanation can be also improved (see below).


Detailed evaluation.

Originality:
- The novelty of this work lies in the (iii) adversarial prior that places an adversarial loss between the generated latent output and a single image retrieved from a large unlabelled database of target output examples (called memory). The adversarial prior has a convolutional form matching local image statistics, rather than the entire image.  The particular form of network architecture with the memory-based fully convolutional adversarial loss appears to be novel and potentially interesting.

- Motivation for the Architecture. The weakest point of the proposed architecture is the ""Memory retrieval engine"" R (section 2.4),
where images are retrieved from the memory by measuring L2 distance on pixel intensities. While this maybe ok for simple problems considered in this work, it is unclear how this can generalize to other more complicated datasets and problems.  
This should be better discussed, better justified and ideally results in some more realistic set-up shown (see below).


Quality:
- Experiments. Results are shown for inpainting of MNIST digits, intrinsic image decomposition on the MIT intrinsic image database, and figure/ground layer extraction on the synthesized dataset of 3D chairs rendered onto background from real photographs.  
 The experimental validation of the model is not very strong and proof-of-concept only. All the experiments are performed in simplified toy set-ups. The MNIST digit inpainting is far from current state-of-the-art on image inpainting in real photographs (see e.g. Pathak et al., 2016). The foreground background separation is done on  only synthetically generated test data. Even for intrinsic image demposition problem there is now relatively large-scale dataset of (Bell et al., 2014), see the citation below.  

While this is probably ok for the ICLR paper, it diminishes the significance of the work. Is this model going to be useful in a real settings? One possibility to address this would be to focus on one of the problems and show results on a challenging state-of-the-art data. It would be great to see the benefits of the memory database. 

S. Bell, K. Bala, and N. Snavely. Intrinsic images in the wild.
ACM Transactions on Graphics, 33(4):159, 2014.

Clarity:
- The clarity of the writing can be improved. I found some of the terminology of the paper, specially the “imagination” and “memory” confusing. From figure 2, it is not clear how the “memories” for the given input image are obtained, which also took me some time to understand.

- To help understand the proposed architecture, it would be useful to draw an illustration of what is happening in the ""feature space”, similar in spirit e.g. to figure 2 in",0
"The paper describes a network architecture for inverse problems in computer vision. Example inverse problems considered are image inpainting, computing intrinsic image decomposition and foreground/background separation.
The architecture is composed of (i) a generator that produces target (latent) output (such as foreground / background regions), 
(ii) renderer that composes that latent output back to the image that can be compared with the input to measure reconstruction error, 
and (iii) adversarial prior that ensures the target output (latent) image respects a certain image statistics.

Strong  points.
- The proposed architecture with memory database is interesting and appears to be novel. 

Weak points:
- Experimental results are only proof-of-concept in toy set-ups and do not clearly demonstrate benefits of the proposed architecture.
- It is unclear whether the memory retrieval engine that retrieves images based on L2 distance on pixel values is going generalize to other more realistic scenarios. 
- Clarity. The clarity of explanation can be also improved (see below).


Detailed evaluation.

Originality:
- The novelty of this work lies in the (iii) adversarial prior that places an adversarial loss between the generated latent output and a single image retrieved from a large unlabelled database of target output examples (called memory). The adversarial prior has a convolutional form matching local image statistics, rather than the entire image.  The particular form of network architecture with the memory-based fully convolutional adversarial loss appears to be novel and potentially interesting.

- Motivation for the Architecture. The weakest point of the proposed architecture is the ""Memory retrieval engine"" R (section 2.4),
where images are retrieved from the memory by measuring L2 distance on pixel intensities. While this maybe ok for simple problems considered in this work, it is unclear how this can generalize to other more complicated datasets and problems.  
This should be better discussed, better justified and ideally results in some more realistic set-up shown (see below).


Quality:
- Experiments. Results are shown for inpainting of MNIST digits, intrinsic image decomposition on the MIT intrinsic image database, and figure/ground layer extraction on the synthesized dataset of 3D chairs rendered onto background from real photographs.  
 The experimental validation of the model is not very strong and proof-of-concept only. All the experiments are performed in simplified toy set-ups. The MNIST digit inpainting is far from current state-of-the-art on image inpainting in real photographs (see e.g. Pathak et al., 2016). The foreground background separation is done on  only synthetically generated test data. Even for intrinsic image demposition problem there is now relatively large-scale dataset of (Bell et al., 2014), see the citation below.  

While this is probably ok for the ICLR paper, it diminishes the significance of the work. Is this model going to be useful in a real settings? One possibility to address this would be to focus on one of the problems and show results on a challenging state-of-the-art data. It would be great to see the benefits of the memory database. 

S. Bell, K. Bala, and N. Snavely. Intrinsic images in the wild.
ACM Transactions on Graphics, 33(4):159, 2014.

Clarity:
- The clarity of the writing can be improved. I found some of the terminology of the paper, specially the “imagination” and “memory” confusing. From figure 2, it is not clear how the “memories” for the given input image are obtained, which also took me some time to understand.

- To help understand the proposed architecture, it would be useful to draw an illustration of what is happening in the ""feature space”, similar in spirit e.g. to figure 2 in",0
"This paper is relatively difficult to parse. Much of the exposition of the proposed algorithm could be better presented using pseudo-code describing the compute flow, or a diagram describing exactly how the updates take place. As it stands, I'm not sure I understand everything. I would also have liked to see exactly described what the various labels in Fig 1 correspond to (""SGD task-wise, 1 comm""? Did you mean layer-wise?).
There are a couple of major issues with the evaluation: first, no comparison is reported against baseline async methods such as using a parameter server. Second, using AlexNet as a benchmark is not informative at all. AlexNet looks very different from any SOTA image recognition model, and in particular it has many fewer layers, which is especially relevant to the discussion in 6.3. It also uses lots of fully-connected layers which affect the compute/communication ratios in ways that are not relevant to most interesting architectures today.",0
"This paper is relatively difficult to parse. Much of the exposition of the proposed algorithm could be better presented using pseudo-code describing the compute flow, or a diagram describing exactly how the updates take place. As it stands, I'm not sure I understand everything. I would also have liked to see exactly described what the various labels in Fig 1 correspond to (""SGD task-wise, 1 comm""? Did you mean layer-wise?).
There are a couple of major issues with the evaluation: first, no comparison is reported against baseline async methods such as using a parameter server. Second, using AlexNet as a benchmark is not informative at all. AlexNet looks very different from any SOTA image recognition model, and in particular it has many fewer layers, which is especially relevant to the discussion in 6.3. It also uses lots of fully-connected layers which affect the compute/communication ratios in ways that are not relevant to most interesting architectures today.",0
"The authors propose to add noise to the gradients computed while optimizing deep neural networks with stochastic gradient based methods. They show results multiple data sets which indicate that the method can counteract bad parameter initialization and that it can be especially beneficial for training more complicated architectures.

The method is tested on a multitude of different tasks and architectures. The results would be more convincing if they would be accompanied by confidence intervals but I understand that some of the experiments must have taken very long to run. I like that the results include both situations in which the gradient noise helps a lot and situations in which it doesn’t seem to add much to the other optimization or initialization tools employed. The quantity of the experiments and the variety of the models provide quite convincing evidence that the effect of the gradient noise generalizes to many settings. The results were not always that convincing. In Section 4.2, the method only helped significantly when a sub-optimal training scheme was used, for example. The results on MNIST are not very good compared to the state-of-the-art. Since the method is so simple, I was hoping to see more theoretical arguments for its usefulness. That said, the experimental investigations into the importance of the annealing procedure, the comparison with the effect of gradient stochasticity and the comparison with weight noise, provide some additional insight.

The paper is well written and cites relevant prior work. The proposed method is described clearly and concisely, which is to be expected given its simplicity. 

The proposed idea is not very original. As the authors acknowledge, very similar algorithms have been used for training and it is pretty much identical to simulating Langevin dynamics but with the goal of finding a single optimum in mind rather than approximating an expected value. The work is the evaluation of an old tool in a new era where models have become bigger and more complex.

Despite the lack of novelty of the method, I do think that the results are valuable. The method is so easy to implement and seems to be so useful for complicated model which are hard to initialize, that it is important for others in the field to know about it. I suspect many people will at least try the method. The variety of the architectures and tasks for which the method was useful suggests that many people may also add it to their repertoire of optimization tricks. 


Pros:
* The idea is easy to implement.
* The method is evaluated on a variety of tasks and for very different models.
* Some interesting experiments which compare the method with similar approaches and investigate the importance of the annealing scheme.
* The paper is well-written.


Cons:
* The idea is not very original.
* There is no clear theoretical motivation of analysis.
* Not all the results are convincing.",0
"The authors propose to add noise to the gradients computed while optimizing deep neural networks with stochastic gradient based methods. They show results multiple data sets which indicate that the method can counteract bad parameter initialization and that it can be especially beneficial for training more complicated architectures.

The method is tested on a multitude of different tasks and architectures. The results would be more convincing if they would be accompanied by confidence intervals but I understand that some of the experiments must have taken very long to run. I like that the results include both situations in which the gradient noise helps a lot and situations in which it doesn’t seem to add much to the other optimization or initialization tools employed. The quantity of the experiments and the variety of the models provide quite convincing evidence that the effect of the gradient noise generalizes to many settings. The results were not always that convincing. In Section 4.2, the method only helped significantly when a sub-optimal training scheme was used, for example. The results on MNIST are not very good compared to the state-of-the-art. Since the method is so simple, I was hoping to see more theoretical arguments for its usefulness. That said, the experimental investigations into the importance of the annealing procedure, the comparison with the effect of gradient stochasticity and the comparison with weight noise, provide some additional insight.

The paper is well written and cites relevant prior work. The proposed method is described clearly and concisely, which is to be expected given its simplicity. 

The proposed idea is not very original. As the authors acknowledge, very similar algorithms have been used for training and it is pretty much identical to simulating Langevin dynamics but with the goal of finding a single optimum in mind rather than approximating an expected value. The work is the evaluation of an old tool in a new era where models have become bigger and more complex.

Despite the lack of novelty of the method, I do think that the results are valuable. The method is so easy to implement and seems to be so useful for complicated model which are hard to initialize, that it is important for others in the field to know about it. I suspect many people will at least try the method. The variety of the architectures and tasks for which the method was useful suggests that many people may also add it to their repertoire of optimization tricks. 


Pros:
* The idea is easy to implement.
* The method is evaluated on a variety of tasks and for very different models.
* Some interesting experiments which compare the method with similar approaches and investigate the importance of the annealing scheme.
* The paper is well-written.


Cons:
* The idea is not very original.
* There is no clear theoretical motivation of analysis.
* Not all the results are convincing.",0
"This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon’s Mechanical Turk illustrated that the model can generate compelling music.

In general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski  et al., (2012).",0
"This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon’s Mechanical Turk illustrated that the model can generate compelling music.

In general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski  et al., (2012).",0
"Summary:
In this paper, the authors study ResNets through a theoretical formulation of a spin glass model. The conclusions are that ResNets behave as an ensemble of shallow networks at the start of training (by examining the magnitude of the weights for paths of a specific length) but this changes through training, through which the scaling parameter C (from assumption A4) increases, causing it to behave as an ensemble of deeper and deeper networks.

Clarity:
This paper was somewhat difficult to follow, being heavy in notation, with perhaps some notation overloading. A summary of some of the proofs in the main text might have been helpful.

Specific Comments:
- In the proof of Lemma 2, I'm not sure where the sequence beta comes from (I don't see how it follows from 11?)

- The ResNet structure used in the paper is somewhat different from normal with multiple layers being skipped? (Can the same analysis be used if only one layer is skipped? It seems like the skipping mostly affects the number of paths there are of a certain length?)

- The new experiments supporting the scale increase in practice are interesting! I'm not sure about Theorems 3, 4 necessarily proving this link theoretically however, particularly given the simplifying assumption at the start of Section 4.2?",0
"Summary:
In this paper, the authors study ResNets through a theoretical formulation of a spin glass model. The conclusions are that ResNets behave as an ensemble of shallow networks at the start of training (by examining the magnitude of the weights for paths of a specific length) but this changes through training, through which the scaling parameter C (from assumption A4) increases, causing it to behave as an ensemble of deeper and deeper networks.

Clarity:
This paper was somewhat difficult to follow, being heavy in notation, with perhaps some notation overloading. A summary of some of the proofs in the main text might have been helpful.

Specific Comments:
- In the proof of Lemma 2, I'm not sure where the sequence beta comes from (I don't see how it follows from 11?)

- The ResNet structure used in the paper is somewhat different from normal with multiple layers being skipped? (Can the same analysis be used if only one layer is skipped? It seems like the skipping mostly affects the number of paths there are of a certain length?)

- The new experiments supporting the scale increase in practice are interesting! I'm not sure about Theorems 3, 4 necessarily proving this link theoretically however, particularly given the simplifying assumption at the start of Section 4.2?",0
"Studying the Hessian in deep learning, the experiments in this paper suggest that the eigenvalue distribution is concentrated around zero and the non zero eigenvalues are related to the complexity of the input data. I find most of the discussions and experiments to be interesting and insightful. However, the current paper could be significantly improved.

Quality:
It seems that the arguments in the paper could be enhanced by more effort and more comprehensive experiments. Performing some of the experiments discussed in the conclusion could certainly help a lot. Some other suggestions:
1- It would be very helpful to add other plots showing the distribution of eigenvalues for some other machine learning method for the purpose of comparison to deep learning.
2- There are some issues about the scaling of the weights and it make sense to normalize the weights each time before calculating the Hessian otherwise the result might be misleading.
3- It might worth trying to find a quantity that measures the singularity of Hessian because it is difficult to visually conclude something from the plots.
4- Adding some plots for the Hessian during the optimization is definitely needed because we mostly care about the Hessian during the optimization not after the convergence.

Clarity:
1- There is no reference to figures in the main text which makes it confusing for the reading to know the context for each figure. For example, when looking at Figure 1, it is not clear that the Hessian is calculated at the beginning of optimization or after convergence.
2- The texts in the figures are very small and hard to read.",0
"Studying the Hessian in deep learning, the experiments in this paper suggest that the eigenvalue distribution is concentrated around zero and the non zero eigenvalues are related to the complexity of the input data. I find most of the discussions and experiments to be interesting and insightful. However, the current paper could be significantly improved.

Quality:
It seems that the arguments in the paper could be enhanced by more effort and more comprehensive experiments. Performing some of the experiments discussed in the conclusion could certainly help a lot. Some other suggestions:
1- It would be very helpful to add other plots showing the distribution of eigenvalues for some other machine learning method for the purpose of comparison to deep learning.
2- There are some issues about the scaling of the weights and it make sense to normalize the weights each time before calculating the Hessian otherwise the result might be misleading.
3- It might worth trying to find a quantity that measures the singularity of Hessian because it is difficult to visually conclude something from the plots.
4- Adding some plots for the Hessian during the optimization is definitely needed because we mostly care about the Hessian during the optimization not after the convergence.

Clarity:
1- There is no reference to figures in the main text which makes it confusing for the reading to know the context for each figure. For example, when looking at Figure 1, it is not clear that the Hessian is calculated at the beginning of optimization or after convergence.
2- The texts in the figures are very small and hard to read.",0
"The paper studies some special cases of neural networks and datasets where optimization fails. Most of the considered models and datasets are however highly constructed and do not follow the basic hyperparameters selection and parameter initialization heuristics. This reduces the practical relevance of the analysis.

The experiment ""bad initialization on MNIST"" shows that for very negative biases or weights drawn from a non-centered distribution, all ReLU activations are ""off"" for all data points, and thus, optimization is prevented. This never occurs in practice, because using proper initialization heuristics avoid these cases.

The ""jellyfish"" dataset constructed by the authors is demonstrated to be difficult to fit by a small model. However, the size/depth of the considered model is unsuitable for this problem.

Proposition 4 assumes that we can choose the mean from which the weight parameters are initialized. This is typically not the case in practice as most initialization heuristics draw weight parameters from a distribution with mean 0.

Proposition 5 considers infinitely deep ReLU networks. Very deep networks would however preferably be of type ResNet.",0
"The paper studies some special cases of neural networks and datasets where optimization fails. Most of the considered models and datasets are however highly constructed and do not follow the basic hyperparameters selection and parameter initialization heuristics. This reduces the practical relevance of the analysis.

The experiment ""bad initialization on MNIST"" shows that for very negative biases or weights drawn from a non-centered distribution, all ReLU activations are ""off"" for all data points, and thus, optimization is prevented. This never occurs in practice, because using proper initialization heuristics avoid these cases.

The ""jellyfish"" dataset constructed by the authors is demonstrated to be difficult to fit by a small model. However, the size/depth of the considered model is unsuitable for this problem.

Proposition 4 assumes that we can choose the mean from which the weight parameters are initialized. This is typically not the case in practice as most initialization heuristics draw weight parameters from a distribution with mean 0.

Proposition 5 considers infinitely deep ReLU networks. Very deep networks would however preferably be of type ResNet.",0
"The paper presents a hierarchical DRL algorithm that solves sequences of navigate-and-act tasks in a 2D maze domain. During training and evaluation, a list of sub-goals represented by text is given to the agent and its goal is to learn to use pre-learned skills in order to solve a list of sub-goals. The authors demonstrate that their method generalizes well to sequences of varying length as well as to new combinations of sub-goals (i.e., if the agent knows how to pick up a diamond and how to visit an apple, it can also visit the diamond). 

Overall, the paper is of high technical quality and presents an interesting and non-trivial combination of state-of-the-art advancements in Deep Learning (DL) and Deep Reinforcement Learning (DRL). In particular, the authors presents a DRL agent that is hierarchical in the sense that it can learn skills and plan using them. The skills are learned using a differential temporally extended memory networks with an attention mechanism. The authors also make a novel use of analogy making and parameter prediction. 

However, I find it difficult to understand from the paper why the presented problem is interesting and why hadn't it bee solved before. Since the domain being evaluated is a simple 2D maze, using deep networks is not well motivated. Similar problems have been solved using simpler models. In particular, there is a reach literature about planning with skills that had been ignored completely by the authors. Since all of the skills are trained prior to the evaluation of the hierarchical agent, the problem that is being solved is much more similar to supervised learning than reinforcement learning (since when using the pre-trained skills the reward is not particularly delayed). The generalization that is demonstrated seems to be limited to breaking a sentence (describing the subtask) into words (item, location, action). 

The paper is difficult to read, it is constantly switching between describing the algorithm and giving technical details. In particular, I find it to be overloaded with details that interfere with the general understanding of the paper. I suggest moving many of the implementation details into the appendix. The paper should be self-contained, please do not assume that the reader is familiar with all the methods that you use and introduce all the relevant notations. 

I believe that the paper will benefit from addressing the problems I described above and will make a better contribution to the community in a future conference.",0
"The paper presents a hierarchical DRL algorithm that solves sequences of navigate-and-act tasks in a 2D maze domain. During training and evaluation, a list of sub-goals represented by text is given to the agent and its goal is to learn to use pre-learned skills in order to solve a list of sub-goals. The authors demonstrate that their method generalizes well to sequences of varying length as well as to new combinations of sub-goals (i.e., if the agent knows how to pick up a diamond and how to visit an apple, it can also visit the diamond). 

Overall, the paper is of high technical quality and presents an interesting and non-trivial combination of state-of-the-art advancements in Deep Learning (DL) and Deep Reinforcement Learning (DRL). In particular, the authors presents a DRL agent that is hierarchical in the sense that it can learn skills and plan using them. The skills are learned using a differential temporally extended memory networks with an attention mechanism. The authors also make a novel use of analogy making and parameter prediction. 

However, I find it difficult to understand from the paper why the presented problem is interesting and why hadn't it bee solved before. Since the domain being evaluated is a simple 2D maze, using deep networks is not well motivated. Similar problems have been solved using simpler models. In particular, there is a reach literature about planning with skills that had been ignored completely by the authors. Since all of the skills are trained prior to the evaluation of the hierarchical agent, the problem that is being solved is much more similar to supervised learning than reinforcement learning (since when using the pre-trained skills the reward is not particularly delayed). The generalization that is demonstrated seems to be limited to breaking a sentence (describing the subtask) into words (item, location, action). 

The paper is difficult to read, it is constantly switching between describing the algorithm and giving technical details. In particular, I find it to be overloaded with details that interfere with the general understanding of the paper. I suggest moving many of the implementation details into the appendix. The paper should be self-contained, please do not assume that the reader is familiar with all the methods that you use and introduce all the relevant notations. 

I believe that the paper will benefit from addressing the problems I described above and will make a better contribution to the community in a future conference.",0
"I really appreciate the directions the authors are taken and I think something quite interesting can come out of it. I hope the authors continue on this path and are able to come up with something quite interesting soon. However I feel this paper right now is not quite ready. Is not clear to me what the results of this work are yet. The preimage construction is not obviously (at least not to me) helpful. It feels like the right direction, but it didn't got to a point where we can use it to identify the underlying mechanism behind our models. We know relu models need to split apart and unite different region of the space, and I think we can agree that we can construct such mechanism (it comes from the fact that relu models are universal approximators) .. though this doesn't speak to what happens in practice.  All in all I think this work needs a bit more work yet.",0
"I really appreciate the directions the authors are taken and I think something quite interesting can come out of it. I hope the authors continue on this path and are able to come up with something quite interesting soon. However I feel this paper right now is not quite ready. Is not clear to me what the results of this work are yet. The preimage construction is not obviously (at least not to me) helpful. It feels like the right direction, but it didn't got to a point where we can use it to identify the underlying mechanism behind our models. We know relu models need to split apart and unite different region of the space, and I think we can agree that we can construct such mechanism (it comes from the fact that relu models are universal approximators) .. though this doesn't speak to what happens in practice.  All in all I think this work needs a bit more work yet.",0
"I have problems understanding the motivation of this paper. The authors claimed to have captured a latent representation of text and image during training and can translate better without images at test time, but didn't demonstrate convincingly that images help (not to mention the setup is a bit strange when there are no images at test time). What I see are only speculative comments: ""we observed some gains, so these should come from our image models"". The qualitative analysis doesn't convince me that the models have learned latent representations; I am guessing the gains are due to less overfitting because of the participation of images during training. 

The dataset is too small to experiment with NMT. I'm not sure if it's fair to compare their models with NMT and VNMT given the following description in Section 4.1 ""VNMT is fine-tuned by NMT and our models are fine-tuned with VNMT"". There should be more explanation on this.

Besides, I have problems with the presentation of this paper.
(a) There are many symbols being used unnecessary. For example: f & g are used for x (source) and y (target) in Section 3.1. 
(b) The ' symbol is not being used in a consistent manner, making it sometimes hard to follow the paper. For example, in section 3.1.2, there are references about h'_\pi obtained from Eq. (3) which is about h_\pi (yes, I understand what the authors mean, but there can be better ways to present that).
(c) I'm not sure if it's correct in Section 3.2.2 h'_z is computed from \mu and \sigma. So how \mu' and \sigma' are being used ?
(d) G+O-AVG should be something like G+O_{AVG}. The minus sign makes it looks like there's an ablation test there. Similarly for other symbols.

Other things: no explanations for Figure 2 & 3. There's a missing \pi symbol in Appendix A before the KL derivation.",0
"I have problems understanding the motivation of this paper. The authors claimed to have captured a latent representation of text and image during training and can translate better without images at test time, but didn't demonstrate convincingly that images help (not to mention the setup is a bit strange when there are no images at test time). What I see are only speculative comments: ""we observed some gains, so these should come from our image models"". The qualitative analysis doesn't convince me that the models have learned latent representations; I am guessing the gains are due to less overfitting because of the participation of images during training. 

The dataset is too small to experiment with NMT. I'm not sure if it's fair to compare their models with NMT and VNMT given the following description in Section 4.1 ""VNMT is fine-tuned by NMT and our models are fine-tuned with VNMT"". There should be more explanation on this.

Besides, I have problems with the presentation of this paper.
(a) There are many symbols being used unnecessary. For example: f & g are used for x (source) and y (target) in Section 3.1. 
(b) The ' symbol is not being used in a consistent manner, making it sometimes hard to follow the paper. For example, in section 3.1.2, there are references about h'_\pi obtained from Eq. (3) which is about h_\pi (yes, I understand what the authors mean, but there can be better ways to present that).
(c) I'm not sure if it's correct in Section 3.2.2 h'_z is computed from \mu and \sigma. So how \mu' and \sigma' are being used ?
(d) G+O-AVG should be something like G+O_{AVG}. The minus sign makes it looks like there's an ablation test there. Similarly for other symbols.

Other things: no explanations for Figure 2 & 3. There's a missing \pi symbol in Appendix A before the KL derivation.",0
"This paper develops Submodular Sum Product Networks (SSPNs) and
an efficient inference algorithm for approximately computing the
most probable labeling of variables in the model. The main
application in the paper is on scene parsing. In this context,
SSPNs define an energy function with a grammar component for
representing a hierarchy of labels and an MRF for encoding
smoothness of labels over space. To perform inference, the
authors develop a move-making algorithm, somewhat in the spirit
of fusion moves (Lempitsky et al., 2010) that repeatedly improves
a solution by considering a large neighborhood of alternative segmentations
and solving an optimization problem to choose the best neighbor.
Empirical results show that the proposed algorithm achieves better
energy that belief propagation of alpha expansion and is much faster.

This is generally a well-executed paper. The model is interesting
and clearly defined, the algorithm is well presented with proper
analysis of the relevant runtimes and guarantees on the
behavior. Overall, the algorithm seems effective at minimizing
the energy of SSPN models.

Having said that, I don't think this paper is a great fit for
ICLR. The model is even somewhat to the antithesis of the idea of
learning representations, in that a highly structured form of
energy function is asserted by the human modeller, and then
inference is performed. I don't see the connection to learning
representations. One additional issue is that while the proposed
algorithm is faster than alternatives, the times are still on the
order of 1-287 seconds per image, which means that the
applicability of this method (as is) to something like training
ConvNets is limited.

Finally, there is no attempt to argue that the model produces
better segmentations than alternative models. The only
evaluations in the paper are on energy values achieved and on
training data.

So overall I think this is a good paper that should be published
at a good machine learning conference, but I don't think ICLR is
the right fit.",0
"This paper develops Submodular Sum Product Networks (SSPNs) and
an efficient inference algorithm for approximately computing the
most probable labeling of variables in the model. The main
application in the paper is on scene parsing. In this context,
SSPNs define an energy function with a grammar component for
representing a hierarchy of labels and an MRF for encoding
smoothness of labels over space. To perform inference, the
authors develop a move-making algorithm, somewhat in the spirit
of fusion moves (Lempitsky et al., 2010) that repeatedly improves
a solution by considering a large neighborhood of alternative segmentations
and solving an optimization problem to choose the best neighbor.
Empirical results show that the proposed algorithm achieves better
energy that belief propagation of alpha expansion and is much faster.

This is generally a well-executed paper. The model is interesting
and clearly defined, the algorithm is well presented with proper
analysis of the relevant runtimes and guarantees on the
behavior. Overall, the algorithm seems effective at minimizing
the energy of SSPN models.

Having said that, I don't think this paper is a great fit for
ICLR. The model is even somewhat to the antithesis of the idea of
learning representations, in that a highly structured form of
energy function is asserted by the human modeller, and then
inference is performed. I don't see the connection to learning
representations. One additional issue is that while the proposed
algorithm is faster than alternatives, the times are still on the
order of 1-287 seconds per image, which means that the
applicability of this method (as is) to something like training
ConvNets is limited.

Finally, there is no attempt to argue that the model produces
better segmentations than alternative models. The only
evaluations in the paper are on energy values achieved and on
training data.

So overall I think this is a good paper that should be published
at a good machine learning conference, but I don't think ICLR is
the right fit.",0
"The paper reports several connections between the image representations in state-of-the are object recognition networks and findings from human visual psychophysics:
1) It shows that the mean L1 distance in the feature space of certain CNN layers is predictive of human noise-detection thresholds in natural images.
2) It reports that for 3 different 2-AFC tasks for which there exists a condition that is hard and one that is easy for humans, the mutual information between decision label and quantised CNN activations is usually higher in the condition that is easier for humans.
3) It reproduces the general bandpass nature of contrast/frequency detection sensitivity in humans. 

While these findings appear interesting, they are also rather anecdotal and some of them seem to be rather trivial (e.g. findings in 2). To make a convincing statement it would be important to explore what aspects of the CNN lead to the reported findings. One possible way of doing that could be to include good baseline models to compare against. As I mentioned before, one such baseline should be reasonable low-level vision model. Another interesting direction would be to compare the results for the same network at different training stages.

In that way one might be able to find out which parts of the reported results can be reproduced by simple low-level image processing systems,  which parts are due to the general deep network’s architecture and which parts arise from the powerful computational properties (object recognition performance) of the CNNs.

In conclusion, I believe that establishing correspondences between state-of-the art CNNs and human vision is a potentially fruitful approach. However to make a convincing point that found correspondences are non-trivial, it is crucial to show that non-trivial aspects of the CNN lead to the reported findings, which was not done. Therefore, the contribution of the paper is limited since I cannot judge whether the findings really tell me something about a unique relation between high-performing CNNs and the human visual system.

UPDATE:

Thank you very much for your extensive revision and inclusion of several of the suggested baselines. 
The results of the baseline models often raise more questions and make the interpretation of the results more complex, but I feel that this reflects the complexity of the topic and makes the work rather more worthwhile. 

One further suggestion: As the experiments with the snapshots of the CaffeNet shows, the direct relationship between CNN performance and prediction accuracy of biological vision known from Yamins et al. 2014 and Cadieu et al. 2014 does not necessarily hold in your experiments. I think this should be discussed somewhere in the paper.

All in all, I think that the paper now constitutes a decent contribution relating state-of-the art CNNs to human psychophysics and I would be happy for this work to be accepted.

I raise the my rating for this paper to 7.",0
"The paper reports several connections between the image representations in state-of-the are object recognition networks and findings from human visual psychophysics:
1) It shows that the mean L1 distance in the feature space of certain CNN layers is predictive of human noise-detection thresholds in natural images.
2) It reports that for 3 different 2-AFC tasks for which there exists a condition that is hard and one that is easy for humans, the mutual information between decision label and quantised CNN activations is usually higher in the condition that is easier for humans.
3) It reproduces the general bandpass nature of contrast/frequency detection sensitivity in humans. 

While these findings appear interesting, they are also rather anecdotal and some of them seem to be rather trivial (e.g. findings in 2). To make a convincing statement it would be important to explore what aspects of the CNN lead to the reported findings. One possible way of doing that could be to include good baseline models to compare against. As I mentioned before, one such baseline should be reasonable low-level vision model. Another interesting direction would be to compare the results for the same network at different training stages.

In that way one might be able to find out which parts of the reported results can be reproduced by simple low-level image processing systems,  which parts are due to the general deep network’s architecture and which parts arise from the powerful computational properties (object recognition performance) of the CNNs.

In conclusion, I believe that establishing correspondences between state-of-the art CNNs and human vision is a potentially fruitful approach. However to make a convincing point that found correspondences are non-trivial, it is crucial to show that non-trivial aspects of the CNN lead to the reported findings, which was not done. Therefore, the contribution of the paper is limited since I cannot judge whether the findings really tell me something about a unique relation between high-performing CNNs and the human visual system.

UPDATE:

Thank you very much for your extensive revision and inclusion of several of the suggested baselines. 
The results of the baseline models often raise more questions and make the interpretation of the results more complex, but I feel that this reflects the complexity of the topic and makes the work rather more worthwhile. 

One further suggestion: As the experiments with the snapshots of the CaffeNet shows, the direct relationship between CNN performance and prediction accuracy of biological vision known from Yamins et al. 2014 and Cadieu et al. 2014 does not necessarily hold in your experiments. I think this should be discussed somewhere in the paper.

All in all, I think that the paper now constitutes a decent contribution relating state-of-the art CNNs to human psychophysics and I would be happy for this work to be accepted.

I raise the my rating for this paper to 7.",0
"This work explores the neural models for sentence summarisation by using a read-again attention model and a copy mechanism which grants the ability of direct copying word representations from the source sentences. The experiments demonstrate the model achieved better results on DUC dataset. Overall, this paper is not well-written. There are confusing points, some of the claims are lack of evidence and the experimental results are incomplete. 

Detailed comments:
 
-Read-again attention. How does it work better than a vanilla attention? What would happen if you read the same sentences multiple times? Have you compared it with staked LSTM (with same number of parameters)? There is no model ablation in the experiment section. 

-Why do you need reading two sentences? The Gigaword dataset is a source-to-compression dataset which does not need multiple input sentences. How do you compare your model with single sent input and two sent input?

-Copy mechanism. What if there are multiple same words appeared in the source sentences to be copied? According to equation (5), you only copy one vector to the decoder. However, there is no this kind of issue for a hard copy mechanism. Besides, there is no comparison between the hard copy mechanism and this vector copy mechanism in the experiment section

-Vocabulary size. This part is a bit off the main track of this paper. If there is no evidence showing this is the special property of vector copy mechanism, it would be trivial in this paper. 

-Experiments. On the DUC dataset, it compares the model with other up-to-date models, while on the Gigaword dataset paper only compares the model with the ABS Rush et al. (2015) and the GRU (?), which are quite weak baseline models. It is irresponsible to claim this model achieved the state-of-the-art performance in the context of summarization.

Typos: (1) Tab. 1. -> Table 1. (2) Fig. 3.1.2.?",0
"This work explores the neural models for sentence summarisation by using a read-again attention model and a copy mechanism which grants the ability of direct copying word representations from the source sentences. The experiments demonstrate the model achieved better results on DUC dataset. Overall, this paper is not well-written. There are confusing points, some of the claims are lack of evidence and the experimental results are incomplete. 

Detailed comments:
 
-Read-again attention. How does it work better than a vanilla attention? What would happen if you read the same sentences multiple times? Have you compared it with staked LSTM (with same number of parameters)? There is no model ablation in the experiment section. 

-Why do you need reading two sentences? The Gigaword dataset is a source-to-compression dataset which does not need multiple input sentences. How do you compare your model with single sent input and two sent input?

-Copy mechanism. What if there are multiple same words appeared in the source sentences to be copied? According to equation (5), you only copy one vector to the decoder. However, there is no this kind of issue for a hard copy mechanism. Besides, there is no comparison between the hard copy mechanism and this vector copy mechanism in the experiment section

-Vocabulary size. This part is a bit off the main track of this paper. If there is no evidence showing this is the special property of vector copy mechanism, it would be trivial in this paper. 

-Experiments. On the DUC dataset, it compares the model with other up-to-date models, while on the Gigaword dataset paper only compares the model with the ABS Rush et al. (2015) and the GRU (?), which are quite weak baseline models. It is irresponsible to claim this model achieved the state-of-the-art performance in the context of summarization.

Typos: (1) Tab. 1. -> Table 1. (2) Fig. 3.1.2.?",0
"SUMMARY.

The paper propose a new scoring function for knowledge base embedding.
The scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.
The proposed function is tested on two tasks knowledge-base completion and question answering.

----------

OVERALL JUDGMENT
While I think this proposed work is very interesting and it is an idea worth to explore further, the presentation and the experimental section of the paper have some problems.
Regarding the presentation, as far as I understand this is not an attention model as intended standardly in the literature.
Plus, it has hardly anything to share with memory networks/neural Turing machines, the parallel that the authors try to make is not very convincing.
Regarding the experimental section, for a fair comparison the authors should test their model on standard benchmarks, reporting state-of-the-art models.
Finally, the paper lack of discussion of results and insights on the behavior of the proposed model.


----------

DETAILED COMMENTS


In section 2.2 when the authors calculate \mu_{context} do not they loose the order of relations? And if it is so, does it make any sense?",0
"SUMMARY.

The paper propose a new scoring function for knowledge base embedding.
The scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.
The proposed function is tested on two tasks knowledge-base completion and question answering.

----------

OVERALL JUDGMENT
While I think this proposed work is very interesting and it is an idea worth to explore further, the presentation and the experimental section of the paper have some problems.
Regarding the presentation, as far as I understand this is not an attention model as intended standardly in the literature.
Plus, it has hardly anything to share with memory networks/neural Turing machines, the parallel that the authors try to make is not very convincing.
Regarding the experimental section, for a fair comparison the authors should test their model on standard benchmarks, reporting state-of-the-art models.
Finally, the paper lack of discussion of results and insights on the behavior of the proposed model.


----------

DETAILED COMMENTS


In section 2.2 when the authors calculate \mu_{context} do not they loose the order of relations? And if it is so, does it make any sense?",0
"This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks.  In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network.

This is an interesting approach for coupling the training of these two models.  The paper however is quite weak on the empirical studies.  In particular:
- The training datasets are tiny, from sets of 1 image to 5-6.  What is the reason for not using larger sets?  I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach.
- For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.  There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing.  Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks.

Another comment is that in the “related work” section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included.

Despite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR.  Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues.",0
"This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks.  In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network.

This is an interesting approach for coupling the training of these two models.  The paper however is quite weak on the empirical studies.  In particular:
- The training datasets are tiny, from sets of 1 image to 5-6.  What is the reason for not using larger sets?  I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach.
- For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.  There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing.  Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks.

Another comment is that in the “related work” section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included.

Despite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR.  Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues.",0
"This paper proposes the Layerwise Origin Target Synthesis (LOTS) method, which entails computing a difference in representation at a given layer in a neural network and then projecting that difference back to input space using backprop. Two types of differences are explored: linear scalings of a single input’s representation and difference vectors between representations of two inputs, where the inputs are of different classes.

In the former case, the LOTS method is used as a visualization of the representation of a specific input example, showing what it would mean, in input space, for the feature representation to be supressed or magnified. While it’s an interesting computation to perform, the value of the visualizations is not very clear.

In the latter case, LOTS is used to generate adversarial examples, moving from an origin image just far enough toward a target image to cause the classification to flip. As expected, the changes required are smaller when LOTS targets a higher layer (in the limit of targetting the last layer, results similar to the original adversarial image results would be obtained).

The paper is an interesting basic exploration and would probably be a great workshop paper. However, the results are probably not quite compelling enough to warrant a full ICLR paper.

A few suggestions for improvement:
 - Several times it is claimed that LOTS can be used as a method for mining for diverse adversarial examples that could be used in training classifiers more robust to adversarial perturbation. But this simple experiment of training on LOTS generated examples isn’t tried. Showing whether the LOTS method outperforms, say, FGS would go a long way toward making a strong paper.
 - How many layers are in the networks used in the paper, and what is their internal structure? This isn’t stated anywhere. I was left wondering whether, say, in Fig 2 the CONV2_1 layer was immediately after the CONV1_1 layer and whether the FC8 layer was the last layer in the network.
 - In Fig 1, 2, 3, and 4, results of the application of LOTS are shown for many intermediate layers but miss for some reason applying it to the input (data) layer and the output/classification (softmax) layer. Showing the full range of possible results would reinforce the interpreatation (for example, in Fig 3, are even larger perturbations necessary in pixel space vs CONV1 space? And does operating directly in softmax space result in smaller perturbations than IP2?)
 - The PASS score is mentioned a couple times but never explained at all. E.g. Fig 1 makes use of it but does not specify such basics as whether higher or lower PASS scores are associated with more or less severe perturbations. A basic explanation would be great.
 - 4.2 states “In summary, the visualized internal feature representations of the origin suggest that lower convolutional layers of the VGG Face model have managed to learn and capture features that provide semantically meaningful and interpretable representations to human observers.” I don’t see that this follows from any results. If this is an important claim to the paper, it should be backed up by additional arguments or results.



1/19/17 UPDATE AFTER REBUTTAL:
Given that experiments were added to the latest version of the paper, I'm increasing my review from 5 -> 6. I think the paper is now just on the accept side of the threshold.",0
"This paper proposes the Layerwise Origin Target Synthesis (LOTS) method, which entails computing a difference in representation at a given layer in a neural network and then projecting that difference back to input space using backprop. Two types of differences are explored: linear scalings of a single input’s representation and difference vectors between representations of two inputs, where the inputs are of different classes.

In the former case, the LOTS method is used as a visualization of the representation of a specific input example, showing what it would mean, in input space, for the feature representation to be supressed or magnified. While it’s an interesting computation to perform, the value of the visualizations is not very clear.

In the latter case, LOTS is used to generate adversarial examples, moving from an origin image just far enough toward a target image to cause the classification to flip. As expected, the changes required are smaller when LOTS targets a higher layer (in the limit of targetting the last layer, results similar to the original adversarial image results would be obtained).

The paper is an interesting basic exploration and would probably be a great workshop paper. However, the results are probably not quite compelling enough to warrant a full ICLR paper.

A few suggestions for improvement:
 - Several times it is claimed that LOTS can be used as a method for mining for diverse adversarial examples that could be used in training classifiers more robust to adversarial perturbation. But this simple experiment of training on LOTS generated examples isn’t tried. Showing whether the LOTS method outperforms, say, FGS would go a long way toward making a strong paper.
 - How many layers are in the networks used in the paper, and what is their internal structure? This isn’t stated anywhere. I was left wondering whether, say, in Fig 2 the CONV2_1 layer was immediately after the CONV1_1 layer and whether the FC8 layer was the last layer in the network.
 - In Fig 1, 2, 3, and 4, results of the application of LOTS are shown for many intermediate layers but miss for some reason applying it to the input (data) layer and the output/classification (softmax) layer. Showing the full range of possible results would reinforce the interpreatation (for example, in Fig 3, are even larger perturbations necessary in pixel space vs CONV1 space? And does operating directly in softmax space result in smaller perturbations than IP2?)
 - The PASS score is mentioned a couple times but never explained at all. E.g. Fig 1 makes use of it but does not specify such basics as whether higher or lower PASS scores are associated with more or less severe perturbations. A basic explanation would be great.
 - 4.2 states “In summary, the visualized internal feature representations of the origin suggest that lower convolutional layers of the VGG Face model have managed to learn and capture features that provide semantically meaningful and interpretable representations to human observers.” I don’t see that this follows from any results. If this is an important claim to the paper, it should be backed up by additional arguments or results.



1/19/17 UPDATE AFTER REBUTTAL:
Given that experiments were added to the latest version of the paper, I'm increasing my review from 5 -> 6. I think the paper is now just on the accept side of the threshold.",0
"This paper proposes new initialization for particular architectures and a correction trick to batch normalization to correct variance introduced by dropout. While authors state interesting observations, the claims are not supported with convincing results.

I guess Figure 1 is only for mnist and for only two values of p with one particular network architecture, the dataset and empirical setup is not clear.

The convergence is demonstrated only for three dropout values in Figure 2 which may cause an unfair comparison. For instance how does the convergence compare for the best dropout rate after cross-validation (three figures each figure has three results for one method with different dropouts [bests cv result for each one])? Also how is the corresponding validation error and test iterations?  Also only mnist does not have to generalize to other benchmarks.

Figure 3 gives closer results for Adam optimizer, learning rate is not selected with random search or bayesian optimization, learning decay iterations fixed and regularization coefficient is set to a small value without tuning. A slightly better tuning of parameters may close the current gap. Also Nesterov based competitor gives unreasonably worse accuracy compared to recent results which may indicate that this experiment should not be taken into account. 

In Table 2, there is no significant improvement on CIFAR10. The CIFAR100 difference is not significant without including batch normalization variance re-estimation. However there is no result for 'original with BN update' therefore it is not clear whether the BN update helps in general or not. SVHN also does not have result for original with BN update.

There should be baselines with batch normalizations for Figure 1,2 3 to support the claims convincingly.  The main criticism about batch normalization is additional computational cost by giving (Mishkin et al, 2016 ) as reference however this should not be a reason to not to compare the initialization to batch-normalization.  In fact, (Mishkin et al, 2016) performs comparison to batch normalization and also with and without data augmentation with recent state of art architectures.

None of the empirical results have data augmentation. It is not clear if the initialization or  batch normalization update will help or make it worse for that case.

Recent state of art methods methods like Res Net variant and Dense Net scale to many depths and report result for ImageNet. Although the authors claim that this can be extended to residual network variants, it is not clear if there is going to be any empirical gain for that architectures.  

This work requires a comprehensive and fair comparison. Otherwise the contribution is not significant.",0
"This paper proposes new initialization for particular architectures and a correction trick to batch normalization to correct variance introduced by dropout. While authors state interesting observations, the claims are not supported with convincing results.

I guess Figure 1 is only for mnist and for only two values of p with one particular network architecture, the dataset and empirical setup is not clear.

The convergence is demonstrated only for three dropout values in Figure 2 which may cause an unfair comparison. For instance how does the convergence compare for the best dropout rate after cross-validation (three figures each figure has three results for one method with different dropouts [bests cv result for each one])? Also how is the corresponding validation error and test iterations?  Also only mnist does not have to generalize to other benchmarks.

Figure 3 gives closer results for Adam optimizer, learning rate is not selected with random search or bayesian optimization, learning decay iterations fixed and regularization coefficient is set to a small value without tuning. A slightly better tuning of parameters may close the current gap. Also Nesterov based competitor gives unreasonably worse accuracy compared to recent results which may indicate that this experiment should not be taken into account. 

In Table 2, there is no significant improvement on CIFAR10. The CIFAR100 difference is not significant without including batch normalization variance re-estimation. However there is no result for 'original with BN update' therefore it is not clear whether the BN update helps in general or not. SVHN also does not have result for original with BN update.

There should be baselines with batch normalizations for Figure 1,2 3 to support the claims convincingly.  The main criticism about batch normalization is additional computational cost by giving (Mishkin et al, 2016 ) as reference however this should not be a reason to not to compare the initialization to batch-normalization.  In fact, (Mishkin et al, 2016) performs comparison to batch normalization and also with and without data augmentation with recent state of art architectures.

None of the empirical results have data augmentation. It is not clear if the initialization or  batch normalization update will help or make it worse for that case.

Recent state of art methods methods like Res Net variant and Dense Net scale to many depths and report result for ImageNet. Although the authors claim that this can be extended to residual network variants, it is not clear if there is going to be any empirical gain for that architectures.  

This work requires a comprehensive and fair comparison. Otherwise the contribution is not significant.",0
"This paper aims to mine explicit rules from KB embedding space, and casts it into a sparse reconstruction problem. Experiments demonstrate its ability of extracting reasonable rules on a few link prediction datasets.

The solution part sounds plausible. However, it confuses me that why we need to mine rules from learned KB embeddings. 

- It is still unclear what information these KB embeddings encode and it looks strange that we aim to learn rules including negation / disjunction from them.

- If the goal is to extract useful rules (for other applications), it is necessary to compare it to “graph random walk” (",0
"This paper aims to mine explicit rules from KB embedding space, and casts it into a sparse reconstruction problem. Experiments demonstrate its ability of extracting reasonable rules on a few link prediction datasets.

The solution part sounds plausible. However, it confuses me that why we need to mine rules from learned KB embeddings. 

- It is still unclear what information these KB embeddings encode and it looks strange that we aim to learn rules including negation / disjunction from them.

- If the goal is to extract useful rules (for other applications), it is necessary to compare it to “graph random walk” (",0
"In this paper, the authors propose a Bayesian variant of the skipgram model to learn word embeddings. There are two important variant compared to the original model. First, aligned sentences from multiple languages are used to train the model. Therefore, the context words of a given target word can be either from the same sentence, or from an aligned sentence in a different language. This allows to learn multilingual embedding. The second difference is that each word is represented by multiple vectors, one for each of its different senses. A latent variable z models which sense should be used, given the context.

Overall, I believe that the idea of using a probabilistic model to capture polysemy is an interesting idea. The model introduced in this paper is a nice generalization of the skipgram model in that direction. However, I found the paper a bit hard to follow. The formulation might probably be simplified (e.g. why not consider a target word w and a context c, where c is either in the source or target language. Since all factors are independent, this should not change the model much, and would make the presentation easier). The performance of all models reported in Table 2 & 3 seem pretty low.

Overall, I like the main idea of the paper, which is to represent word senses by latent variables in a probabilistic model. I feel that the method could be presented more clearly, which would make the paper much stronger. I also have some concerns regarding the experimental results.

Pros:
Interesting extension of skipgram to capture polysemy.
Cons:
The paper is not clearly written.
Results reported in the paper seems pretty low.",0
"In this paper, the authors propose a Bayesian variant of the skipgram model to learn word embeddings. There are two important variant compared to the original model. First, aligned sentences from multiple languages are used to train the model. Therefore, the context words of a given target word can be either from the same sentence, or from an aligned sentence in a different language. This allows to learn multilingual embedding. The second difference is that each word is represented by multiple vectors, one for each of its different senses. A latent variable z models which sense should be used, given the context.

Overall, I believe that the idea of using a probabilistic model to capture polysemy is an interesting idea. The model introduced in this paper is a nice generalization of the skipgram model in that direction. However, I found the paper a bit hard to follow. The formulation might probably be simplified (e.g. why not consider a target word w and a context c, where c is either in the source or target language. Since all factors are independent, this should not change the model much, and would make the presentation easier). The performance of all models reported in Table 2 & 3 seem pretty low.

Overall, I like the main idea of the paper, which is to represent word senses by latent variables in a probabilistic model. I feel that the method could be presented more clearly, which would make the paper much stronger. I also have some concerns regarding the experimental results.

Pros:
Interesting extension of skipgram to capture polysemy.
Cons:
The paper is not clearly written.
Results reported in the paper seems pretty low.",0
"The authors propose methods for wild variational inference, in which the
variational approximating distribution may not have a directly accessible
density function. Their approach is based on the Stain's operator, which acts
on a given function and returns a zero mean function with respect to a given
density function which may not be normalized.

Quality:

The derviations seem to be technically sound. However, my impression is that
the authors are not very careful and honest at evaluating both the strengths
and weaknesses of the proposed work. How does the method perform in cases in
which the distribution to be approximated is high dimensional? The logistic
regression problem considered only has 54 dimensions. How would this method
perform in a neural network in which the number of weights is goint to be way
much larger? The logistic regression model is rather simple and its posterior
will be likely to be close to Gaussian. How would the method perform in more
complicated posteriors such as the ones of Bayesia neural networks?

Clarity:

The paper is not clearly written. I found it very really hard to follow and not
focused. The authors describe way too many methods: 1) Stein's variational
gradient descent (SVGD), 2) Amortized SVGD, 3) Kernelized Stein discrepancy
(KSD), 4) Lavengin inference network, not to mention the introduction to
Stein's discrepancy. I found very difficult to indentify the clear
contributions of the paper with so many different techniques.

Originality:

It is not clear how original the proposed contributions are. The first of the
proposed methods is also discussed in

Wang, Dilin and Liu, Qiang. Learning to draw samples: With application to
amortized mle for generative adversarial learning. Submitted to ICLR 2017, 2016

How does this work differ from that one?

Significance:

It is very hard to evaluate the importance of proposed methods. The authors
only report results on a 1d toy problem with a mixture of Gaussians and on a
logistic regression model with dimension 54. In both cases the distributions to
be approximated are very simple and of low dimension. In the regression case
the posterior is also likely to be close to Gaussian and therefore not clear
what advances the proposed method would provide with respect to other more
simple approaches. The authors do not compare with simple variational
approaches based on Gaussian approximations.",0
"The authors propose methods for wild variational inference, in which the
variational approximating distribution may not have a directly accessible
density function. Their approach is based on the Stain's operator, which acts
on a given function and returns a zero mean function with respect to a given
density function which may not be normalized.

Quality:

The derviations seem to be technically sound. However, my impression is that
the authors are not very careful and honest at evaluating both the strengths
and weaknesses of the proposed work. How does the method perform in cases in
which the distribution to be approximated is high dimensional? The logistic
regression problem considered only has 54 dimensions. How would this method
perform in a neural network in which the number of weights is goint to be way
much larger? The logistic regression model is rather simple and its posterior
will be likely to be close to Gaussian. How would the method perform in more
complicated posteriors such as the ones of Bayesia neural networks?

Clarity:

The paper is not clearly written. I found it very really hard to follow and not
focused. The authors describe way too many methods: 1) Stein's variational
gradient descent (SVGD), 2) Amortized SVGD, 3) Kernelized Stein discrepancy
(KSD), 4) Lavengin inference network, not to mention the introduction to
Stein's discrepancy. I found very difficult to indentify the clear
contributions of the paper with so many different techniques.

Originality:

It is not clear how original the proposed contributions are. The first of the
proposed methods is also discussed in

Wang, Dilin and Liu, Qiang. Learning to draw samples: With application to
amortized mle for generative adversarial learning. Submitted to ICLR 2017, 2016

How does this work differ from that one?

Significance:

It is very hard to evaluate the importance of proposed methods. The authors
only report results on a 1d toy problem with a mixture of Gaussians and on a
logistic regression model with dimension 54. In both cases the distributions to
be approximated are very simple and of low dimension. In the regression case
the posterior is also likely to be close to Gaussian and therefore not clear
what advances the proposed method would provide with respect to other more
simple approaches. The authors do not compare with simple variational
approaches based on Gaussian approximations.",0
"The paper provides a first study of customized precision hardware for large convolutional networks, namely alexnet, vgg and googlenet. It shows that it is possible to achieve larger speed-ups using floating-point precision (up to 7x) when using fewer bits, and better than using fixed-point representations. 

The paper also explores predicting custom floating-point precision parameters directly from the neural network activations, avoiding exhaustive search, but i could not follow this part. Only the activations of the last layer are evaluated, but on what data ? On all the validation set ? Why would this be faster than computing the classification accuracy ?

The results should be useful for hardware manufacturers, but with a catch. All popular convolutional networks now use batch normalization, while none of the evaluated ones do. It may well be that the conclusions of this study will be completely different on batch normalization networks, and fixed-point representations are best there, but that remains to be seen. It seems like something worth exploring.

Overall there is not a great deal of novelty other than being a useful study on numerical precision trade-offs at neural network test time. Training time is also something of interest. There are a lot more researchers trying to train new networks fast than trying to evaluate old ones fast. 

I am also no expert in digital logic design, but my educated guess is that this paper is marginally below the acceptance threshold.",0
"The paper provides a first study of customized precision hardware for large convolutional networks, namely alexnet, vgg and googlenet. It shows that it is possible to achieve larger speed-ups using floating-point precision (up to 7x) when using fewer bits, and better than using fixed-point representations. 

The paper also explores predicting custom floating-point precision parameters directly from the neural network activations, avoiding exhaustive search, but i could not follow this part. Only the activations of the last layer are evaluated, but on what data ? On all the validation set ? Why would this be faster than computing the classification accuracy ?

The results should be useful for hardware manufacturers, but with a catch. All popular convolutional networks now use batch normalization, while none of the evaluated ones do. It may well be that the conclusions of this study will be completely different on batch normalization networks, and fixed-point representations are best there, but that remains to be seen. It seems like something worth exploring.

Overall there is not a great deal of novelty other than being a useful study on numerical precision trade-offs at neural network test time. Training time is also something of interest. There are a lot more researchers trying to train new networks fast than trying to evaluate old ones fast. 

I am also no expert in digital logic design, but my educated guess is that this paper is marginally below the acceptance threshold.",0
"I'd like to thank the authors for their detailed response and clarifications.

This work proposes new training scheme for online sparse dictionary learning. The model assumes a non-stationary flow of the incoming data. The goal (and the challenge) is to learn a model in an online manner in a way that is capable of  adjusting to the new incoming data without forgetting how to represent previously seen data. The proposed approach deals with this problem by incorporating a mechanism for adding or deleting atoms in the dictionary. This procedure is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus. 

The paper has two main innovations over the baseline approach (Mairal et al): (i) “neuronal birth” which represents an adaptive way of increasing the number of atoms in the dictionary (ii) ""neuronal death"", which corresponds to removing “useless” dictionary atoms.

Neural death is implemented by including an group-sparsity regularization to the dictionary atoms themselves (the group corresponds to a column of the dictionary). This promotes to shrink to zero atoms that are not very useful, keeping controlled the increase of the dictionary size.

I believe that the strong side of the paper is its connections with the adult neurogenesis phenomenon, which is, in my opinion a very nice feature.
The paper is very well written and easy to follow.

On the other hand, the overall technique is not very novel. Although not exactly equivalent, similar ideas have been explored. While the neural death is implemente elegantly with a sparsity-promoting regularization term, the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data. Which depending on the ""level"" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set. Still, having adaptive dictionary size is very interesting.

The authors could also cite some references in model selection literature. In particular, some ideas such as MDL have been used for automatically selecting the dictionary size (I believe this work does not address the online setting, but still its a relevant reference to have). For instance,

Ramirez, Ignacio, and Guillermo Sapiro. ""An MDL framework for sparse coding and dictionary learning."" IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927.",0
"I'd like to thank the authors for their detailed response and clarifications.

This work proposes new training scheme for online sparse dictionary learning. The model assumes a non-stationary flow of the incoming data. The goal (and the challenge) is to learn a model in an online manner in a way that is capable of  adjusting to the new incoming data without forgetting how to represent previously seen data. The proposed approach deals with this problem by incorporating a mechanism for adding or deleting atoms in the dictionary. This procedure is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus. 

The paper has two main innovations over the baseline approach (Mairal et al): (i) “neuronal birth” which represents an adaptive way of increasing the number of atoms in the dictionary (ii) ""neuronal death"", which corresponds to removing “useless” dictionary atoms.

Neural death is implemented by including an group-sparsity regularization to the dictionary atoms themselves (the group corresponds to a column of the dictionary). This promotes to shrink to zero atoms that are not very useful, keeping controlled the increase of the dictionary size.

I believe that the strong side of the paper is its connections with the adult neurogenesis phenomenon, which is, in my opinion a very nice feature.
The paper is very well written and easy to follow.

On the other hand, the overall technique is not very novel. Although not exactly equivalent, similar ideas have been explored. While the neural death is implemente elegantly with a sparsity-promoting regularization term, the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data. Which depending on the ""level"" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set. Still, having adaptive dictionary size is very interesting.

The authors could also cite some references in model selection literature. In particular, some ideas such as MDL have been used for automatically selecting the dictionary size (I believe this work does not address the online setting, but still its a relevant reference to have). For instance,

Ramirez, Ignacio, and Guillermo Sapiro. ""An MDL framework for sparse coding and dictionary learning."" IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927.",0
"This paper considers the problem of decoding diverge solutions from neural sequence models. It basically adds an additional term to the log-likelihood of standard neural sequence models, and this additional term will encourage the solutions to be diverse. In addition to solve the inference, this paper uses a modified beam search.

On the plus side, there is not much work on producing diverse solutions in RNN/LSTM models. This paper represents one of the few works on this topic. And this paper is well-written and easy to follow.

The novel of this paper is relatively small. There has been a lot of prior work on producing diverse models in the area of probailistic graphical models. Most of them introduce an additional term in the objective function to encourage diversity. From that perspective, the solution proposed in this paper is not that different from previous work. Of course, one can argue that most previous work focues on probabilistic graphical models, while this paper focuses on RNN/LSTM models. But since RNN/LSTM can be simply interpreted as a probabilistic model, I would consider it a small novelty.

The diverse beam search seems to straightforward, i.e. it partitions the beam search space into groups, and does not consider the diversity within group (in order to reduce the search space). To me, this seems to be a simple trick. Note most previous work on diverse solutions in probabilistic graphical models usually involve developing some nontrivial algorithmic solutions, e.g. in order to achieve efficiency. In comparison, the proposed solution in this paper seems to be simplistic for a paper.

The experimental results how improvement over previous methods (Li & Jurafsky, 2015, 2016). But it is hard to say how rigorous the comparisons are, since they are based on the authors' own implementation of (Li & Jurasky, 2015, 2016).

---------------
update: given that the authors made the code available (I do hope the code will remain publicly available), this has alleviated some of my concerns about the rigor of the experiments. I will raise my rate to 6.",0
"This paper considers the problem of decoding diverge solutions from neural sequence models. It basically adds an additional term to the log-likelihood of standard neural sequence models, and this additional term will encourage the solutions to be diverse. In addition to solve the inference, this paper uses a modified beam search.

On the plus side, there is not much work on producing diverse solutions in RNN/LSTM models. This paper represents one of the few works on this topic. And this paper is well-written and easy to follow.

The novel of this paper is relatively small. There has been a lot of prior work on producing diverse models in the area of probailistic graphical models. Most of them introduce an additional term in the objective function to encourage diversity. From that perspective, the solution proposed in this paper is not that different from previous work. Of course, one can argue that most previous work focues on probabilistic graphical models, while this paper focuses on RNN/LSTM models. But since RNN/LSTM can be simply interpreted as a probabilistic model, I would consider it a small novelty.

The diverse beam search seems to straightforward, i.e. it partitions the beam search space into groups, and does not consider the diversity within group (in order to reduce the search space). To me, this seems to be a simple trick. Note most previous work on diverse solutions in probabilistic graphical models usually involve developing some nontrivial algorithmic solutions, e.g. in order to achieve efficiency. In comparison, the proposed solution in this paper seems to be simplistic for a paper.

The experimental results how improvement over previous methods (Li & Jurafsky, 2015, 2016). But it is hard to say how rigorous the comparisons are, since they are based on the authors' own implementation of (Li & Jurasky, 2015, 2016).

---------------
update: given that the authors made the code available (I do hope the code will remain publicly available), this has alleviated some of my concerns about the rigor of the experiments. I will raise my rate to 6.",0
"The proposed method is simple and elegant; it builds upon the huge success of gradient based optimization for deep non-linear function approximators and combines it with established (linear) many-view CCA methods. A major contribution of this paper is the derivation of the gradients with respect to the non-linear encoding networks which project the different views into a common space. The derivation seems correct. In general this approach seems very interesting and I could imagine that it might be applicable to many other similarly structured problems.
The paper is well written; but it could be enhanced with an explicit description of the complete algorithm which also highlights how the joint embeddings G and U are updated. 
 
I don’t have prior experience with CCA-style many-view techniques and it is therefore hard for me to judge the practical/empirical progress presented here. But the experiments seem reasonable convincing; although generally only performed on small and medium sized datasets.
  
Detailed comments: 

The colours or the sign of the x-axis in figure 3b seem to be flipped compared to figure 4.
  
It would be nice to additionally see a continuous (rainbow-coloured) version for Figures 2, 3 and 4 to better identify neighbouring datapoints; but more importantly: I’d like to see how the average reconstruction error between the individual network outputs and the learned representation develop during training.  Is the mismatch between different views on a validation/test-set a useful metric for cross validation? In general, it seems the method is sensitive to regularization and hyperparameter selection  (because it has many more parameters compared to GCCA and different regularization parameters have been chosen for different views) and I wonder if there is a clear metric to optimize these.",0
"The proposed method is simple and elegant; it builds upon the huge success of gradient based optimization for deep non-linear function approximators and combines it with established (linear) many-view CCA methods. A major contribution of this paper is the derivation of the gradients with respect to the non-linear encoding networks which project the different views into a common space. The derivation seems correct. In general this approach seems very interesting and I could imagine that it might be applicable to many other similarly structured problems.
The paper is well written; but it could be enhanced with an explicit description of the complete algorithm which also highlights how the joint embeddings G and U are updated. 
 
I don’t have prior experience with CCA-style many-view techniques and it is therefore hard for me to judge the practical/empirical progress presented here. But the experiments seem reasonable convincing; although generally only performed on small and medium sized datasets.
  
Detailed comments: 

The colours or the sign of the x-axis in figure 3b seem to be flipped compared to figure 4.
  
It would be nice to additionally see a continuous (rainbow-coloured) version for Figures 2, 3 and 4 to better identify neighbouring datapoints; but more importantly: I’d like to see how the average reconstruction error between the individual network outputs and the learned representation develop during training.  Is the mismatch between different views on a validation/test-set a useful metric for cross validation? In general, it seems the method is sensitive to regularization and hyperparameter selection  (because it has many more parameters compared to GCCA and different regularization parameters have been chosen for different views) and I wonder if there is a clear metric to optimize these.",0
"This paper proposes an ""interactive"" version of the bAbI dataset by adding supporting questions/answers to the dataset in cases where there is not enough information to answer the question. Interactive QA is certainly an interesting problem and is well-motivated by the paper. However, I don't feel like the bAbI extension is adequately explained. For example, the baseline DMN and MemN2N models on the IQA task are ""take both statements and question as input and then
estimate an answer."" Their task is then fundamentally more difficult from the CAN's because they do not distinguish ""feedback"" from the original context; perhaps a more fair approach would be to treat **every** question (both supporting and original questions) as individual instances. Also, how were the supporting questions and the user feedback generated? How many templates / words were used to create them? The dataset creation details are missing, and if space is an issue, a lot of basic exposition on things like GRU / sentence encodings can be cut (or at least greatly shortened) and replaced with pointers to the original papers. 

Another issue I had is that the model attempts to generate these synthetic questions; if there are just one or two templates, why not just predict the values that fill these templates? So instead of generating ""Which bedroom, master one or guest one?"" with an RNN decoder, just predict ""which"" or ""which bedroom""... isn't this sufficient? In the end, these just seem like more supporting facts, not actual interaction with users, and the fact that it is run on only three of the original twenty tasks make the conclusions hard to trust.

In conclusion, I think the paper has a strong idea and motivation, but the experiments are not convincing for the paper to be accepted at ICLR.",0
"This paper proposes an ""interactive"" version of the bAbI dataset by adding supporting questions/answers to the dataset in cases where there is not enough information to answer the question. Interactive QA is certainly an interesting problem and is well-motivated by the paper. However, I don't feel like the bAbI extension is adequately explained. For example, the baseline DMN and MemN2N models on the IQA task are ""take both statements and question as input and then
estimate an answer."" Their task is then fundamentally more difficult from the CAN's because they do not distinguish ""feedback"" from the original context; perhaps a more fair approach would be to treat **every** question (both supporting and original questions) as individual instances. Also, how were the supporting questions and the user feedback generated? How many templates / words were used to create them? The dataset creation details are missing, and if space is an issue, a lot of basic exposition on things like GRU / sentence encodings can be cut (or at least greatly shortened) and replaced with pointers to the original papers. 

Another issue I had is that the model attempts to generate these synthetic questions; if there are just one or two templates, why not just predict the values that fill these templates? So instead of generating ""Which bedroom, master one or guest one?"" with an RNN decoder, just predict ""which"" or ""which bedroom""... isn't this sufficient? In the end, these just seem like more supporting facts, not actual interaction with users, and the fact that it is run on only three of the original twenty tasks make the conclusions hard to trust.

In conclusion, I think the paper has a strong idea and motivation, but the experiments are not convincing for the paper to be accepted at ICLR.",0
"This paper presents a method to learn both a model and inference procedure at the same time with recurrent neural networks in the context of inverse problems.
The proposed method is interesting and results are quite good. The paper is also nicely presented. 

I would be happy to see some discussion about what the network learns in practice about natural images in the case of denoising. What are the filters like? Is it particularly sensitive to different structures in images? edges? Also, what is the state in the recurrent unit used for? when are the gates open etc.

Nevertheless, I think this is nice work which should be accepted.",0
"This paper presents a method to learn both a model and inference procedure at the same time with recurrent neural networks in the context of inverse problems.
The proposed method is interesting and results are quite good. The paper is also nicely presented. 

I would be happy to see some discussion about what the network learns in practice about natural images in the case of denoising. What are the filters like? Is it particularly sensitive to different structures in images? edges? Also, what is the state in the recurrent unit used for? when are the gates open etc.

Nevertheless, I think this is nice work which should be accepted.",0
"After rebuttal:

Thanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself, and as the authors mention, it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover, some claims in the paper look wrong in the light of these results, for example:
- ""This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.""
- ""Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.""

These statements, and possibly other parts of the paper, have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.

--------
Initial review:

The paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect, fairly convincing inpaintings are produced.

The proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side, the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete, see below). Overall, I am in the borderline mode, and I will gladly raise the score if the authors address my concerns regarding the experiments.

1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper, but I do not understand why the authors are not willing to simply run their method with AlexNet architecture, although two commenters asked them to do so. Such an experiment would strongly support authors’ claims. Current reasoning that “we thought it reasonable to use more current models while making the difference clear” is not convincing. It is great that better architectures lead to better results, but it is also very important to properly compare to prior work. On a related topic, Doersch et al. also tried using VGG architecture, would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro, ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.

2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance, [Pathak et al 2016]) is missing, and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results, but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at, but they do not add as much to the paper as they could.",0
"After rebuttal:

Thanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself, and as the authors mention, it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover, some claims in the paper look wrong in the light of these results, for example:
- ""This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.""
- ""Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.""

These statements, and possibly other parts of the paper, have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.

--------
Initial review:

The paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect, fairly convincing inpaintings are produced.

The proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side, the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete, see below). Overall, I am in the borderline mode, and I will gladly raise the score if the authors address my concerns regarding the experiments.

1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper, but I do not understand why the authors are not willing to simply run their method with AlexNet architecture, although two commenters asked them to do so. Such an experiment would strongly support authors’ claims. Current reasoning that “we thought it reasonable to use more current models while making the difference clear” is not convincing. It is great that better architectures lead to better results, but it is also very important to properly compare to prior work. On a related topic, Doersch et al. also tried using VGG architecture, would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro, ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.

2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance, [Pathak et al 2016]) is missing, and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results, but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at, but they do not add as much to the paper as they could.",0
"Approaches like adaptive dropout also have the binary mask as a function of input to a neuron very similar to the proposed approach. It is not clear, even from the new draft, how the proposed approach differs to Adaptive dropout in terms of functionality. The experimental validation is also not extensive since comparison to SOTA is not included.",0
"Approaches like adaptive dropout also have the binary mask as a function of input to a neuron very similar to the proposed approach. It is not clear, even from the new draft, how the proposed approach differs to Adaptive dropout in terms of functionality. The experimental validation is also not extensive since comparison to SOTA is not included.",0
"This paper addresses the problem of the influence of mini-batch size on the SGD convergence in a general non-convex setting. The results are then translated to analyze the influence of the number of learners on ASGD. I find the problem addressed in the paper relevant and the theoretical part clearly written. The experimental evaluation is somehow limited though. I would like to see experiments on more data sets and more architectures, as well as richer evaluation, e.g. N=16 is a fairly small experiment. It would also enhance the paper if the experiments were showing a similar behavior of other popular methods like momentum SGD or maybe EASGD (the latter in distributed setting). I understand the last evaluation does not directly lie in the scope of the paper, though adding these few experiments do not require much additional work and should be done.",0
"This paper addresses the problem of the influence of mini-batch size on the SGD convergence in a general non-convex setting. The results are then translated to analyze the influence of the number of learners on ASGD. I find the problem addressed in the paper relevant and the theoretical part clearly written. The experimental evaluation is somehow limited though. I would like to see experiments on more data sets and more architectures, as well as richer evaluation, e.g. N=16 is a fairly small experiment. It would also enhance the paper if the experiments were showing a similar behavior of other popular methods like momentum SGD or maybe EASGD (the latter in distributed setting). I understand the last evaluation does not directly lie in the scope of the paper, though adding these few experiments do not require much additional work and should be done.",0
"This paper proposes a new kind of generative model based on an annealing process, where the transition probabilities are learned directly to maximize a variational lower bound on the log-likelihood. Overall, the idea is clever and appealing, but I think the paper needs more quantitative validation and better discussion of the relationship with prior work.

In terms of prior work, AIS and RAISE are both closely related algorithms, and share much of the mathematical structure with the proposed method. For this reason, it’s not sufficient to mention them in passing in the related work section; those methods and their relationship to variational walkback need to be discussed in detail. If I understand correctly, the proposed method is essentially an extension of RAISE where the transition probabilities are learned rather than fixed based on an existing MRF. I think this is an interesting and worthwhile extension, but the relationship to existing work needs to be clarified.

The analysis of Appendix D seems incorrect. It derives a formula for the ratios of prior and posterior probabilities, but this formula only holds under the assumption of constant temperature (in which case the ratio is very large). When the temperature is varied, the analysis of Neal (2001) applies, and the answer is different. 

One of the main selling points of the method is that it optimizes a variational lower bound on the log-likelihood; even more accurate estimates can be obtained using importance sampling. It ought to be easy to report log-likelihood estimates for this method, so I wonder why such estimates aren’t reported. There are lots of prior results to compare against on MNIST. (In addition, a natural baseline would be RAISE, so that one can check if the ability to learn the transitions actually helps.)

I think the basic idea here is a sound one, so I would be willing to raise my score if the above issues are addressed in a revised version.


Minor comments:

“A recognized obstacle to training undirected graphical models… is that ML training requires sampling from MCMC chains in the inner loop of training, for each example.” This seems like an unfair characterization, since the standard algorithm is PCD, which usually takes only a single step per mini-batch.

Some of the methods discussed in the related work are missing citations.

The method is justified in terms of “carving the energy function in the right direction at each point”, but I’m not sure this is actually what’s happening. Isn’t the point of the method that it can optimize a lower bound on the log-likelihood, and therefore learn a globally correct allocation of probability mass?",0
"This paper proposes a new kind of generative model based on an annealing process, where the transition probabilities are learned directly to maximize a variational lower bound on the log-likelihood. Overall, the idea is clever and appealing, but I think the paper needs more quantitative validation and better discussion of the relationship with prior work.

In terms of prior work, AIS and RAISE are both closely related algorithms, and share much of the mathematical structure with the proposed method. For this reason, it’s not sufficient to mention them in passing in the related work section; those methods and their relationship to variational walkback need to be discussed in detail. If I understand correctly, the proposed method is essentially an extension of RAISE where the transition probabilities are learned rather than fixed based on an existing MRF. I think this is an interesting and worthwhile extension, but the relationship to existing work needs to be clarified.

The analysis of Appendix D seems incorrect. It derives a formula for the ratios of prior and posterior probabilities, but this formula only holds under the assumption of constant temperature (in which case the ratio is very large). When the temperature is varied, the analysis of Neal (2001) applies, and the answer is different. 

One of the main selling points of the method is that it optimizes a variational lower bound on the log-likelihood; even more accurate estimates can be obtained using importance sampling. It ought to be easy to report log-likelihood estimates for this method, so I wonder why such estimates aren’t reported. There are lots of prior results to compare against on MNIST. (In addition, a natural baseline would be RAISE, so that one can check if the ability to learn the transitions actually helps.)

I think the basic idea here is a sound one, so I would be willing to raise my score if the above issues are addressed in a revised version.


Minor comments:

“A recognized obstacle to training undirected graphical models… is that ML training requires sampling from MCMC chains in the inner loop of training, for each example.” This seems like an unfair characterization, since the standard algorithm is PCD, which usually takes only a single step per mini-batch.

Some of the methods discussed in the related work are missing citations.

The method is justified in terms of “carving the energy function in the right direction at each point”, but I’m not sure this is actually what’s happening. Isn’t the point of the method that it can optimize a lower bound on the log-likelihood, and therefore learn a globally correct allocation of probability mass?",0
"This paper proposed to use the BPA criterion for classifier ensembles.

My major concern with the paper is that it attempts to mix quite a few concepts together, and as a result, some of the simple notions becomes a bit hard to understand. For example:

(1) ""Distributed"" in this paper basically means classifier ensembles, and has nothing to do with the distributed training or distributed computation mechanism. Granted, one can train these individual classifiers in a distributed fashion but this is not the point of the paper.

(2) The paper uses ""Transfer learning"" in its narrow sense: it basically means fine-tuning the last layer of a pre-trained classifier.

Aside from the concept mixture of the paper, other comments I have about the paper are:

(1) I am not sure how BPA address class inbalance better than simple re-weighting. Essentially, the BPA criteria is putting equal weights on different classes, regardless of the number of training data points each class has. This is a very easy thing to address in conventional training: adding a class-specific weight term to each data point with the value being the inverse of the number of data points will do.

(2) Algorithm 2 is not presented correctly as it implies that test data is used during training, which is not correct: only training and validation dataset should be used. I find the paper's use of ""train/validation"" and ""test"" quite confusing: why ""train/validation"" is always presented together? How to properly distinguish between them?

(3) If I understand correctly, the paper is proposing to compute the BPA in a batch fashion, i.e. BPA can only be computed when running the model over the full train/validation dataset. This contradicts with the stochastic gradient descent that are usually used in deep net training - how does BPA deal with that? I believe that an experimental report on the computation cost and timing is missing.

In general, I find the paper not presented in its clearest form and a number of key definitions ambiguous.",0
"This paper proposed to use the BPA criterion for classifier ensembles.

My major concern with the paper is that it attempts to mix quite a few concepts together, and as a result, some of the simple notions becomes a bit hard to understand. For example:

(1) ""Distributed"" in this paper basically means classifier ensembles, and has nothing to do with the distributed training or distributed computation mechanism. Granted, one can train these individual classifiers in a distributed fashion but this is not the point of the paper.

(2) The paper uses ""Transfer learning"" in its narrow sense: it basically means fine-tuning the last layer of a pre-trained classifier.

Aside from the concept mixture of the paper, other comments I have about the paper are:

(1) I am not sure how BPA address class inbalance better than simple re-weighting. Essentially, the BPA criteria is putting equal weights on different classes, regardless of the number of training data points each class has. This is a very easy thing to address in conventional training: adding a class-specific weight term to each data point with the value being the inverse of the number of data points will do.

(2) Algorithm 2 is not presented correctly as it implies that test data is used during training, which is not correct: only training and validation dataset should be used. I find the paper's use of ""train/validation"" and ""test"" quite confusing: why ""train/validation"" is always presented together? How to properly distinguish between them?

(3) If I understand correctly, the paper is proposing to compute the BPA in a batch fashion, i.e. BPA can only be computed when running the model over the full train/validation dataset. This contradicts with the stochastic gradient descent that are usually used in deep net training - how does BPA deal with that? I believe that an experimental report on the computation cost and timing is missing.

In general, I find the paper not presented in its clearest form and a number of key definitions ambiguous.",0
"In this paper, the authors explicitly design geometrical structure into a CNN by combining it with a Scattering network. This aids stability and limited-data performance. The paper is well written, the contribution of combining Scattering and CNNs is novel and the results seem promising. I feel that such work was a missing piece in the Scattering literature to make it useful for practical applications.

I wish the authors would have investigated the effect of the stable bottom layers with respect to adversarial examples. This can be done in a relatively straightforward way with software like cleverhans [1] or deep fool [2]. It would be very interesting if the first layer's stability in the hybrid architectures increases robustness significantly, as this would tell us that these fooling images are related to low-level geometry. Finding that this is not the case, would be very interesting as well.

Further, the proposed architecture is not evaluated on real limited data problems. This would further strengthen the improved generalization claim. However, I admit that the Cifar-100 / Cifar-10 difference already seems like a promising indicator in this regard.

If one of the two points above will be addressed in an additional experiment, I would be happy to raise my score from 6 to 7.

Summary: 

+ An interesting approach is presented that might be useful for real-world limited data scenarios.
+ Limited data results look promising.
- Adversarial examples are not investigated in the experimental section.
- No realistic small-data problem is addressed.

Minor:
- The authors should add a SOTA ResNet to Table 3, as NiN is indeed out of fashion these days.
- Some typos: tacke, developping, learni.

[1]",0
"In this paper, the authors explicitly design geometrical structure into a CNN by combining it with a Scattering network. This aids stability and limited-data performance. The paper is well written, the contribution of combining Scattering and CNNs is novel and the results seem promising. I feel that such work was a missing piece in the Scattering literature to make it useful for practical applications.

I wish the authors would have investigated the effect of the stable bottom layers with respect to adversarial examples. This can be done in a relatively straightforward way with software like cleverhans [1] or deep fool [2]. It would be very interesting if the first layer's stability in the hybrid architectures increases robustness significantly, as this would tell us that these fooling images are related to low-level geometry. Finding that this is not the case, would be very interesting as well.

Further, the proposed architecture is not evaluated on real limited data problems. This would further strengthen the improved generalization claim. However, I admit that the Cifar-100 / Cifar-10 difference already seems like a promising indicator in this regard.

If one of the two points above will be addressed in an additional experiment, I would be happy to raise my score from 6 to 7.

Summary: 

+ An interesting approach is presented that might be useful for real-world limited data scenarios.
+ Limited data results look promising.
- Adversarial examples are not investigated in the experimental section.
- No realistic small-data problem is addressed.

Minor:
- The authors should add a SOTA ResNet to Table 3, as NiN is indeed out of fashion these days.
- Some typos: tacke, developping, learni.

[1]",0
"This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large.

This paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.

The paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces:
  - a straightforward variant of PQ for unnormalized vectors,
  - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,
  - hashing tricks and bloom filter are simply borrowed from previous papers.

These techniques are quite generic and could as well be used in other works. 


Here are some minor problems with the paper:

  - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).
  
  - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.
  

Overall this looks like a solid work, but with potentially limited impact research-wise.",0
"This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large.

This paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.

The paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces:
  - a straightforward variant of PQ for unnormalized vectors,
  - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,
  - hashing tricks and bloom filter are simply borrowed from previous papers.

These techniques are quite generic and could as well be used in other works. 


Here are some minor problems with the paper:

  - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).
  
  - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.
  

Overall this looks like a solid work, but with potentially limited impact research-wise.",0
"The authors propose and evaluate using SPN's to generate embeddings of input and output variables, and using MPN to decode output embeddings to output variables. The advantage of predicting label embeddings is to decouple dependencies in the predicted space. The authors show experimentally that using SPN based embeddings is better than those produced by RBM's.

This paper is fairly dense and a bit hard to read. After the discussion, the main contributions of the authors are:

1. They propose the scheme of learning SPN's over Y and then using MPN's to decode the output, or just SPNs to embed X.
2. They propose how to decode MPN's with partial data.
3. They perform some analysis of when their scheme will lead to perfect encoding/decodings.
4. They run many, many experiments comparing various ways of using their proposed method to make predictions on multi-label classification datasets.

My main concerns with this paper are as follows:

- The point of this paper is about using generative models for representation learning. In their experiments, the main task is discriminative; e.g. predict multiple Y from X. The only discriminative baseline is a L2 regularized logistic regression, which does not have any structure on the output; it'd be nice to see how a discriminative structured prediction method would do, such as CRF or belief propagation. 

- The many experiments suggest that their encoder/decoder scheme is working better than the alternatives; can you please give more details on the relative computation complexity of each method?

- One thing I'm still having trouble understanding is *why* this method works better than MADE and the other alternatives. Is it learning a better model of the distribution of Y? Is it better at separating out correlations in the output into individual nodes?  Does it have larger representations? 

- I think the experiments are overkill and if anything, they way they are presented detract from the paper. There's already far too many numbers and graphs presented to be easy to understand.  If I have to dig through hundreds of numbers to figure out if your claim is correct, the paper is not clear enough. And, I said this before in my comments, please do not refer to Q1, Q2, etc. -- these shortcuts let you make the paper more dense with fewer words but at the cost of readability.

I *think* I convinced myself that your method works...I would love to see a table that shows, for each condition: (A) a baseline X->Y, (B) one *average* result across datasets for your method, and (C) one *average* result from a reasonable best competitor method. Please show for both the exact match and hamming losses, as that will demonstrate the gap between independent linear prediction and structured prediction. That would still be plenty of numbers but would make it much easier for the reader to verify your claims and you can put everything else in the Appendix.  E.g. something like:

Input | Predicted Output | Decoder | Hamming | Exact Match
----
X | P(Y) | CRF | xx.xx | xx.xx   (this is your baseline)
SPN E_X | P(Y) | n/a | xx.xx | xx.xx 
X | SPN E_Y | MPN | xx.xx | xx.xx  (given X, predict E_Y, then decode it with an MPN)

Does a presentation like that make sense? It's just really hard and time-consuming for me as a reviewer to verify your results, the way you've laid them out currently.",0
"The authors propose and evaluate using SPN's to generate embeddings of input and output variables, and using MPN to decode output embeddings to output variables. The advantage of predicting label embeddings is to decouple dependencies in the predicted space. The authors show experimentally that using SPN based embeddings is better than those produced by RBM's.

This paper is fairly dense and a bit hard to read. After the discussion, the main contributions of the authors are:

1. They propose the scheme of learning SPN's over Y and then using MPN's to decode the output, or just SPNs to embed X.
2. They propose how to decode MPN's with partial data.
3. They perform some analysis of when their scheme will lead to perfect encoding/decodings.
4. They run many, many experiments comparing various ways of using their proposed method to make predictions on multi-label classification datasets.

My main concerns with this paper are as follows:

- The point of this paper is about using generative models for representation learning. In their experiments, the main task is discriminative; e.g. predict multiple Y from X. The only discriminative baseline is a L2 regularized logistic regression, which does not have any structure on the output; it'd be nice to see how a discriminative structured prediction method would do, such as CRF or belief propagation. 

- The many experiments suggest that their encoder/decoder scheme is working better than the alternatives; can you please give more details on the relative computation complexity of each method?

- One thing I'm still having trouble understanding is *why* this method works better than MADE and the other alternatives. Is it learning a better model of the distribution of Y? Is it better at separating out correlations in the output into individual nodes?  Does it have larger representations? 

- I think the experiments are overkill and if anything, they way they are presented detract from the paper. There's already far too many numbers and graphs presented to be easy to understand.  If I have to dig through hundreds of numbers to figure out if your claim is correct, the paper is not clear enough. And, I said this before in my comments, please do not refer to Q1, Q2, etc. -- these shortcuts let you make the paper more dense with fewer words but at the cost of readability.

I *think* I convinced myself that your method works...I would love to see a table that shows, for each condition: (A) a baseline X->Y, (B) one *average* result across datasets for your method, and (C) one *average* result from a reasonable best competitor method. Please show for both the exact match and hamming losses, as that will demonstrate the gap between independent linear prediction and structured prediction. That would still be plenty of numbers but would make it much easier for the reader to verify your claims and you can put everything else in the Appendix.  E.g. something like:

Input | Predicted Output | Decoder | Hamming | Exact Match
----
X | P(Y) | CRF | xx.xx | xx.xx   (this is your baseline)
SPN E_X | P(Y) | n/a | xx.xx | xx.xx 
X | SPN E_Y | MPN | xx.xx | xx.xx  (given X, predict E_Y, then decode it with an MPN)

Does a presentation like that make sense? It's just really hard and time-consuming for me as a reviewer to verify your results, the way you've laid them out currently.",0
"The paper proposes an approach to sequence transduction for the case when a monotonic alignment between the input and the output is plausible. It is assumed that the alignment can be provided as a part of training data, with Chinese Restaurant process being used in the actual experiments. 

The idea makes sense, although its applicability is limited to the domains where a monotonic alignment is available. But as discussed during the pre-review period, there has been a lot of strongly overlapping related work, such as probabilistic models with hard-alignment (Sequence Transduction With Recurrent Neural Network, Graves et al, 2012) and also attempts to use external alignments in end-to-end models (A Neural Transducer, Jaitly et al, 2015). That said, I do not think the approach is sufficiently novel. 

I also have a concern regarding the evaluation. I do not think it is fair to compare the proposed model that depends on external alignment with the vanilla soft-attention model that learns alignments from scratch. In a control experiment soft-attention could be trained to match the external alignment. Such a pretraining could reduce overfitting on the small dataset, the one on which the proposed approach brings the most improvement. On a larger dataset, especially SIGMORPHON, the improvements are not very big and are only obtained for a certain class of languages.

To sum up, two main issues are (a) lack of novelty (b) the comparison of a model trained with external alignment and one without it.",0
"The paper proposes an approach to sequence transduction for the case when a monotonic alignment between the input and the output is plausible. It is assumed that the alignment can be provided as a part of training data, with Chinese Restaurant process being used in the actual experiments. 

The idea makes sense, although its applicability is limited to the domains where a monotonic alignment is available. But as discussed during the pre-review period, there has been a lot of strongly overlapping related work, such as probabilistic models with hard-alignment (Sequence Transduction With Recurrent Neural Network, Graves et al, 2012) and also attempts to use external alignments in end-to-end models (A Neural Transducer, Jaitly et al, 2015). That said, I do not think the approach is sufficiently novel. 

I also have a concern regarding the evaluation. I do not think it is fair to compare the proposed model that depends on external alignment with the vanilla soft-attention model that learns alignments from scratch. In a control experiment soft-attention could be trained to match the external alignment. Such a pretraining could reduce overfitting on the small dataset, the one on which the proposed approach brings the most improvement. On a larger dataset, especially SIGMORPHON, the improvements are not very big and are only obtained for a certain class of languages.

To sum up, two main issues are (a) lack of novelty (b) the comparison of a model trained with external alignment and one without it.",0
"As you noted for Figure 5 Left, sometimes it seems sufficient to tune learning rates. I see your argument for Figure 6 Right, 
but 
1) not for all good learning rates make Adam fail, I guess you selected the one where it did (note that Adam was several times faster than Eve in the beginning)
2) I don't buy ""Eve always converges"" because you show it only for 0.1 and since Eve is not Adam, 0.1 of Adam is not 0.1 of Eve because of d_t. 

To my understanding, you define d_t over time with 3 hyperparameters. Similarly, one can define d_t directly. The behaviour of d_t that you show is not extraordinary and can be parameterized. If Eve is better than Adam, then looking at d_t we can directly see whether we underestimated or overestimated learning rates. You could argue that Eve does it automatically but you do tune learning rates for each problem individually anyway.",0
"As you noted for Figure 5 Left, sometimes it seems sufficient to tune learning rates. I see your argument for Figure 6 Right, 
but 
1) not for all good learning rates make Adam fail, I guess you selected the one where it did (note that Adam was several times faster than Eve in the beginning)
2) I don't buy ""Eve always converges"" because you show it only for 0.1 and since Eve is not Adam, 0.1 of Adam is not 0.1 of Eve because of d_t. 

To my understanding, you define d_t over time with 3 hyperparameters. Similarly, one can define d_t directly. The behaviour of d_t that you show is not extraordinary and can be parameterized. If Eve is better than Adam, then looking at d_t we can directly see whether we underestimated or overestimated learning rates. You could argue that Eve does it automatically but you do tune learning rates for each problem individually anyway.",0
"The paper proposes a model that aims at learning to label nodes of graph in a semi-supervised setting. The idea of the model is based on the use of the graph structure to regularize the representations learned at the node levels. Experimental results are provided on different tasks

The underlying idea of this paper (graph regularization) has been already explored in different papers – e.g 'Learning latent representations of nodes for classifying in heterogeneous social networks' [Jacob et al. 2014],   [Weston et al 2012] where a real graph structure is used instead of a built one. The experiments lack of strong comparisons with other graph models (e.g Iterative Classification, 'Learning from labeled and unlabeled data on a directed graph', ...). So the novelty of the paper and the experimental protocol are not strong enough to accpet the paper.

Pros:
* Learning over graph is an important topic

Cons:
* Many existing approaches have already exploited the same types of ideas, resulting in very close models
* Lack of comparison w.r.t existing models",0
"The paper proposes a model that aims at learning to label nodes of graph in a semi-supervised setting. The idea of the model is based on the use of the graph structure to regularize the representations learned at the node levels. Experimental results are provided on different tasks

The underlying idea of this paper (graph regularization) has been already explored in different papers – e.g 'Learning latent representations of nodes for classifying in heterogeneous social networks' [Jacob et al. 2014],   [Weston et al 2012] where a real graph structure is used instead of a built one. The experiments lack of strong comparisons with other graph models (e.g Iterative Classification, 'Learning from labeled and unlabeled data on a directed graph', ...). So the novelty of the paper and the experimental protocol are not strong enough to accpet the paper.

Pros:
* Learning over graph is an important topic

Cons:
* Many existing approaches have already exploited the same types of ideas, resulting in very close models
* Lack of comparison w.r.t existing models",0
"The paper extends the NTM by a trainable memory addressing scheme.
The paper also investigates both continuous/differentiable as well as discrete/non-differentiable addressing mechanisms.

Pros:
* Extension to NTM with trainable addressing.
* Experiments with discrete addressing.
* Experiments on bAbI QA tasks.

Cons:
* Big gap to MemN2N and DMN+ in performance.
* Code not available.
* There could be more experiments on other real-world tasks.",0
"The paper extends the NTM by a trainable memory addressing scheme.
The paper also investigates both continuous/differentiable as well as discrete/non-differentiable addressing mechanisms.

Pros:
* Extension to NTM with trainable addressing.
* Experiments with discrete addressing.
* Experiments on bAbI QA tasks.

Cons:
* Big gap to MemN2N and DMN+ in performance.
* Code not available.
* There could be more experiments on other real-world tasks.",0
"This paper presents a system approach to combine multiple modalities to perform classification in a practical scenario (e-commerce).

In general, I find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multi-modal analysis, and the rest of the paper offers no surprise in implementing such approaches. This seems to be a better fit for venues that focus more on production systems, and seems to be a bad fit for ICLR where the focus is more on research of novel algorithms and theories.",0
"This paper presents a system approach to combine multiple modalities to perform classification in a practical scenario (e-commerce).

In general, I find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multi-modal analysis, and the rest of the paper offers no surprise in implementing such approaches. This seems to be a better fit for venues that focus more on production systems, and seems to be a bad fit for ICLR where the focus is more on research of novel algorithms and theories.",0
"I reviewed the manuscript as of December 6th.

Summary:
The authors build upon generative adversarial networks for the purpose of steganalysis -- i.e. detecting hidden messages in a payload. The authors describe a new model architecture in which a new element, a 'steganalyser' is added a training objective to the GAN model.

Major Comments:
The authors introduce an interesting new direction for applying generative networks. That said, I think the premise of the paper could stand some additional exposition. How exactly would a SGAN method be employed? This is not clear from the paper. Why does the model require a generative model? Steganalysis by itself seems like a classification problem (i.e. a binary decision if there a hidden message?) Would you envision that a user has a message to send and does not care about the image (container) that it is being sent with? Or does the user have an image and the network generates a synthetic version of the image as a container and then hide the message in the container? Or is the SGAN somehow trained as a method for detecting hidden codes performed by any algorithm in an image? Explicitly describing the use-case would help with interpreting the results in the paper.

Additionally, the experiments and analysis in this paper is quite light as the authors only report a few steganalysis performance numbers in the tables (Table 1,2,3). A more extensive analysis seems warranted to explore the parameter space and provide a quantitative comparison with other methods discussed (e.g. HUGO, WOW, LSB, etc.) When is it appropriate to use this method over the others? Why does the seed effect the quality of results? Does a fixed seed correspond realistic scenario for employing this method?

Minor comments:
- Is Figure 1 necessary?
- Why does the seed value effect the quality of the predictive performance of the model?",0
"I reviewed the manuscript as of December 6th.

Summary:
The authors build upon generative adversarial networks for the purpose of steganalysis -- i.e. detecting hidden messages in a payload. The authors describe a new model architecture in which a new element, a 'steganalyser' is added a training objective to the GAN model.

Major Comments:
The authors introduce an interesting new direction for applying generative networks. That said, I think the premise of the paper could stand some additional exposition. How exactly would a SGAN method be employed? This is not clear from the paper. Why does the model require a generative model? Steganalysis by itself seems like a classification problem (i.e. a binary decision if there a hidden message?) Would you envision that a user has a message to send and does not care about the image (container) that it is being sent with? Or does the user have an image and the network generates a synthetic version of the image as a container and then hide the message in the container? Or is the SGAN somehow trained as a method for detecting hidden codes performed by any algorithm in an image? Explicitly describing the use-case would help with interpreting the results in the paper.

Additionally, the experiments and analysis in this paper is quite light as the authors only report a few steganalysis performance numbers in the tables (Table 1,2,3). A more extensive analysis seems warranted to explore the parameter space and provide a quantitative comparison with other methods discussed (e.g. HUGO, WOW, LSB, etc.) When is it appropriate to use this method over the others? Why does the seed effect the quality of results? Does a fixed seed correspond realistic scenario for employing this method?

Minor comments:
- Is Figure 1 necessary?
- Why does the seed value effect the quality of the predictive performance of the model?",0
"Summary: The paper proposes a large-scale dataset for reading comprehension, with the final goal of releasing 1 million questions and answers. The authors have currently released 100,000 queries and their answers. The dataset differs from existing reading comprehension datasets mainly w.r.t queries being sampled from user queries rather than being generated by crowd-workers and answers being generated by crowd-workers rather than being spans of text from the provided passage. The paper presents some analysis of the dataset such as distribution of answer types. The paper also presents the results of some generative and some cloze-style models on the MS MARCO dataset.

Strengths:

1. The paper provides useful insights about the limitations of the existing reading comprehension datasets – questions asked by crowd-workers have different distribution compared to that of questions asked by actual users of intelligent agents, answers being restricted to span from the reading text rather than requiring reasoning across multiple pieces of text/passages.

2. MS MARCO dataset has novel useful characteristics compared to existing reading comprehension datasets – questions are sampled from user queries, answers are generated by humans.

3. The experimental evaluation of the existing baseline models on the MS MARCO dataset is satisfactory.

Weaknesses/Suggestions:

1. The paper does not report human performance on the dataset. Human performance should be reported to estimate the difficulty of the dataset. The degree of inter-human agreement will also reflect how well the metric (being used to compute inter-human agreement and accuracies of the baseline models) can deal with variance in the sentence structure with similar semantics.

2. I would like to see the comparison between the answer type distribution in the MS MARCO dataset and that in existing reading comprehension datasets such as SQuAD. This would ground the claim made in the paper the distributions of questions asked by crowd-workers is different from that of user queries.

3. The paper uses automatic metrics such as ROUGE, BLEU for evaluating natural language answers. However, it is known that such metrics poorly correlate with human judgement for tasks such as image caption evaluation (Chen et al., Microsoft COCO Captions: Data Collection and Evaluation Server, CoRR abs/1504.00325 (2015)). So, I wonder how authors justify using such metrics for evaluating open-ended natural language answers.

4. The paper mentions that a classifier was used to filter answer seeking queries from all Bing queries. It would be good to mention the accuracy of this classifier. This will provide insights into what percentage of the MS MARCO questions are answer seeking queries. Similarly, what is the accuracy of the information retrieval based system used to retrieve passages for filtered queries?

5. Please include the description of the best passage baseline in the paper.
  
6. Fix opening quotes, i.e. ” -> “ (for instance, on page 5, ”what” -> “what”).

Review Summary: The paper is well motivated, the use of user queries and human generated answers makes the dataset different from existing datasets. However, I would like to see the human performance on the dataset and quantitative comparison between the distribution of questions obtained from user queries and that of crowd-sourced questions. I would also like the authors to comment on the use of automatic metrics (such as ROUGE, BLEU) in the light of the fact that such metrics do not correlate well with human judgements for tasks such as image caption evaluation.",0
"Summary: The paper proposes a large-scale dataset for reading comprehension, with the final goal of releasing 1 million questions and answers. The authors have currently released 100,000 queries and their answers. The dataset differs from existing reading comprehension datasets mainly w.r.t queries being sampled from user queries rather than being generated by crowd-workers and answers being generated by crowd-workers rather than being spans of text from the provided passage. The paper presents some analysis of the dataset such as distribution of answer types. The paper also presents the results of some generative and some cloze-style models on the MS MARCO dataset.

Strengths:

1. The paper provides useful insights about the limitations of the existing reading comprehension datasets – questions asked by crowd-workers have different distribution compared to that of questions asked by actual users of intelligent agents, answers being restricted to span from the reading text rather than requiring reasoning across multiple pieces of text/passages.

2. MS MARCO dataset has novel useful characteristics compared to existing reading comprehension datasets – questions are sampled from user queries, answers are generated by humans.

3. The experimental evaluation of the existing baseline models on the MS MARCO dataset is satisfactory.

Weaknesses/Suggestions:

1. The paper does not report human performance on the dataset. Human performance should be reported to estimate the difficulty of the dataset. The degree of inter-human agreement will also reflect how well the metric (being used to compute inter-human agreement and accuracies of the baseline models) can deal with variance in the sentence structure with similar semantics.

2. I would like to see the comparison between the answer type distribution in the MS MARCO dataset and that in existing reading comprehension datasets such as SQuAD. This would ground the claim made in the paper the distributions of questions asked by crowd-workers is different from that of user queries.

3. The paper uses automatic metrics such as ROUGE, BLEU for evaluating natural language answers. However, it is known that such metrics poorly correlate with human judgement for tasks such as image caption evaluation (Chen et al., Microsoft COCO Captions: Data Collection and Evaluation Server, CoRR abs/1504.00325 (2015)). So, I wonder how authors justify using such metrics for evaluating open-ended natural language answers.

4. The paper mentions that a classifier was used to filter answer seeking queries from all Bing queries. It would be good to mention the accuracy of this classifier. This will provide insights into what percentage of the MS MARCO questions are answer seeking queries. Similarly, what is the accuracy of the information retrieval based system used to retrieve passages for filtered queries?

5. Please include the description of the best passage baseline in the paper.
  
6. Fix opening quotes, i.e. ” -> “ (for instance, on page 5, ”what” -> “what”).

Review Summary: The paper is well motivated, the use of user queries and human generated answers makes the dataset different from existing datasets. However, I would like to see the human performance on the dataset and quantitative comparison between the distribution of questions obtained from user queries and that of crowd-sourced questions. I would also like the authors to comment on the use of automatic metrics (such as ROUGE, BLEU) in the light of the fact that such metrics do not correlate well with human judgements for tasks such as image caption evaluation.",0
"The authors of this work propose a learnable approach to reducing the dimensionality of learned filters in deep neural networks. This is an interesting approach, but the presented work looks a bit raw.

1. There are many typos in this manuscript. 
2. The experimental results are rather weak and don't show much improvement in accuracy. Instead the authors could position this work as a compression mechanism and would have to compare to low rank approximation of filters for DNNs. Yet this is not done. 
3. Aside from compression, OMG can be viewed as a form of regularization to reduce the unnecessary capacity of the network to improve generalization. Again, this is not addressed in enough detail.
4. If the authors care to compare their approach to other 1-shot learning methods, then they would have to evaluate their approach with siamese and triplet learning networks. This isn't done.",0
"The authors of this work propose a learnable approach to reducing the dimensionality of learned filters in deep neural networks. This is an interesting approach, but the presented work looks a bit raw.

1. There are many typos in this manuscript. 
2. The experimental results are rather weak and don't show much improvement in accuracy. Instead the authors could position this work as a compression mechanism and would have to compare to low rank approximation of filters for DNNs. Yet this is not done. 
3. Aside from compression, OMG can be viewed as a form of regularization to reduce the unnecessary capacity of the network to improve generalization. Again, this is not addressed in enough detail.
4. If the authors care to compare their approach to other 1-shot learning methods, then they would have to evaluate their approach with siamese and triplet learning networks. This isn't done.",0
"SUMMARY.
This paper presents a method for enriching medical concepts with their parent nodes in an ontology.
The method employs an attention mechanism over the parent nodes of a medical concept to create a richer representation of the concept itself.
The rationale of this is that for  infrequent medical concepts the attention mechanism will rely more on general concepts, higher in the ontology hierarchy, while for frequent ones will focus on the specific concept.
The attention mechanism is trained together with a recurrent neural network and the model accuracy is tested on two tasks.
The first task aims at prediction the diagnosis categories at each time step, while the second task aims at predicting whether or not a heart failure is likely to happen after the T-th step.

Results shows that the proposed model works well in condition of data insufficiency.

----------

OVERALL JUDGMENT
The proposed model is simple but interesting.
The ideas presented are worth to expand but there are also some points where the authors could have done better.
The learning of the representation of concepts in the ontology is a bit naive, for example the authors could have used some kind of knowledge base factorization approach to learn the concepts, or some graph convolutional approach.
I do not see why the the very general factorization methods for knowledge bases do not apply in the case of ontology learning.
I also found strange that the representation of leaves are fine tuned while the inner nodes are not, it is a specific reason to do so?

Regarding the presentation, the paper is clear and the qualitative evaluation is insightful.


----------

DETAILED COMMENTS

Figure 2. Please use the same image format with the same resolution.",0
"SUMMARY.
This paper presents a method for enriching medical concepts with their parent nodes in an ontology.
The method employs an attention mechanism over the parent nodes of a medical concept to create a richer representation of the concept itself.
The rationale of this is that for  infrequent medical concepts the attention mechanism will rely more on general concepts, higher in the ontology hierarchy, while for frequent ones will focus on the specific concept.
The attention mechanism is trained together with a recurrent neural network and the model accuracy is tested on two tasks.
The first task aims at prediction the diagnosis categories at each time step, while the second task aims at predicting whether or not a heart failure is likely to happen after the T-th step.

Results shows that the proposed model works well in condition of data insufficiency.

----------

OVERALL JUDGMENT
The proposed model is simple but interesting.
The ideas presented are worth to expand but there are also some points where the authors could have done better.
The learning of the representation of concepts in the ontology is a bit naive, for example the authors could have used some kind of knowledge base factorization approach to learn the concepts, or some graph convolutional approach.
I do not see why the the very general factorization methods for knowledge bases do not apply in the case of ontology learning.
I also found strange that the representation of leaves are fine tuned while the inner nodes are not, it is a specific reason to do so?

Regarding the presentation, the paper is clear and the qualitative evaluation is insightful.


----------

DETAILED COMMENTS

Figure 2. Please use the same image format with the same resolution.",0
"The authors propose a simple idea. They penalize confident predictions by using the entropy of the predictive distribution as a regularizer. The authors consider two variations on this idea. In one, they penalize the divergence from the uniform distribution. In the other variation, they penalize distance from the base rates. They term this variation ""unigram"" but I find the name odd as I've never seen multi-class labels described as unigrams before. What would a bigram be? 

The idea is simple,  and while it's been used in the context of reinforcement learning, it hasn't been popularized as a regularizer for improving generalization in supervised learning. 

The justifications for the idea still lacks analysis. And the author responses comparing it to L2 regularization have some holes. A simple number line example with polynomial regression makes clear how L2 regularization could prevent a model from badly overfitting to accommodate every data point. In contrast, it seems trivial to fit every data point and satisfy arbitrarily high entropy. Of course, the un-regularized optimization is to maximize log likelihood, not simply to maximize accuracy.  And perhaps something interesting may be happening at the interplay between the log likelihood objective and the regularization objective. But the paper doesn't indicate precisely what.

I could imagine the following scenario: when the network outputs probabilities near 0, it can get high loss (if the label is 1). The entropy regularization could be stabilizing the gradient, preventing sharp loss on outlier examples. The regularization then might owe mainly to faster convergence. Could the authors analyze the effect empirically, on the distribution of the gradient norms? 

The strength of this paper is its empirical rigor. The authors take their idea and put it through its paces on a host of popular and classic benchmarks spanning CNNs and RNNs. It appears that on some datasets, especially language modeling, the confidence penalty outperforms label smoothing. 

At present, I rate this paper as a borderline contribution but I'm open to revising my review pending further modifications. 

Typo:
In related work: ""Penalizing entropy"" - you mean penalizing low entropy",0
"The authors propose a simple idea. They penalize confident predictions by using the entropy of the predictive distribution as a regularizer. The authors consider two variations on this idea. In one, they penalize the divergence from the uniform distribution. In the other variation, they penalize distance from the base rates. They term this variation ""unigram"" but I find the name odd as I've never seen multi-class labels described as unigrams before. What would a bigram be? 

The idea is simple,  and while it's been used in the context of reinforcement learning, it hasn't been popularized as a regularizer for improving generalization in supervised learning. 

The justifications for the idea still lacks analysis. And the author responses comparing it to L2 regularization have some holes. A simple number line example with polynomial regression makes clear how L2 regularization could prevent a model from badly overfitting to accommodate every data point. In contrast, it seems trivial to fit every data point and satisfy arbitrarily high entropy. Of course, the un-regularized optimization is to maximize log likelihood, not simply to maximize accuracy.  And perhaps something interesting may be happening at the interplay between the log likelihood objective and the regularization objective. But the paper doesn't indicate precisely what.

I could imagine the following scenario: when the network outputs probabilities near 0, it can get high loss (if the label is 1). The entropy regularization could be stabilizing the gradient, preventing sharp loss on outlier examples. The regularization then might owe mainly to faster convergence. Could the authors analyze the effect empirically, on the distribution of the gradient norms? 

The strength of this paper is its empirical rigor. The authors take their idea and put it through its paces on a host of popular and classic benchmarks spanning CNNs and RNNs. It appears that on some datasets, especially language modeling, the confidence penalty outperforms label smoothing. 

At present, I rate this paper as a borderline contribution but I'm open to revising my review pending further modifications. 

Typo:
In related work: ""Penalizing entropy"" - you mean penalizing low entropy",0
"The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting. In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks, the method aims to identify individual task and solve them separately. To this end, the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence, i.e., a task only changes representation between training episodes. The method has been evaluated on two tasks, multi-task slot-car racing and mobile navigation to prove its efficacy.

there were several unclear issues:

1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general mutiple task setting, in particular over the multi-task learning?
The reply of the authors was not fully satisfactory. The argument did not support the lack of comparison to multi-task joint-learning. It seems they don't plan to include any comparison neither. I think it's important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage.

2.Following up to the previous question, please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem).
The explanation of the authors did provide more details and more explicit information. 

3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.
The author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.

In summary, while there are remaining concerns about lacking comparisons, the is a weak tendency towards accepting the submission.",0
"The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting. In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks, the method aims to identify individual task and solve them separately. To this end, the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence, i.e., a task only changes representation between training episodes. The method has been evaluated on two tasks, multi-task slot-car racing and mobile navigation to prove its efficacy.

there were several unclear issues:

1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general mutiple task setting, in particular over the multi-task learning?
The reply of the authors was not fully satisfactory. The argument did not support the lack of comparison to multi-task joint-learning. It seems they don't plan to include any comparison neither. I think it's important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage.

2.Following up to the previous question, please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem).
The explanation of the authors did provide more details and more explicit information. 

3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.
The author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.

In summary, while there are remaining concerns about lacking comparisons, the is a weak tendency towards accepting the submission.",0
"Overall, this is a nice paper. Developing a unifying framework for these newer
neural models is a worthwhile endeavor.

However, it's unclear if the DRAGNN framework (in its current form) is a
significant standalone contribution. The main idea is straightforward: use a
transition system to unroll a computation graph. When you implement models in
this way you can reuse code because modules can be mixed and matched. This is
nice, but (in my opinion) is just good software engineering, not machine 
learning research.

Moreover, there appears to be little incentive to use DRAGNN, as there are no
'free things' (benefits) that you get by using the framework. For example:

- If you write your neuralnet in an automatic differentiation library (e.g.,
  tensorflow or dynet) you get gradients for 'free'.

- In the VW framework, there are efficiency tricks that 'the credit assignment
  compiler' provides for you, which would be tedious to implement on your
  own. There is also a variety of algorithms for training the model in a
  principled way (i.e., without exposure bias).

I don't feel that my question about the limitations of the framework has been
satisfactorily addressed. Let me ask it in a different way: Can you give me
examples of a few models that I can't (nicely) express in the DRAGNN framework?
What if I wanted to implement",0
"Overall, this is a nice paper. Developing a unifying framework for these newer
neural models is a worthwhile endeavor.

However, it's unclear if the DRAGNN framework (in its current form) is a
significant standalone contribution. The main idea is straightforward: use a
transition system to unroll a computation graph. When you implement models in
this way you can reuse code because modules can be mixed and matched. This is
nice, but (in my opinion) is just good software engineering, not machine 
learning research.

Moreover, there appears to be little incentive to use DRAGNN, as there are no
'free things' (benefits) that you get by using the framework. For example:

- If you write your neuralnet in an automatic differentiation library (e.g.,
  tensorflow or dynet) you get gradients for 'free'.

- In the VW framework, there are efficiency tricks that 'the credit assignment
  compiler' provides for you, which would be tedious to implement on your
  own. There is also a variety of algorithms for training the model in a
  principled way (i.e., without exposure bias).

I don't feel that my question about the limitations of the framework has been
satisfactorily addressed. Let me ask it in a different way: Can you give me
examples of a few models that I can't (nicely) express in the DRAGNN framework?
What if I wanted to implement",0
"This paper proposes an alternative to Conditional Variational Auto-Encoders and Conditional MultiModal Auto-Encoders to perform inference of missing modalities in dataset with multiple modalities. The proposed approach is a Variational Auto-Encoder jointly on all the modalities  with additional KL divergence penalties between the approximate posterior given all the modalities and the approximate posterior given a subset of the modalities. The approach is named Joint Multimodal Variational Auto-Encoder.
The authors make a connection between this approach and the Variation of Information. It is unclear why the authors chose the JMVAE approach instead of a more elegant Variation of Information approach.
Another unaddressed issue is the scalability of the method. As far as I can tell (given that no code is provided and the specification of the encoder is missing), this approach requires a new encoder per subset of missing modalities. Right now this approach seems to scale since there are only two modalities.
The fact that the estimating the log-likelihood log(p(x)) using multiple modalities provide a lower value than with just one in Table 1 is a bit odd. Could you explain that ?
The comparison with between the representation learned by JMVAE and CVAE might be unfair given that the representation of CVAE is learned conditionally, on the label in the case of MNIST, and should therefore not consider the label in this representation. Intuitively, this representation could represent ""style"" as shown in (Kingma et al., 2014) in their conditional generation figure.
For CelebA, comparing log-likelihood on models that use GANs is probably not significant since GAN does not optimizes log-likelihood. 
Overall this is an interesting problem and there are also interesting ideas worth exploring further, but the execution of the paper requires more work.",0
"This paper proposes an alternative to Conditional Variational Auto-Encoders and Conditional MultiModal Auto-Encoders to perform inference of missing modalities in dataset with multiple modalities. The proposed approach is a Variational Auto-Encoder jointly on all the modalities  with additional KL divergence penalties between the approximate posterior given all the modalities and the approximate posterior given a subset of the modalities. The approach is named Joint Multimodal Variational Auto-Encoder.
The authors make a connection between this approach and the Variation of Information. It is unclear why the authors chose the JMVAE approach instead of a more elegant Variation of Information approach.
Another unaddressed issue is the scalability of the method. As far as I can tell (given that no code is provided and the specification of the encoder is missing), this approach requires a new encoder per subset of missing modalities. Right now this approach seems to scale since there are only two modalities.
The fact that the estimating the log-likelihood log(p(x)) using multiple modalities provide a lower value than with just one in Table 1 is a bit odd. Could you explain that ?
The comparison with between the representation learned by JMVAE and CVAE might be unfair given that the representation of CVAE is learned conditionally, on the label in the case of MNIST, and should therefore not consider the label in this representation. Intuitively, this representation could represent ""style"" as shown in (Kingma et al., 2014) in their conditional generation figure.
For CelebA, comparing log-likelihood on models that use GANs is probably not significant since GAN does not optimizes log-likelihood. 
Overall this is an interesting problem and there are also interesting ideas worth exploring further, but the execution of the paper requires more work.",0
"I find this paper not very compelling.  The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable.  However, this was precisely the point of Rae et al.    There are a  number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards).   Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.     I also find the repeated distinction between ""mips"" and ""nns"" distracting; most libraries that can do one can do the other, or inputs can be modified  to switch between the problems; indeed the authors do this when they convert to the  ""mcss"" problem.",0
"I find this paper not very compelling.  The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable.  However, this was precisely the point of Rae et al.    There are a  number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards).   Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.     I also find the repeated distinction between ""mips"" and ""nns"" distracting; most libraries that can do one can do the other, or inputs can be modified  to switch between the problems; indeed the authors do this when they convert to the  ""mcss"" problem.",0
"Summary of the paper

The paper studies the invertiblity of convolutional neural network in the random model. A reconstruction algorithm similar to IHT is proposed for layer-wise inversion of the network.
 

Clarity:

- The paper is confusing wrt to standard notations in deep learning.

Comments:

The paper makes two simplifications in the analysis of a CNN, that makes it map to a model based compressive sensing framework:

1-  The non linearity (RELU) is dropped. This is a big simplification, for random gaussian weights for instance we know by JL that we can preserve L_2 distance, when RELU is applied the metric changes (see for instance the kernel for n=1 in",0
"Summary of the paper

The paper studies the invertiblity of convolutional neural network in the random model. A reconstruction algorithm similar to IHT is proposed for layer-wise inversion of the network.
 

Clarity:

- The paper is confusing wrt to standard notations in deep learning.

Comments:

The paper makes two simplifications in the analysis of a CNN, that makes it map to a model based compressive sensing framework:

1-  The non linearity (RELU) is dropped. This is a big simplification, for random gaussian weights for instance we know by JL that we can preserve L_2 distance, when RELU is applied the metric changes (see for instance the kernel for n=1 in",0
"This paper presents a thorough analysis of different methods to do curriculum learning. The major issue I have with it is that the dataset used seems very specific and does not necessarily justified, as mentioned by AnonReviewer3. It would have been great to see experiments on more standard tasks. Also, I really can't understand how the performance of FFNN models can be so good, please elaborate on this (see last comment).
However, the paper is well written, the comparisons of the described methods are interesting and would probably apply to some other datasets as well.

The paper is way too long (18 pages!). Please reduce it or move some of the results to an appendix section.

The method described is extremely similar to the one described in Reinforcement learning neural turing machines (Zaremba et al., 2016,",0
"This paper presents a thorough analysis of different methods to do curriculum learning. The major issue I have with it is that the dataset used seems very specific and does not necessarily justified, as mentioned by AnonReviewer3. It would have been great to see experiments on more standard tasks. Also, I really can't understand how the performance of FFNN models can be so good, please elaborate on this (see last comment).
However, the paper is well written, the comparisons of the described methods are interesting and would probably apply to some other datasets as well.

The paper is way too long (18 pages!). Please reduce it or move some of the results to an appendix section.

The method described is extremely similar to the one described in Reinforcement learning neural turing machines (Zaremba et al., 2016,",0
"The paper presents a method to learn graph embeddings in a unsupervised way using random walks. It is well written and the execution appears quite accurate. The area of learning whole graph representations does not seem to be very well explored in general, and the proposed approach enjoys having very few competitors.

In a nutshell, the idea is to linearize the graph using random walks and to compute the embedding of the central segment of each walk using the skip-thought criterion. Being not an expert in biology, I can not comment whether or not this makes sense, but the gains reported in Table 2 are quite significant. 

An anonymous public comment compared this work to a number of others in which the problem of learning representations of nodes is considered. While this is arguably a different goal, one natural baseline would be to pool these representations using mean- or max- pooling. It would very interesting to do such a comparison, especially given that the considered approach heavily relies on pooling (see Figure 3(c))

To sum up, I think it is a nice paper, and with more baselines I would be ready to further increase the numerical score.",0
"The paper presents a method to learn graph embeddings in a unsupervised way using random walks. It is well written and the execution appears quite accurate. The area of learning whole graph representations does not seem to be very well explored in general, and the proposed approach enjoys having very few competitors.

In a nutshell, the idea is to linearize the graph using random walks and to compute the embedding of the central segment of each walk using the skip-thought criterion. Being not an expert in biology, I can not comment whether or not this makes sense, but the gains reported in Table 2 are quite significant. 

An anonymous public comment compared this work to a number of others in which the problem of learning representations of nodes is considered. While this is arguably a different goal, one natural baseline would be to pool these representations using mean- or max- pooling. It would very interesting to do such a comparison, especially given that the considered approach heavily relies on pooling (see Figure 3(c))

To sum up, I think it is a nice paper, and with more baselines I would be ready to further increase the numerical score.",0
"This work investigates the performance of transfer learning from resource-rich setup (BookTest, CNN/Daily Mail corpora) to low-resource (bAbI, SQuAD benchmarks) settings. Experiments show poor improvements in 0-shot learning. However, when the model is exposed to few training instances some improvements are observed.

The claims made here require a more comprehensive analysis. I criticize the use of bAbI as a low-resource real-world scenario. bAbI is designed as a unit test and is far from representing many natural language phenomena. Thus, the claims related to bAbI can only be weak evidence for questioning transfer learning high-resource to low-resource in real-world scenarios. I highly recommend using recently proposed real-world scenarios [1,2].

More importantly, the work does not explain why and how do we get improvement using transfer learning. They remotely address this by hypothesizing the knowledge of transfer is not just encoded in embeddings but also in the model. Considering the related work [3], these claims bring a marginal novelty and still ""how and why"" should be central in this work.  

[1]",0
"This work investigates the performance of transfer learning from resource-rich setup (BookTest, CNN/Daily Mail corpora) to low-resource (bAbI, SQuAD benchmarks) settings. Experiments show poor improvements in 0-shot learning. However, when the model is exposed to few training instances some improvements are observed.

The claims made here require a more comprehensive analysis. I criticize the use of bAbI as a low-resource real-world scenario. bAbI is designed as a unit test and is far from representing many natural language phenomena. Thus, the claims related to bAbI can only be weak evidence for questioning transfer learning high-resource to low-resource in real-world scenarios. I highly recommend using recently proposed real-world scenarios [1,2].

More importantly, the work does not explain why and how do we get improvement using transfer learning. They remotely address this by hypothesizing the knowledge of transfer is not just encoded in embeddings but also in the model. Considering the related work [3], these claims bring a marginal novelty and still ""how and why"" should be central in this work.  

[1]",0
"The paper presents one of the first neural translation systems that operates purely at the character-level, another one being",0
"The paper presents one of the first neural translation systems that operates purely at the character-level, another one being",0
"Overview: This work seems very promising, but I believe it should be compared with more baselines, and more precisely described and explained, from a signal processing point of view.

Pros:
New descriptor
Fast implementation

Cons:
a) Lack of rigor
b) Too long accordingly to the content
c) The computational gain of the algorithm is not clear
d) The work is not compared with its most obvious baseline: a scattering transform

I will detail each cons.

a) Section 1:
The author  motivates the use of scattering transform because it defines a contraction of the space that relies on geometric features.
"" The nonlinearity used in the scattering network is the complex modulus which is piecewise linear.""
A real modulus is piecewise linear. A complex modulus has a shape of bell when interpreting C as R^2. Could you clarify?
\Omega is not introduced.

Could you give a precise reference (page+paper) of this claim: “Higher order nonlinearity refers to |x|^2 instead of |x| as it is usually done in the scattering network.” ?

Section 2:
The motivation of the non-linearity is not clear. First, this non-linearity might potentially increase a lot the variance of your architecture since it depends on higher moments(up to 4). I think a fair analysis would be to compute numerically the normalized variance (e.g. divided by the averaged l^2 norm), as a sanity check. Besides, one should prove that the energy is decreasing. It is not possible to argue that this architecture is similar to a scattering transform which has precise mathematical foundations and those results are required, since the setting is different.

Permutation is not a relevant variability.

The notion of sparsity during the whole paper sometimes refers to the number of 0 value, either the l^1 norm. Mathematically, a small value, even 10^-1000 is still a non 0 value.

Did you compute the graph of the figure 4 on the bird dataset? You might use a ratio instead for clarity. 

The wavelet that is defined is not a morlet wavelet (",0
"Overview: This work seems very promising, but I believe it should be compared with more baselines, and more precisely described and explained, from a signal processing point of view.

Pros:
New descriptor
Fast implementation

Cons:
a) Lack of rigor
b) Too long accordingly to the content
c) The computational gain of the algorithm is not clear
d) The work is not compared with its most obvious baseline: a scattering transform

I will detail each cons.

a) Section 1:
The author  motivates the use of scattering transform because it defines a contraction of the space that relies on geometric features.
"" The nonlinearity used in the scattering network is the complex modulus which is piecewise linear.""
A real modulus is piecewise linear. A complex modulus has a shape of bell when interpreting C as R^2. Could you clarify?
\Omega is not introduced.

Could you give a precise reference (page+paper) of this claim: “Higher order nonlinearity refers to |x|^2 instead of |x| as it is usually done in the scattering network.” ?

Section 2:
The motivation of the non-linearity is not clear. First, this non-linearity might potentially increase a lot the variance of your architecture since it depends on higher moments(up to 4). I think a fair analysis would be to compute numerically the normalized variance (e.g. divided by the averaged l^2 norm), as a sanity check. Besides, one should prove that the energy is decreasing. It is not possible to argue that this architecture is similar to a scattering transform which has precise mathematical foundations and those results are required, since the setting is different.

Permutation is not a relevant variability.

The notion of sparsity during the whole paper sometimes refers to the number of 0 value, either the l^1 norm. Mathematically, a small value, even 10^-1000 is still a non 0 value.

Did you compute the graph of the figure 4 on the bird dataset? You might use a ratio instead for clarity. 

The wavelet that is defined is not a morlet wavelet (",0
"The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.  A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.

 It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work.
 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.
 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.
 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.",0
"The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.  A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.

 It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work.
 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.
 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.
 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.",0
"The authors mention that they are not aiming to have SOTA results.
However, that an ensemble of resnets has lower performance than some of single network results, indicates that further experimentation preferably on larger datasets is necessary.
The literature review could at least mention some existing works such as wide resnets",0
"The authors mention that they are not aiming to have SOTA results.
However, that an ensemble of resnets has lower performance than some of single network results, indicates that further experimentation preferably on larger datasets is necessary.
The literature review could at least mention some existing works such as wide resnets",0
"This paper proposes a new approach to model based reinforcement learning and
evaluates it on 3 ATARI games. The approach involves training a model that
predicts a sequence of rewards and probabilities of losing a life given a
context of frames and a sequence of actions. The controller samples random
sequences of actions and executes the one that balances the probabilities of
earning a point and losing a life given some thresholds. The proposed system
learns to play 3 Atari games both individually and when trained on all 3 in a
multi-task setup at super-human level.

The results presented in the paper are very encouraging but there are many
ad-hoc design choices in the design of the system. The paper also provides
little insight into the importance of the different components of the system.

Main concerns:
- The way predicted rewards and life loss probabilities are combined is very ad-hoc.
  The natural way to do this would be by learning a Q-value, instead different
  rules are devised for different games.
- Is a model actually being learned and improved? It would be good to see
  predictions for several actions sequences from some carefully chosen start
  states. This would be good to see both on a game where the approach works and
  on a game where it fails. The learning progress could also be measured by
  plotting the training loss on a fixed holdout set of sequences.
- How important is the proposed RRNN architecture? Would it still work without
  the residual connections? Would a standard LSTM also work?

Minor points:
- Intro, paragraph 2 - There is a lot of much earlier work on using models in
  RL. For example, see Dyna and ""Memory approaches to reinforcement learning in
  non-Markovian domains"" by Lin and Mitchell to name just two.
- Section 3.1 - Minor point, but using a_i to represent the observation is
  unusual.  Why not use o_i for observations and a_i for actions?
- Section 3.2.2 - Notation again, r_i was used earlier to represent the
  reward at time i but it is being used again for something else.
- Observation 1 seems somewhat out of place. Citing the layer normalization
  paper for the motivation is enough.
- Section 3.2.2, second last paragraph - How is memory decoupled from
  computation here? Models like neural turning machines accomplish this by using
  an external memory, but this looks like an RNN with skip connections.
- Section 3.3, second paragraph - Whether the model overfits or not depends on
  the data. The approach doesn't work with demonstrations precisely because it
  would overfit.
- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy
  instead of Morimoto et al.

Overall I think the paper has some really promising ideas and encouraging
results but is missing a few exploratory/ablation experiments and some polish.",0
"This paper proposes a new approach to model based reinforcement learning and
evaluates it on 3 ATARI games. The approach involves training a model that
predicts a sequence of rewards and probabilities of losing a life given a
context of frames and a sequence of actions. The controller samples random
sequences of actions and executes the one that balances the probabilities of
earning a point and losing a life given some thresholds. The proposed system
learns to play 3 Atari games both individually and when trained on all 3 in a
multi-task setup at super-human level.

The results presented in the paper are very encouraging but there are many
ad-hoc design choices in the design of the system. The paper also provides
little insight into the importance of the different components of the system.

Main concerns:
- The way predicted rewards and life loss probabilities are combined is very ad-hoc.
  The natural way to do this would be by learning a Q-value, instead different
  rules are devised for different games.
- Is a model actually being learned and improved? It would be good to see
  predictions for several actions sequences from some carefully chosen start
  states. This would be good to see both on a game where the approach works and
  on a game where it fails. The learning progress could also be measured by
  plotting the training loss on a fixed holdout set of sequences.
- How important is the proposed RRNN architecture? Would it still work without
  the residual connections? Would a standard LSTM also work?

Minor points:
- Intro, paragraph 2 - There is a lot of much earlier work on using models in
  RL. For example, see Dyna and ""Memory approaches to reinforcement learning in
  non-Markovian domains"" by Lin and Mitchell to name just two.
- Section 3.1 - Minor point, but using a_i to represent the observation is
  unusual.  Why not use o_i for observations and a_i for actions?
- Section 3.2.2 - Notation again, r_i was used earlier to represent the
  reward at time i but it is being used again for something else.
- Observation 1 seems somewhat out of place. Citing the layer normalization
  paper for the motivation is enough.
- Section 3.2.2, second last paragraph - How is memory decoupled from
  computation here? Models like neural turning machines accomplish this by using
  an external memory, but this looks like an RNN with skip connections.
- Section 3.3, second paragraph - Whether the model overfits or not depends on
  the data. The approach doesn't work with demonstrations precisely because it
  would overfit.
- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy
  instead of Morimoto et al.

Overall I think the paper has some really promising ideas and encouraging
results but is missing a few exploratory/ablation experiments and some polish.",0
"In this submission, an interesting approach to character-based language modeling is pursued that retains word-level representations both in the context, and optionally also in the output. However, the approach is not new, cf. (Kim et al. 2015) as cited in the submission, as well as (Jozefowicz et al. 2016). Both Kim and Jozefowicz already go beyond this submission by applying the approach using RNNs/LSTMs. Also, Jozefowicz et al. provide a comparative discussion of different approaches to character-level modeling, which I am missing here, at least by discussing this existing work. THe remaining novelty of the approach then would be its application to machine translation, although it remains somewhat unclear, inhowfar reranking of N-best lists can handle the OOV problem - the translation-related part of the OVV problem should be elaborated here. That said, some of the claims of this submission seems somewhat exaggerated, like the statement in Sec. 2.3: ""making the notion of vocabulary obsolete"", whereas the authors e.g. express doubts concerning the interpretation of perplexity w/o an explicit output vocabulary. For example modeling of especially frequent word forms still can be expected to contribute, as shown in e.g. arXiv:1609.08144

Sec. 2.3: You claim that the objective requires a finite vocabulary. This statement only is correct if the units considered are limited to full word forms. However, using subwords and even individual characters, implicitly larger and even infinite vocabularies can be covered with the log-likelihood criterion. Even though this require a model different from the one proposed here, the corresponding statement should qualified in this respect.

The way character embeddings are used for the output should be clarified. The description in Sec. 2.4 is not explicit enough in my view.

Concerning the configuration of NCE, it would be desirable to get a better idea of how you arrived at your specific configuration and parameterization described in Sec. 3.4.

Sec. 4.1: you might want to mention that (Kim et al. 2015) came to similar conclusions w.r.t. the performance of using character embeddings at the output, and discuss the suggestions for possible improvements given therein.

Sec. 4.2: there are ways to calculate and interpret perplexity for unknown words, cf. (Shaik et al. IWSLT 2013).

Sec. 4.4 and Table 4: the size of the full training vocabulary should be provided here.

Minor comments:
p. 2, bottom: three different input layer -> three different input layers (plural)
Fig. 1: fonts within the figure are way too small
p. 3, first item below Fig. 1: that we will note WE -> that we will denote WE
Sec. 2.3: the parameters estimation -> the parameter estimation (or: the parameters' estimation)
p. 5, first paragraph: in factored way -> in a factored way
p. 5, second paragraph: a n-best list, a nk-best list -> an n-best list, an nk-best list
Sec. 4.2, last sentence: Despite adaptive gradient, -> verb and article missing",0
"In this submission, an interesting approach to character-based language modeling is pursued that retains word-level representations both in the context, and optionally also in the output. However, the approach is not new, cf. (Kim et al. 2015) as cited in the submission, as well as (Jozefowicz et al. 2016). Both Kim and Jozefowicz already go beyond this submission by applying the approach using RNNs/LSTMs. Also, Jozefowicz et al. provide a comparative discussion of different approaches to character-level modeling, which I am missing here, at least by discussing this existing work. THe remaining novelty of the approach then would be its application to machine translation, although it remains somewhat unclear, inhowfar reranking of N-best lists can handle the OOV problem - the translation-related part of the OVV problem should be elaborated here. That said, some of the claims of this submission seems somewhat exaggerated, like the statement in Sec. 2.3: ""making the notion of vocabulary obsolete"", whereas the authors e.g. express doubts concerning the interpretation of perplexity w/o an explicit output vocabulary. For example modeling of especially frequent word forms still can be expected to contribute, as shown in e.g. arXiv:1609.08144

Sec. 2.3: You claim that the objective requires a finite vocabulary. This statement only is correct if the units considered are limited to full word forms. However, using subwords and even individual characters, implicitly larger and even infinite vocabularies can be covered with the log-likelihood criterion. Even though this require a model different from the one proposed here, the corresponding statement should qualified in this respect.

The way character embeddings are used for the output should be clarified. The description in Sec. 2.4 is not explicit enough in my view.

Concerning the configuration of NCE, it would be desirable to get a better idea of how you arrived at your specific configuration and parameterization described in Sec. 3.4.

Sec. 4.1: you might want to mention that (Kim et al. 2015) came to similar conclusions w.r.t. the performance of using character embeddings at the output, and discuss the suggestions for possible improvements given therein.

Sec. 4.2: there are ways to calculate and interpret perplexity for unknown words, cf. (Shaik et al. IWSLT 2013).

Sec. 4.4 and Table 4: the size of the full training vocabulary should be provided here.

Minor comments:
p. 2, bottom: three different input layer -> three different input layers (plural)
Fig. 1: fonts within the figure are way too small
p. 3, first item below Fig. 1: that we will note WE -> that we will denote WE
Sec. 2.3: the parameters estimation -> the parameter estimation (or: the parameters' estimation)
p. 5, first paragraph: in factored way -> in a factored way
p. 5, second paragraph: a n-best list, a nk-best list -> an n-best list, an nk-best list
Sec. 4.2, last sentence: Despite adaptive gradient, -> verb and article missing",0
"The paper describes an extension of the HasheNets work, with several novel twists. Instead of using a single hash function, the proposed HFH approach uses multiple hash function to associate each ""virtual"" (to-be-synthesized) weight location to several components of an underlying parameter vector (shared across all layers). These components are then passed through a small MLP to synthesize the final weight.

This is an interesting and novel idea, and the experiments demonstrate that it improves substantially over HashedNets. However, HashedNets is not a particularly compelling technique for neural network model compression, especially when compared with more recent work on pruning- and quantization-based approaches. The experiments in this paper demonstrate that the proposed approach yields worse accuracy at the same compression ratios as pruning-based approaches, while providing no runtime speedup benefits. While the authors mention the technique is only 20% slower (which I am pleasantly surprised by), I don't understand why this technique should ever be used over competing approaches for the kinds of networks the authors present experimental results on. The authors suggest that the technique could be combined with pruning based approaches... this may be true, but no experiments to this effect are provided. The paper also suggests that ease of setting the compression ratio is a benefit of HFH, but I don't think that's a sufficient win to justify the numerous other downsides (in accuracy and speed). 

In response to a question, the authors point out that the technique works very well for compressing embeddings, and for this setting the technique does appear like a genuinely useful contribution, given the marginal overhead and substantial train-time benefits. If the paper focused on this setting and showed experimental results on e.g. language modeling tasks or other scenarios with high-dimensional sparse/one-hot inputs require large embedding layers, I could enthusiastically recommend acceptance. However for the CNN and MLP networks which are the main focus of the experiments, I don't think the technique is suitable, as much as I like the basic idea.",0
"The paper describes an extension of the HasheNets work, with several novel twists. Instead of using a single hash function, the proposed HFH approach uses multiple hash function to associate each ""virtual"" (to-be-synthesized) weight location to several components of an underlying parameter vector (shared across all layers). These components are then passed through a small MLP to synthesize the final weight.

This is an interesting and novel idea, and the experiments demonstrate that it improves substantially over HashedNets. However, HashedNets is not a particularly compelling technique for neural network model compression, especially when compared with more recent work on pruning- and quantization-based approaches. The experiments in this paper demonstrate that the proposed approach yields worse accuracy at the same compression ratios as pruning-based approaches, while providing no runtime speedup benefits. While the authors mention the technique is only 20% slower (which I am pleasantly surprised by), I don't understand why this technique should ever be used over competing approaches for the kinds of networks the authors present experimental results on. The authors suggest that the technique could be combined with pruning based approaches... this may be true, but no experiments to this effect are provided. The paper also suggests that ease of setting the compression ratio is a benefit of HFH, but I don't think that's a sufficient win to justify the numerous other downsides (in accuracy and speed). 

In response to a question, the authors point out that the technique works very well for compressing embeddings, and for this setting the technique does appear like a genuinely useful contribution, given the marginal overhead and substantial train-time benefits. If the paper focused on this setting and showed experimental results on e.g. language modeling tasks or other scenarios with high-dimensional sparse/one-hot inputs require large embedding layers, I could enthusiastically recommend acceptance. However for the CNN and MLP networks which are the main focus of the experiments, I don't think the technique is suitable, as much as I like the basic idea.",0
"The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning. It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline. In addition, from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct.

The authors put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods. Additionally, the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive.

My major criticism is that the paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature. To make this apparent, I here summarise each paragraph of the conclusion section:

Paragraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline / Some evidence for hypothesis of Mozer & Smolensky, but further investigation needed

Paragraph 2: Introduced 2nd order Taylor method / Does not fare well against baseline

Paragraph 3: Re-training may help but is not fair

Paragraph 4: Brute-force can prune 40-70% in shallow networks

Paragraph 5: Brute-force less effective in deep networks

Paragraph 6: Not all neurons contribute equally to performance of network

The title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method, is not about benchmarking pruning algorithms but is instead about the learnt representations. But only two or three sentences in the conclusion, and no sentence in the part on results in the abstract, even refers to neural representations. In an answer to the pre-review questions the authors stated:

> Furthermore, we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be 
> pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network. This would be 
> impossible if neurons did not belong to the distinct classes we describe.""

But this can already be concluded from the 2nd order method, which has a similar characteristic and is based on other 2nd order methods (not shown here). What is the motivation to introduce a new 2nd order method here?

In addition, some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method. Optimally, one would need to consider all the different ways of pruning (which, of course, scales exponentially with the number of neurons and is computationally infeasible). Notably, the authors do consider this limitation in the context of conventional pruning methods in the conclusions: ""Third, we assumed that pruning could be done in a serial fashion [...]. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process"". But the brute-force pruning process is also serial - why is that not a problem?

All in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods. I would suggest to focus the paper in the following way: first, use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force, or want to learn something about the learnt representations). In this way you need to write little about this second-order tuning methods, and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!). Then concentrate on 2-layer MNIST and a deeper CIFAR10 network. Further focus the paper by adding an itemised list of the exact contributions that you make, and streamline the paper accordingly. These measures could strongly boost the impact of your work but will require a major revision.

PS: I think the confusion starts with the following sentence in the abstract: ""In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning."" Both aspects are pretty orthogonal, but are completely mixed up in the paper.",0
"The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning. It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline. In addition, from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct.

The authors put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods. Additionally, the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive.

My major criticism is that the paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature. To make this apparent, I here summarise each paragraph of the conclusion section:

Paragraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline / Some evidence for hypothesis of Mozer & Smolensky, but further investigation needed

Paragraph 2: Introduced 2nd order Taylor method / Does not fare well against baseline

Paragraph 3: Re-training may help but is not fair

Paragraph 4: Brute-force can prune 40-70% in shallow networks

Paragraph 5: Brute-force less effective in deep networks

Paragraph 6: Not all neurons contribute equally to performance of network

The title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method, is not about benchmarking pruning algorithms but is instead about the learnt representations. But only two or three sentences in the conclusion, and no sentence in the part on results in the abstract, even refers to neural representations. In an answer to the pre-review questions the authors stated:

> Furthermore, we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be 
> pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network. This would be 
> impossible if neurons did not belong to the distinct classes we describe.""

But this can already be concluded from the 2nd order method, which has a similar characteristic and is based on other 2nd order methods (not shown here). What is the motivation to introduce a new 2nd order method here?

In addition, some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method. Optimally, one would need to consider all the different ways of pruning (which, of course, scales exponentially with the number of neurons and is computationally infeasible). Notably, the authors do consider this limitation in the context of conventional pruning methods in the conclusions: ""Third, we assumed that pruning could be done in a serial fashion [...]. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process"". But the brute-force pruning process is also serial - why is that not a problem?

All in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods. I would suggest to focus the paper in the following way: first, use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force, or want to learn something about the learnt representations). In this way you need to write little about this second-order tuning methods, and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!). Then concentrate on 2-layer MNIST and a deeper CIFAR10 network. Further focus the paper by adding an itemised list of the exact contributions that you make, and streamline the paper accordingly. These measures could strongly boost the impact of your work but will require a major revision.

PS: I think the confusion starts with the following sentence in the abstract: ""In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning."" Both aspects are pretty orthogonal, but are completely mixed up in the paper.",0
"This paper presents iterative PoWER, an off-policy variation on PoWER, a policy gradient algorithm in the reward-weighted family.

I'm not familiar enough with this type lower bound scheme to comment on it. It looks like the end result is less conservative step sizes in policy parameter space. All expectation-based algorithms (and their KL-regularized cousins a-la TRPO) take smallish steps, and this might be a sensible way to accelerate them.

The description of the experiments in Section VI is insufficient for reproducibility. Is ""The cart moved right"" supposed to be ""a positive force is applied to the cart""? How is negative force applied? What is the representation of the state? What is the distribution of initial states? A linear policy is insufficient for swing up and balance of a cart-pole. Are you only doing balancing? What is the noise magnitude of the policy? How was it chosen? How long were the episodes?

The footnote at the bottom of page 8 threw me off. If you're using Newton's method, where is the discussion of gradients and Hessians? I thought the argmax_theta operator was a stand-in for an EM-style step, which I how I read Eq (8) in the Kober paper.",0
"This paper presents iterative PoWER, an off-policy variation on PoWER, a policy gradient algorithm in the reward-weighted family.

I'm not familiar enough with this type lower bound scheme to comment on it. It looks like the end result is less conservative step sizes in policy parameter space. All expectation-based algorithms (and their KL-regularized cousins a-la TRPO) take smallish steps, and this might be a sensible way to accelerate them.

The description of the experiments in Section VI is insufficient for reproducibility. Is ""The cart moved right"" supposed to be ""a positive force is applied to the cart""? How is negative force applied? What is the representation of the state? What is the distribution of initial states? A linear policy is insufficient for swing up and balance of a cart-pole. Are you only doing balancing? What is the noise magnitude of the policy? How was it chosen? How long were the episodes?

The footnote at the bottom of page 8 threw me off. If you're using Newton's method, where is the discussion of gradients and Hessians? I thought the argmax_theta operator was a stand-in for an EM-style step, which I how I read Eq (8) in the Kober paper.",0
"A few issues with this paper:
1- I find finding #2 trivial and unworthy of mention, but the author don't seem to agree with me that it is. See discussions.
2- Finding #1 relies on Fig #4, which appears very noisy and doesn't provide any error analysis. It makes me question how robust this finding is. One would have naively expected the power usage trend to mirror Fig #3, but given the level of noise, I can't convince myself whether the null hypothesis of there being no dependency between batch size and power consumption is more likely than the alternative.
3- Paper is unfriendly to colorblind readers (or those with B/W printers)

Overall, this paper is a reasonable review of where we are in terms of SOTA vision architectures, but doesn't provide much new insight. I found most interesting the clear illustration that VGG models stand out in terms of being a bad tradeoff in resource-constrained environments (too many researchers are tempted to benchmark their model compression algorithm on VGG-class models because that's always where one can show 10x improvements without doing much.)",0
"A few issues with this paper:
1- I find finding #2 trivial and unworthy of mention, but the author don't seem to agree with me that it is. See discussions.
2- Finding #1 relies on Fig #4, which appears very noisy and doesn't provide any error analysis. It makes me question how robust this finding is. One would have naively expected the power usage trend to mirror Fig #3, but given the level of noise, I can't convince myself whether the null hypothesis of there being no dependency between batch size and power consumption is more likely than the alternative.
3- Paper is unfriendly to colorblind readers (or those with B/W printers)

Overall, this paper is a reasonable review of where we are in terms of SOTA vision architectures, but doesn't provide much new insight. I found most interesting the clear illustration that VGG models stand out in terms of being a bad tradeoff in resource-constrained environments (too many researchers are tempted to benchmark their model compression algorithm on VGG-class models because that's always where one can show 10x improvements without doing much.)",0
"The paper presents a new environment, called Retro Learning Environment (RLE), for reinforcement learning. The authors focus on Super Nintendo but claim that the interface supports many others (including ALE). Benchmark results are given for standard algorithms in 5 new Super Nintendo games, and some results using a new ""rivalry metric"".

These environments (or, more generally, standardized evaluation methods like public data sets, competitions, etc.) have a long history of improving the quality of AI and machine learning research. One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results. In this sense, the RLE could be a worthy contribution to the field by encouraging new challenging domains for research.

That said, the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains. The results of experiments themselves are for existing algorithms. There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning. And, yes, domain knowledge helps, but this is obvious. The rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly, it gets evaluated on how well it does against the in-game AI! 

Also the part of the paper that describes the scientific results (especially the rivalry training) is less polished, so this is disappointing. In the end, I'm not very excited about this paper.

I was hoping for a more significant scientific contribution to accompany in this new environment. It's not clear if this is necessary for publication, but also it's not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example, mloss.org could be a better 'venue', JMLR has an associated journal track for accompanying papers:",0
"The paper presents a new environment, called Retro Learning Environment (RLE), for reinforcement learning. The authors focus on Super Nintendo but claim that the interface supports many others (including ALE). Benchmark results are given for standard algorithms in 5 new Super Nintendo games, and some results using a new ""rivalry metric"".

These environments (or, more generally, standardized evaluation methods like public data sets, competitions, etc.) have a long history of improving the quality of AI and machine learning research. One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results. In this sense, the RLE could be a worthy contribution to the field by encouraging new challenging domains for research.

That said, the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains. The results of experiments themselves are for existing algorithms. There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning. And, yes, domain knowledge helps, but this is obvious. The rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly, it gets evaluated on how well it does against the in-game AI! 

Also the part of the paper that describes the scientific results (especially the rivalry training) is less polished, so this is disappointing. In the end, I'm not very excited about this paper.

I was hoping for a more significant scientific contribution to accompany in this new environment. It's not clear if this is necessary for publication, but also it's not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example, mloss.org could be a better 'venue', JMLR has an associated journal track for accompanying papers:",0
"This paper presents a hierarchical attention-based method for document classification. 
The main idea is to first run a bidirectional LSTM to get global context vector, and then run another attention-based bidirectional LSTM that uses the final hidden state from the first pass to weight local context vectors (TS-ATT). 
A simpler architecture that removes the first LSTM and uses the output of the second LSTM as the global context vector is also proposed (SS-ATT). 
Experiments on three datasets are presented, however the results are mostly not state-of-the-art.

I think the idea is nice, but the experiment results are not convincing enough to justify this new model architecture. 
Why is your Yelp 2013 dataset smaller than the original Tang et al, 2015 paper that has ~300k documents? 
I noticed your other datasets are also quite small. Is it because your model is difficult to scale to large datasets?
You should also include results from Tang et al., 2015 in Table 2 that achieves 65.1% accuracy on Yelp 2013 (why is your number so much lower?)
I also suggest removing phrases such as ""Learning to Understand"" when presenting their model.
Overall, I think that this submission is a better fit for the workshop.

Minor comments:
- gloal -> global
- Not needing a pretrained embeddings, while of course nice, is not that big of a deal. Various models will work just fine without pretrained embeddings.",0
"This paper presents a hierarchical attention-based method for document classification. 
The main idea is to first run a bidirectional LSTM to get global context vector, and then run another attention-based bidirectional LSTM that uses the final hidden state from the first pass to weight local context vectors (TS-ATT). 
A simpler architecture that removes the first LSTM and uses the output of the second LSTM as the global context vector is also proposed (SS-ATT). 
Experiments on three datasets are presented, however the results are mostly not state-of-the-art.

I think the idea is nice, but the experiment results are not convincing enough to justify this new model architecture. 
Why is your Yelp 2013 dataset smaller than the original Tang et al, 2015 paper that has ~300k documents? 
I noticed your other datasets are also quite small. Is it because your model is difficult to scale to large datasets?
You should also include results from Tang et al., 2015 in Table 2 that achieves 65.1% accuracy on Yelp 2013 (why is your number so much lower?)
I also suggest removing phrases such as ""Learning to Understand"" when presenting their model.
Overall, I think that this submission is a better fit for the workshop.

Minor comments:
- gloal -> global
- Not needing a pretrained embeddings, while of course nice, is not that big of a deal. Various models will work just fine without pretrained embeddings.",0
"The paper explores a VAE architecture and training procedure that allows to generate new samples of a concept based on several exemplars that are shown to the model. The proposed architecture processes the set of exemplars with a recurrent neural network and aggregation procedure similar to the one used in Matching Networks. The resulting ""summary"" is used to condition a generative model (a VAE) that produces new samples of the same kind as the exemplars shown. The proposed aggregation and conditioning procedure are better suited to sets of exemplars that come from several classes than simple averaging.
Perhaps surprisingly the model generalizes from generation conditioned on samples from 2 classes to generation conditioned on samples from 4 classes.
The experiments are conducted on the OMNIGLOT dataset and are quite convincing. An explicit comparison to previous works is lacking, but this is explained in the appendices, and a comparison to architectures similar to previous work is presented.",0
"The paper explores a VAE architecture and training procedure that allows to generate new samples of a concept based on several exemplars that are shown to the model. The proposed architecture processes the set of exemplars with a recurrent neural network and aggregation procedure similar to the one used in Matching Networks. The resulting ""summary"" is used to condition a generative model (a VAE) that produces new samples of the same kind as the exemplars shown. The proposed aggregation and conditioning procedure are better suited to sets of exemplars that come from several classes than simple averaging.
Perhaps surprisingly the model generalizes from generation conditioned on samples from 2 classes to generation conditioned on samples from 4 classes.
The experiments are conducted on the OMNIGLOT dataset and are quite convincing. An explicit comparison to previous works is lacking, but this is explained in the appendices, and a comparison to architectures similar to previous work is presented.",0
"UPDATE:  I have read the authors' responses.  I did not read the social media comments about this paper prior to reviewing it.  

I appreciate the authors' updates in response to the reviewer comments.  Overall, however, my review stands.  The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach.  I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well.  I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not).  This is a general point and the program chairs may disagree with it, of course.

I have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus.  

************************

ORIGINAL REVIEW:

The authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus.  This is a nice result to know about--yet another example of a really nice result that one can get the first time one applies such methods to an old task--and all of the work that went into getting it looks solid (and likely involved some significant engineering effort).  However, this in itself is not sufficiently novel for publication at ICLR.  The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language.  Some specifics on what I think needs to be revised:

- First, the claim of being the first to do sentence-level lipreading.  As mentioned in a pre-review comment, this is not true.  The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public).  Ideally the title should also be changed in light of this.

- The comparison with human lipreaders needs to be qualified a bit.  This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints.  This is great, but not a general statement about LipNet vs. humans.

- The paper contains some unnecessary motivational platitudes.  We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well.  The McGurk effect does not show that lipreading plays a crucial role in human communication.

- It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well.  So I am not sure about the ""importance of spatiotemporal feature extraction"" as stated in the conclusion.

Some more minor comments, typos, etc.:

- citations for LSTMs, CTC, etc. should be provided the first time they are mentioned.
- I did not quite follow the justification for upsampling.
- what is meant by ""lip-rounding vowels""?  They seem to include almost all English vowels.
- Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one?  Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it.
- ""Given that the speakers are British, the confusion between /aa/ and /ay/..."" -- I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American).
- The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig. 3(b,c).  For example, there does not seem to be any confusion between /m/ and /b/ or between /m/ and /p/.
- ""lipreading actuations"":  I am not sure what ""actuations"" means in this context
- ""palato-alvealoar"" --> ""palato-alveolar""
- ""Articulatorily alveolar"" --> ""Alveolar""?",0
"UPDATE:  I have read the authors' responses.  I did not read the social media comments about this paper prior to reviewing it.  

I appreciate the authors' updates in response to the reviewer comments.  Overall, however, my review stands.  The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach.  I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well.  I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not).  This is a general point and the program chairs may disagree with it, of course.

I have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus.  

************************

ORIGINAL REVIEW:

The authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus.  This is a nice result to know about--yet another example of a really nice result that one can get the first time one applies such methods to an old task--and all of the work that went into getting it looks solid (and likely involved some significant engineering effort).  However, this in itself is not sufficiently novel for publication at ICLR.  The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language.  Some specifics on what I think needs to be revised:

- First, the claim of being the first to do sentence-level lipreading.  As mentioned in a pre-review comment, this is not true.  The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public).  Ideally the title should also be changed in light of this.

- The comparison with human lipreaders needs to be qualified a bit.  This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints.  This is great, but not a general statement about LipNet vs. humans.

- The paper contains some unnecessary motivational platitudes.  We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well.  The McGurk effect does not show that lipreading plays a crucial role in human communication.

- It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well.  So I am not sure about the ""importance of spatiotemporal feature extraction"" as stated in the conclusion.

Some more minor comments, typos, etc.:

- citations for LSTMs, CTC, etc. should be provided the first time they are mentioned.
- I did not quite follow the justification for upsampling.
- what is meant by ""lip-rounding vowels""?  They seem to include almost all English vowels.
- Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one?  Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it.
- ""Given that the speakers are British, the confusion between /aa/ and /ay/..."" -- I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American).
- The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig. 3(b,c).  For example, there does not seem to be any confusion between /m/ and /b/ or between /m/ and /p/.
- ""lipreading actuations"":  I am not sure what ""actuations"" means in this context
- ""palato-alvealoar"" --> ""palato-alveolar""
- ""Articulatorily alveolar"" --> ""Alveolar""?",0
"The paper proposed to analyze several recently developed machine readers and found that some machine readers could potentially take advantages of the entity marker (given that the same marker points out to the same entity). I usually like analysis papers, but I found the argument proposed in this paper not very clear.

I like the experiments on the Stanford reader, which shows that the entity marker in fact helps the Stanford reader on WDW. I found that results rather interesting.

However, I found the organization and the overall message of this paper quite confusing. First of all, it feels that the authors want to explain the above behavior with some definition of the “structures”. However, I am not sure that how successful the attempt is. For me, it is still not clear what the structures are. This makes reading section 4 a bit frustrating. 

I am also not sure what is the take home message of this paper. Does it mean that the entity marking should be used in the MR models? Should we design models that can also model the entity reference at the same time? What are the roles of the linguistic features here? Should we use linguistic structure to overcome the reference issue?

Overall, I feel that the analysis is interesting, but I feel that the paper can benefit from having a more focused argument.",0
"The paper proposed to analyze several recently developed machine readers and found that some machine readers could potentially take advantages of the entity marker (given that the same marker points out to the same entity). I usually like analysis papers, but I found the argument proposed in this paper not very clear.

I like the experiments on the Stanford reader, which shows that the entity marker in fact helps the Stanford reader on WDW. I found that results rather interesting.

However, I found the organization and the overall message of this paper quite confusing. First of all, it feels that the authors want to explain the above behavior with some definition of the “structures”. However, I am not sure that how successful the attempt is. For me, it is still not clear what the structures are. This makes reading section 4 a bit frustrating. 

I am also not sure what is the take home message of this paper. Does it mean that the entity marking should be used in the MR models? Should we design models that can also model the entity reference at the same time? What are the roles of the linguistic features here? Should we use linguistic structure to overcome the reference issue?

Overall, I feel that the analysis is interesting, but I feel that the paper can benefit from having a more focused argument.",0
"This paper presents a principled optimization method for SGNS (word2vec).

While the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see ""Improving Distributional Similarity with Lessons Learned from Word Embeddings"", (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future.",0
"This paper presents a principled optimization method for SGNS (word2vec).

While the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see ""Improving Distributional Similarity with Lessons Learned from Word Embeddings"", (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future.",0
"This paper proposes to initialize the weights of a deep neural network layer-wise with a marginal Fisher analysis model, making use of potentially the similarity metric.
 
Pros: 
There are a lot of experiments, albeit small datasets, that the authors tested their proposed method on.

Cons:
lacking baseline such as discriminatively trained convolutional network on standard dataset such as CIFAR-10.
It is also unclear how costly in computation to compute the association matrix A in equation 4.

This is an OK paper, where a new idea is proposed, and combined with other existing ideas such as greedy-layerwise stacking, dropout, and denoising auto-encoders.
However, there have been many papers with similar ideas perhaps 3-5 years ago, e.g. SPCANet. 

Therefore, the main novelty is the use of marginal Fisher Analysis as a new layer. This would be ok, but the baselines to demonstrate that this approach works better is missing. In particular, I'd like to see a conv net or fully connected net trained from scratch with good initialization would do at these problems.

To improve the paper, the authors should try to demonstrate without doubt that initializing layers with MFA is better than just random weight matrices.",0
"This paper proposes to initialize the weights of a deep neural network layer-wise with a marginal Fisher analysis model, making use of potentially the similarity metric.
 
Pros: 
There are a lot of experiments, albeit small datasets, that the authors tested their proposed method on.

Cons:
lacking baseline such as discriminatively trained convolutional network on standard dataset such as CIFAR-10.
It is also unclear how costly in computation to compute the association matrix A in equation 4.

This is an OK paper, where a new idea is proposed, and combined with other existing ideas such as greedy-layerwise stacking, dropout, and denoising auto-encoders.
However, there have been many papers with similar ideas perhaps 3-5 years ago, e.g. SPCANet. 

Therefore, the main novelty is the use of marginal Fisher Analysis as a new layer. This would be ok, but the baselines to demonstrate that this approach works better is missing. In particular, I'd like to see a conv net or fully connected net trained from scratch with good initialization would do at these problems.

To improve the paper, the authors should try to demonstrate without doubt that initializing layers with MFA is better than just random weight matrices.",0
"This paper presents a new technique for adapting a neural network to a new task for which there is not a lot of training data. The most widely used current technique is that of fine-tuning. The idea in this paper is to instead learn a network that learns features that are complementary to the fixed network. Additionally, the authors consider the setting where the new network/features are “stitched” to the old one at various levels in the hieararchy, rather that it just being a parallel “tower”. 

This work is similar in spirit (if not in some details) to the Progressive Nets paper by Rusu et al, as already discussed. The motivations and experiments are certainly different so this submission has merit on its own.

The idea of learning a “residual” with the stitched connnections is very similar in spirit to the ResNet work. It would be nice to compare and contrast those approaches.

I’ve never seen a batch being used 5 times in a row during training, does this work better than just regular SGD?

In Figure 5 it’d be nice to label the y-axis. That Figure would also benefit from not being a bar chart, but simply emulating Figure 4, which is much more readable!

Figure 5 again: what is an untrained model? It’s not immediately obvious why this is a good idea at all. Is TFT-1 simply fine-tuning one more layer than “Retrain Softmax”?

I think that the results at the end of section 3 are a bit weak because of usage of a big network. I would definitely like to see how the results change if using a smaller net.

The authors claim throughout the paper that the purpose of the added connections and layers is to learn *complementary* features and they show this with some figures. The latter are a convinving evidence, but not proof or guarantee that this is what is actually happening. I suggest the authors consider adding an explicit constraint in their loss that encourages that, e.g. by having a soft orthogonality constraing (assuming one can project intermediate features to some common feature dimensionality). The usage of very small L2 regularization maybe achieves the same thing, but there’s no evidence for that in the paper (in that we don’t have any visualizations of what happens if there’s no L2 reg.).

One of the big questions for me while reading the paper was how would an ensemble of 2 pre-trained nets would do on the tasks that the authors consider. This is especially relevant in the cars classification example, where I suspect that a strong baseline is that of fine-tuning VGG on this task, fine-tuning resnet on this task, and possibly training a linear combination of the two outputs or just averaging them naively.

Disappointing that there are no results in figure 4, 5 and 8 except the ones from this paper. It’s really hard to situate this paper if we don’t actually know how it compares to previously published results.


In general, this was an interesting and potentially useful piece of work. The problem of efficiently reusing the previously trained classifier for retraining on a small set is certainly interesting to the community. While I think that this paper takes a good step in the right direction, it falls a bit short in some dimensions (comparisons with more serious baselines, more understanding etc).",0
"This paper presents a new technique for adapting a neural network to a new task for which there is not a lot of training data. The most widely used current technique is that of fine-tuning. The idea in this paper is to instead learn a network that learns features that are complementary to the fixed network. Additionally, the authors consider the setting where the new network/features are “stitched” to the old one at various levels in the hieararchy, rather that it just being a parallel “tower”. 

This work is similar in spirit (if not in some details) to the Progressive Nets paper by Rusu et al, as already discussed. The motivations and experiments are certainly different so this submission has merit on its own.

The idea of learning a “residual” with the stitched connnections is very similar in spirit to the ResNet work. It would be nice to compare and contrast those approaches.

I’ve never seen a batch being used 5 times in a row during training, does this work better than just regular SGD?

In Figure 5 it’d be nice to label the y-axis. That Figure would also benefit from not being a bar chart, but simply emulating Figure 4, which is much more readable!

Figure 5 again: what is an untrained model? It’s not immediately obvious why this is a good idea at all. Is TFT-1 simply fine-tuning one more layer than “Retrain Softmax”?

I think that the results at the end of section 3 are a bit weak because of usage of a big network. I would definitely like to see how the results change if using a smaller net.

The authors claim throughout the paper that the purpose of the added connections and layers is to learn *complementary* features and they show this with some figures. The latter are a convinving evidence, but not proof or guarantee that this is what is actually happening. I suggest the authors consider adding an explicit constraint in their loss that encourages that, e.g. by having a soft orthogonality constraing (assuming one can project intermediate features to some common feature dimensionality). The usage of very small L2 regularization maybe achieves the same thing, but there’s no evidence for that in the paper (in that we don’t have any visualizations of what happens if there’s no L2 reg.).

One of the big questions for me while reading the paper was how would an ensemble of 2 pre-trained nets would do on the tasks that the authors consider. This is especially relevant in the cars classification example, where I suspect that a strong baseline is that of fine-tuning VGG on this task, fine-tuning resnet on this task, and possibly training a linear combination of the two outputs or just averaging them naively.

Disappointing that there are no results in figure 4, 5 and 8 except the ones from this paper. It’s really hard to situate this paper if we don’t actually know how it compares to previously published results.


In general, this was an interesting and potentially useful piece of work. The problem of efficiently reusing the previously trained classifier for retraining on a small set is certainly interesting to the community. While I think that this paper takes a good step in the right direction, it falls a bit short in some dimensions (comparisons with more serious baselines, more understanding etc).",0
"This paper provides two RNN-based architectures for extractive document summarization. The first, ""Classify"", reads in the whole document and traverses the sentences a second time to decide whether to include them or not (0/1 decisions). The second, ""Select"",  reads in the whole document and picks the most relevant sentence one at the time. The models assume that oracle extractive summaries exist, and a pseudo ground-truth generation procedure is used, which mimics Svore et al. (2007) among others. 

Overall, this paper seems a small increment over Cheng & Lapata (2016) and performance is similar or worse to that paper. The problem of single document extractive summarization is not particularly exciting since in DUC 2002 (14 years ago) existing models could not beat the lead baseline (which selects the first sentences of the document). It's a pity that this paper doesn't address the most interesting problems of abstractive summarization or apply the proposed approach to multi-document summarization. It's also a little disappointing that the maximum sentence length had to be capped to 50, which suggests the model has some trouble to scale.",0
"This paper provides two RNN-based architectures for extractive document summarization. The first, ""Classify"", reads in the whole document and traverses the sentences a second time to decide whether to include them or not (0/1 decisions). The second, ""Select"",  reads in the whole document and picks the most relevant sentence one at the time. The models assume that oracle extractive summaries exist, and a pseudo ground-truth generation procedure is used, which mimics Svore et al. (2007) among others. 

Overall, this paper seems a small increment over Cheng & Lapata (2016) and performance is similar or worse to that paper. The problem of single document extractive summarization is not particularly exciting since in DUC 2002 (14 years ago) existing models could not beat the lead baseline (which selects the first sentences of the document). It's a pity that this paper doesn't address the most interesting problems of abstractive summarization or apply the proposed approach to multi-document summarization. It's also a little disappointing that the maximum sentence length had to be capped to 50, which suggests the model has some trouble to scale.",0
"This paper proposed a hardware accelerator for DNN. It utilized the fact that DNN are very tolerant to low precision inference and outperforms a state-of-the-art bit-parallel accelerator by 1.90x without any loss in accuracy while it is 1.17x more energy efficient. TRT requires no network retraining. It achieved super linear scales of performance with area.

The first concern is that this paper doesn't seem very well-suited to ICLR. The circuit diagrams makes it more interesting for the hardware or circuit design community. 

The second concern is the ""take-away for machine learning community"", seeing from the response, the take-away is using low-precision to make inference cheaper. This is not novel enough. In last year's ICLR, there were at least 4 papers discussing using low precision to make DNN more efficient. These ideas have also been explored in the authors' previous papers.",0
"This paper proposed a hardware accelerator for DNN. It utilized the fact that DNN are very tolerant to low precision inference and outperforms a state-of-the-art bit-parallel accelerator by 1.90x without any loss in accuracy while it is 1.17x more energy efficient. TRT requires no network retraining. It achieved super linear scales of performance with area.

The first concern is that this paper doesn't seem very well-suited to ICLR. The circuit diagrams makes it more interesting for the hardware or circuit design community. 

The second concern is the ""take-away for machine learning community"", seeing from the response, the take-away is using low-precision to make inference cheaper. This is not novel enough. In last year's ICLR, there were at least 4 papers discussing using low precision to make DNN more efficient. These ideas have also been explored in the authors' previous papers.",0
"CONTRIBUTIONS 
This paper introduces a method for learning semantic ""word-like"" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs. Joint training allows one to embed both image and spoken language captions into a shared representation space. Audio-visual groundings are generated by measuring affinity between image patches and audio clips. This allows the model to relate specific visual regions to specific audio segments. Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.


NOVELTY+SIGNIFICANCE
As correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).

However, bidirectional image/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016). Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work. The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.

The methods employed for this association are relatively straightforward combination of standard techniques with limited novelty. The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.


MISSING CITATION
There is a lot of work in this area spanning computer vision, natural language, and speech recognition. One key missing reference:

Ngiam, et al. ""Multimodal deep learning."" ICML 2011


POSITIVE POINTS
- Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image/audio retrieval
- The presented method performs efficient acoustic pattern discovery
- The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs

NEGATIVE POINTS
- Limited novelty, especially compared with Harwath et al, NIPS 2016
- Although it gives good results, the clustering method has limited novelty and feels heuristic
- The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices",0
"CONTRIBUTIONS 
This paper introduces a method for learning semantic ""word-like"" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs. Joint training allows one to embed both image and spoken language captions into a shared representation space. Audio-visual groundings are generated by measuring affinity between image patches and audio clips. This allows the model to relate specific visual regions to specific audio segments. Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.


NOVELTY+SIGNIFICANCE
As correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).

However, bidirectional image/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016). Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work. The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.

The methods employed for this association are relatively straightforward combination of standard techniques with limited novelty. The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.


MISSING CITATION
There is a lot of work in this area spanning computer vision, natural language, and speech recognition. One key missing reference:

Ngiam, et al. ""Multimodal deep learning."" ICML 2011


POSITIVE POINTS
- Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image/audio retrieval
- The presented method performs efficient acoustic pattern discovery
- The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs

NEGATIVE POINTS
- Limited novelty, especially compared with Harwath et al, NIPS 2016
- Although it gives good results, the clustering method has limited novelty and feels heuristic
- The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices",0
"UPDATE: I have read the authors' rebuttal and also the other comments in this paper's thread. My thoughts have not changed.

The authors propose using a mixture prior rather than a uni-modal
prior for variational auto-encoders. They argue that the simple
uni-modal prior ""hinders the overall expressivity of the learned model
as it cannot possibly capture more complex aspects of the data
distribution.""

I find the motivation of the paper suspicious because while the prior
may be uni-modal, the posterior distribution is certainly not.
Furthermore, a uni-modal distribution on the latent variable space can
certainly still lead to the capturing of complex, multi-modal data
distributions. (As the most trivial case, take the latent variable
space to be a uniform distribution; take the likelihood to be a
point mass given by applying the true data distribution's inverse CDF
to the uniform. Such a model can capture any distribution.)

In addition, multi-modality is arguably an overfocused concept in the
literature, where the (latent variable) space is hardly anymore worth
capturing from a mixture of simple distributions when it is often a
complex nonlinear space. It is unclear from the experiments how much
the influence of the prior's multimodality influences the posterior to
capture more complex phenomena, and whether this is any better than
considering a more complex (but still reparameterizable) distribution
on the latent space.

I recommend that this paper be rejected, and encourage the authors to
more extensively study the effect of different priors.

I'd also like to make two additional comments:

While there is no length restriction at ICLR, the 14 page document can
be significantly condensed without loss of describing their innovation
or clarity. I recommend the authors do so.

Finally, I think it's important to note the controversy in this paper.
It was submitted with many significant incomplete details (e.g., no experiments,
many missing citations, a figure placed inside that was pencilled in
by hand, and several missing paragraphs). These details were not
completed until roughly a week(?) later. I recommend the chairs discuss
this in light of what should be allowed next year.",0
"UPDATE: I have read the authors' rebuttal and also the other comments in this paper's thread. My thoughts have not changed.

The authors propose using a mixture prior rather than a uni-modal
prior for variational auto-encoders. They argue that the simple
uni-modal prior ""hinders the overall expressivity of the learned model
as it cannot possibly capture more complex aspects of the data
distribution.""

I find the motivation of the paper suspicious because while the prior
may be uni-modal, the posterior distribution is certainly not.
Furthermore, a uni-modal distribution on the latent variable space can
certainly still lead to the capturing of complex, multi-modal data
distributions. (As the most trivial case, take the latent variable
space to be a uniform distribution; take the likelihood to be a
point mass given by applying the true data distribution's inverse CDF
to the uniform. Such a model can capture any distribution.)

In addition, multi-modality is arguably an overfocused concept in the
literature, where the (latent variable) space is hardly anymore worth
capturing from a mixture of simple distributions when it is often a
complex nonlinear space. It is unclear from the experiments how much
the influence of the prior's multimodality influences the posterior to
capture more complex phenomena, and whether this is any better than
considering a more complex (but still reparameterizable) distribution
on the latent space.

I recommend that this paper be rejected, and encourage the authors to
more extensively study the effect of different priors.

I'd also like to make two additional comments:

While there is no length restriction at ICLR, the 14 page document can
be significantly condensed without loss of describing their innovation
or clarity. I recommend the authors do so.

Finally, I think it's important to note the controversy in this paper.
It was submitted with many significant incomplete details (e.g., no experiments,
many missing citations, a figure placed inside that was pencilled in
by hand, and several missing paragraphs). These details were not
completed until roughly a week(?) later. I recommend the chairs discuss
this in light of what should be allowed next year.",0
"This paper introduces pointer-network neural networks, which are applied to referring expressions in three small-scale language modeling tasks: dialogue modeling, recipe modeling and news article modeling. When conditioned on the co-reference chain, the proposed models outperform standard sequence-to-sequence models with attention.

The proposed models are essentially variants of pointer networks with copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Ling et al., 2016), which have been modified to take into account reference chains. As such, the main architectural novelty lies in 1) restricting the pointer mechanism to focus on co-referenced entities, 2) applying pointer mechanism to 2D arrays (tables), and 3) training with supervised alignments. Although useful in practice, these are minor contributions from an architectural perspective.

The empirical contributions are centred around measuring perplexity on the three language modeling tasks. Measuring perplexity is typical for standard language modeling tasks, but is really an unreliable proxy for dialogue modeling and recipe generation performance. In addition to this, both the dialogue and recipe tasks are tiny compared to standard language modeling tasks. This makes it difficult to evaluate the impact of the dialogue and recipe modeling results. For example, if one was to bootstrap from a larger corpus, it seems likely that a standard sequence-to-sequence model with attention would yield performance comparable to the proposed models (with enough data, the attention mechanism could learn to align referring entities by itself). The language modeling task on news article (Gigaword) seems to yield the most conclusive results. However, the dataset for this task is non-standard and results are provided for only a single baseline. Overall, this limits the conclusions we can draw from the empirical experiments.


Finally, the paper itself contains many errors, including mathematical errors, grammatical errors and typos:
- Eq. (1) is missing a sum over $z_i$.
- ""into the a decoder LSTM"" -> ""into the decoder LSTM""
- ""denoted as his"" -> ""denoted as""
- ""Surprising,"" -> ""Surprisingly,""
- ""torkens"" -> ""tokens""
- ""if follows that the next token"" -> ""the next token""
- In the ""COREFERENCE BASED LANGUAGE MODEL"" sub-section, what does $M$ denote?
- In the sentence: ""The attribute of each column is denoted as $s_c, where $c$ is the c-th attribute"". For these definitions to be make sense, $s_c$ has to be a one-hot vector. If yes, please clarify this in the text.
- ""the weighted sum is performed"" -> ""the weighted sum is computed""
- ""a attribute"" -> ""an attribute""
- In the paragraph on Pointer Switch, change $p(z_{i,v} |s_{i,v}) = 1$ -> $p(z_{i,v} |s_{i,v}) = 0$.
- In the ""Table Pointer"" paragraph, I assume you mean outer-product instead of cross-product? Otherwise, I don't see how the equations add up.


Other comments:
- For the ""Attention based decoder"", is the attention computed using the word embeddings themselves or the hidden states of the sentence encoder? Also, it applied only to the previous turn of the dialogue or to the entire dialogue history? Please clarify this.
- What's the advantage of using an ""Entity state update"" rule, compared to a pointer network or copy network, which you used in the dialogue and recipe tasks? Please elaborate on this.
- In the Related Work section, the following sentence is not quite accurate: ""For the task oriented dialogues, most of them embed the seq2seq model in traditional dialogue systems while our model queries the database directly."". There are task-oriented dialogue models which do query databases during natural language generation. See, for example, ""A Network-based End-to-End Trainable Task-oriented Dialogue System"" by Wen et al.",0
"This paper introduces pointer-network neural networks, which are applied to referring expressions in three small-scale language modeling tasks: dialogue modeling, recipe modeling and news article modeling. When conditioned on the co-reference chain, the proposed models outperform standard sequence-to-sequence models with attention.

The proposed models are essentially variants of pointer networks with copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Ling et al., 2016), which have been modified to take into account reference chains. As such, the main architectural novelty lies in 1) restricting the pointer mechanism to focus on co-referenced entities, 2) applying pointer mechanism to 2D arrays (tables), and 3) training with supervised alignments. Although useful in practice, these are minor contributions from an architectural perspective.

The empirical contributions are centred around measuring perplexity on the three language modeling tasks. Measuring perplexity is typical for standard language modeling tasks, but is really an unreliable proxy for dialogue modeling and recipe generation performance. In addition to this, both the dialogue and recipe tasks are tiny compared to standard language modeling tasks. This makes it difficult to evaluate the impact of the dialogue and recipe modeling results. For example, if one was to bootstrap from a larger corpus, it seems likely that a standard sequence-to-sequence model with attention would yield performance comparable to the proposed models (with enough data, the attention mechanism could learn to align referring entities by itself). The language modeling task on news article (Gigaword) seems to yield the most conclusive results. However, the dataset for this task is non-standard and results are provided for only a single baseline. Overall, this limits the conclusions we can draw from the empirical experiments.


Finally, the paper itself contains many errors, including mathematical errors, grammatical errors and typos:
- Eq. (1) is missing a sum over $z_i$.
- ""into the a decoder LSTM"" -> ""into the decoder LSTM""
- ""denoted as his"" -> ""denoted as""
- ""Surprising,"" -> ""Surprisingly,""
- ""torkens"" -> ""tokens""
- ""if follows that the next token"" -> ""the next token""
- In the ""COREFERENCE BASED LANGUAGE MODEL"" sub-section, what does $M$ denote?
- In the sentence: ""The attribute of each column is denoted as $s_c, where $c$ is the c-th attribute"". For these definitions to be make sense, $s_c$ has to be a one-hot vector. If yes, please clarify this in the text.
- ""the weighted sum is performed"" -> ""the weighted sum is computed""
- ""a attribute"" -> ""an attribute""
- In the paragraph on Pointer Switch, change $p(z_{i,v} |s_{i,v}) = 1$ -> $p(z_{i,v} |s_{i,v}) = 0$.
- In the ""Table Pointer"" paragraph, I assume you mean outer-product instead of cross-product? Otherwise, I don't see how the equations add up.


Other comments:
- For the ""Attention based decoder"", is the attention computed using the word embeddings themselves or the hidden states of the sentence encoder? Also, it applied only to the previous turn of the dialogue or to the entire dialogue history? Please clarify this.
- What's the advantage of using an ""Entity state update"" rule, compared to a pointer network or copy network, which you used in the dialogue and recipe tasks? Please elaborate on this.
- In the Related Work section, the following sentence is not quite accurate: ""For the task oriented dialogues, most of them embed the seq2seq model in traditional dialogue systems while our model queries the database directly."". There are task-oriented dialogue models which do query databases during natural language generation. See, for example, ""A Network-based End-to-End Trainable Task-oriented Dialogue System"" by Wen et al.",0
"The authors propose a method to generate adversarial examples w/o relying on knowledge of the network architecture or network gradients.

The idea has some merit, however, as mentioned by one of the reviewers, the field has been studied widely, including black box setups.

My main concern is that the first set of experiments allows images that are not in image space. The authors acknowledge this fact on page 7 in the first paragraph. In my opinion, this renders these experiments completely meaningless. At the very least, the outcome is not surprising to me at all.

The greedy search procedure remedies this issue. The description of the proposed method is somewhat convoluted. AFAICT, first a candidate set of pixels is generated by using PERT. Then the pixels are perturbed using CYCLIC.
It is not clear why this approach results in good/minimal perturbations as the candidate pixels are found using a large ""p"" that can result in images outside the image space. The choice of this method does not seem to be motivated by the authors.

In conclusion, while the authors to an interesting investigation and propose a method to generate adversarial images from a black-box network, the overall approach and conclusions seem relatively straight forward. The paper is verbosely written and I feel like the findings could be summarized much more succinctly.",0
"The authors propose a method to generate adversarial examples w/o relying on knowledge of the network architecture or network gradients.

The idea has some merit, however, as mentioned by one of the reviewers, the field has been studied widely, including black box setups.

My main concern is that the first set of experiments allows images that are not in image space. The authors acknowledge this fact on page 7 in the first paragraph. In my opinion, this renders these experiments completely meaningless. At the very least, the outcome is not surprising to me at all.

The greedy search procedure remedies this issue. The description of the proposed method is somewhat convoluted. AFAICT, first a candidate set of pixels is generated by using PERT. Then the pixels are perturbed using CYCLIC.
It is not clear why this approach results in good/minimal perturbations as the candidate pixels are found using a large ""p"" that can result in images outside the image space. The choice of this method does not seem to be motivated by the authors.

In conclusion, while the authors to an interesting investigation and propose a method to generate adversarial images from a black-box network, the overall approach and conclusions seem relatively straight forward. The paper is verbosely written and I feel like the findings could be summarized much more succinctly.",0
"Authors propose the use of layer-wise language model-like pretraining for encoder-decoder models. This allows to leverage separate source and target corpora (in unsupervised manner) without necessity of large amounts of parallel training corpora. The idea is in principle fairly simple, and rely on initial optimising both encoder and decoder with LSTM tasked to perform language modelling. 

The ideas are not new, and the paper is more like a successful compilation of several approaches that have been around for some time. The experimental validation, though, offers some interesting insights into importance of initialization, and the effectiveness of different initialisations approaches in enc-dec setting.

The regulariser you propose to use on page 3, looks like typical multi-task objective function, especially it is used in an alternating manner would be interesting to see whether similar performance might have been obtained starting with this objective, from random initialisation.

You should probably give credit for encoder-decoder like-RNN models published in 1990s.

Minors:
Pg. 2, Sec 2.1 2nd paragraph: can be different sizes -> can be of different sizes",0
"Authors propose the use of layer-wise language model-like pretraining for encoder-decoder models. This allows to leverage separate source and target corpora (in unsupervised manner) without necessity of large amounts of parallel training corpora. The idea is in principle fairly simple, and rely on initial optimising both encoder and decoder with LSTM tasked to perform language modelling. 

The ideas are not new, and the paper is more like a successful compilation of several approaches that have been around for some time. The experimental validation, though, offers some interesting insights into importance of initialization, and the effectiveness of different initialisations approaches in enc-dec setting.

The regulariser you propose to use on page 3, looks like typical multi-task objective function, especially it is used in an alternating manner would be interesting to see whether similar performance might have been obtained starting with this objective, from random initialisation.

You should probably give credit for encoder-decoder like-RNN models published in 1990s.

Minors:
Pg. 2, Sec 2.1 2nd paragraph: can be different sizes -> can be of different sizes",0
"The paper is a novel application for the sticky HDP-HMM, focused on correctly identifying the number of components in bird and whale song across a variety of datasets. It's nice to see the model applied to an interesting dataset. My main issues with the paper have to do with structure and the choice of representation used in the model. Namely:

The organization of the paper could be significantly improved. There is a lot of repetitive introduction that adds little to the paper. The first and last two sentences of the abstract could be cut. Many other parts of the abstract basically repeat the introduction. The second paragraph of section 2.3 also repeats your introduction - by now we know what you're doing. I think most people reading this will have no idea what Kershenbaum (2014) is. The description of the data should go in the experiments section. ""Different hypotheses for the songs were emitted"" in the introduction is odd phrasing. Figure 4 should be the first figure and go in the introduction. Figure 5 should be in the methods section. A summary of Table 1 should be in the experiments section. Generally the writing could be tightened quite a bit, which would make space for these figures. The description of the HDP-HMM, which mostly follows the existing literature, is well done.

Some general questions about the methods used:

If you're interested in scalable inference, why use Gibbs sampling? Why not the beam sampler (van Gael 2008), which at least recently was the state of the art for MCMC inference in the HDP-HMM? More generally, why use MCMC at all? For very large datasets, most of the Bayesian ML community has converged on stochastic variational inference as the most practical method (eg Wang, Paisley and Blei 2011).

If your interest is mainly in the number of clusters, how would you address the fact that DP mixture models are known not to be consistent for estimating the true number of clusters (Miller and Harrison 2013)?

MFCC features are calibrated to the human auditory system, not bird or whale auditory systems. In your data, do you calibrate the MFCC scale to be closer to the auditory systems of the animals that generated the song?

And a final suggestion for future work, which could use the results presented here as a baseline:

Given the success of LSTMs in speech recognition in recent years, it may be the case that deep learned representations are superior to linear features (like the means of each cluster in an HDP-HMM) for animal song as well. Have you considered a hybrid model, similar to recent work combining autoencoders and graphical models (Johnson, Duvenaud, Wiltschko, Datta and Adams 2016)?",0
"The paper is a novel application for the sticky HDP-HMM, focused on correctly identifying the number of components in bird and whale song across a variety of datasets. It's nice to see the model applied to an interesting dataset. My main issues with the paper have to do with structure and the choice of representation used in the model. Namely:

The organization of the paper could be significantly improved. There is a lot of repetitive introduction that adds little to the paper. The first and last two sentences of the abstract could be cut. Many other parts of the abstract basically repeat the introduction. The second paragraph of section 2.3 also repeats your introduction - by now we know what you're doing. I think most people reading this will have no idea what Kershenbaum (2014) is. The description of the data should go in the experiments section. ""Different hypotheses for the songs were emitted"" in the introduction is odd phrasing. Figure 4 should be the first figure and go in the introduction. Figure 5 should be in the methods section. A summary of Table 1 should be in the experiments section. Generally the writing could be tightened quite a bit, which would make space for these figures. The description of the HDP-HMM, which mostly follows the existing literature, is well done.

Some general questions about the methods used:

If you're interested in scalable inference, why use Gibbs sampling? Why not the beam sampler (van Gael 2008), which at least recently was the state of the art for MCMC inference in the HDP-HMM? More generally, why use MCMC at all? For very large datasets, most of the Bayesian ML community has converged on stochastic variational inference as the most practical method (eg Wang, Paisley and Blei 2011).

If your interest is mainly in the number of clusters, how would you address the fact that DP mixture models are known not to be consistent for estimating the true number of clusters (Miller and Harrison 2013)?

MFCC features are calibrated to the human auditory system, not bird or whale auditory systems. In your data, do you calibrate the MFCC scale to be closer to the auditory systems of the animals that generated the song?

And a final suggestion for future work, which could use the results presented here as a baseline:

Given the success of LSTMs in speech recognition in recent years, it may be the case that deep learned representations are superior to linear features (like the means of each cluster in an HDP-HMM) for animal song as well. Have you considered a hybrid model, similar to recent work combining autoencoders and graphical models (Johnson, Duvenaud, Wiltschko, Datta and Adams 2016)?",0
"The authors proposed RASOR to address the problem of finding the best answer span according to a given question. The focus of the paper is mainly on how to model the relationship between question and the answer spans. The idea proposed by this paper is reasonable, but not ground breaking. The analysis is interesting and potentially useful. I would hope the authors can go extra miles to analyze different choices of boundary prediction models and make a more convincing case for the necessity of modeling the score of the span globally.

The main idea behind RASOR is to globally normalize and rank the scores of the possible answer spans. RASOR is able to achieve this by first modeling the hidden vectors of all words with LSTMs. Then, the representation of a text span is formed by concatenating the corresponding hidden vectors of the start and the end word of the corresponding chunk. The approach is reasonable, but not earth shattering. Also, the table 6 shows that the improvement over end-prediction point is not very large.

I appreciate the fact that authors conduct several analysis experiments as some of them are quite interesting. For example, it seems that question independent representation is also very import to the performance. In addition to the current analysis, I also want to get a clear idea on what makes the current model be better than the Match-LSTM. Is it hyper-parameter tuning? Or it is due to the use of the question independent representation?

Another good thing about the proposed model is that it is relatively simple, so there is a chance that the proposed techniques can be combined with other newly proposed ones.",0
"The authors proposed RASOR to address the problem of finding the best answer span according to a given question. The focus of the paper is mainly on how to model the relationship between question and the answer spans. The idea proposed by this paper is reasonable, but not ground breaking. The analysis is interesting and potentially useful. I would hope the authors can go extra miles to analyze different choices of boundary prediction models and make a more convincing case for the necessity of modeling the score of the span globally.

The main idea behind RASOR is to globally normalize and rank the scores of the possible answer spans. RASOR is able to achieve this by first modeling the hidden vectors of all words with LSTMs. Then, the representation of a text span is formed by concatenating the corresponding hidden vectors of the start and the end word of the corresponding chunk. The approach is reasonable, but not earth shattering. Also, the table 6 shows that the improvement over end-prediction point is not very large.

I appreciate the fact that authors conduct several analysis experiments as some of them are quite interesting. For example, it seems that question independent representation is also very import to the performance. In addition to the current analysis, I also want to get a clear idea on what makes the current model be better than the Match-LSTM. Is it hyper-parameter tuning? Or it is due to the use of the question independent representation?

Another good thing about the proposed model is that it is relatively simple, so there is a chance that the proposed techniques can be combined with other newly proposed ones.",0
"Thank you for an interesting read.

I found the application of VRNN type generative model to financial data very promising. But since I don't have enough background knowledge to judge whether the performance gap is significant or not, I wouldn't recommend acceptance at this stage. 

To me, the biggest issue for this paper is that I'm not sure if the paper contains significant novelty. The RNN-VAE combination has been around for more than a year and this paper does not propose significant changes to it. Maybe this paper fits better to an application targeting conference, rather than ICLR. But I'm not exactly sure about ICLR's acceptance criteria, and maybe the committee actually prefer great performances and interesting applications?",0
"Thank you for an interesting read.

I found the application of VRNN type generative model to financial data very promising. But since I don't have enough background knowledge to judge whether the performance gap is significant or not, I wouldn't recommend acceptance at this stage. 

To me, the biggest issue for this paper is that I'm not sure if the paper contains significant novelty. The RNN-VAE combination has been around for more than a year and this paper does not propose significant changes to it. Maybe this paper fits better to an application targeting conference, rather than ICLR. But I'm not exactly sure about ICLR's acceptance criteria, and maybe the committee actually prefer great performances and interesting applications?",0
"Authors present a parameterized variant of ELU and show that the proposed function helps to deal with vanishing gradients in deep networks in a way better than existing non-linearities. They present both a theoretical analysis and practical validation for presented approach. 

Interesting observations on statistics of the PELU parameters are reported. Perhaps explanation for the observed evolution of parameters can help better understand the non-linearity. It is hard to evaluate the experimental validation presented given the difference in number of parameters compared to other approaches.",0
"Authors present a parameterized variant of ELU and show that the proposed function helps to deal with vanishing gradients in deep networks in a way better than existing non-linearities. They present both a theoretical analysis and practical validation for presented approach. 

Interesting observations on statistics of the PELU parameters are reported. Perhaps explanation for the observed evolution of parameters can help better understand the non-linearity. It is hard to evaluate the experimental validation presented given the difference in number of parameters compared to other approaches.",0
"This paper proposed to use unsupervised learning to learn features in a reinforcement learning setting. It is unclear what ""unsupervised"" means here since the ""causality prior"" uses reward signals for training. This is reinforcement learning, not unsupervised learning.

The experiments are also very premature. The task is as simple as moving the head of the robot left or right. There is also no comparison to baselines.

In conclusions section, the authors claim the proposed method can be used for transfer learning without experiments to backup the claim.

Overall this paper is confusing and premature.",0
"This paper proposed to use unsupervised learning to learn features in a reinforcement learning setting. It is unclear what ""unsupervised"" means here since the ""causality prior"" uses reward signals for training. This is reinforcement learning, not unsupervised learning.

The experiments are also very premature. The task is as simple as moving the head of the robot left or right. There is also no comparison to baselines.

In conclusions section, the authors claim the proposed method can be used for transfer learning without experiments to backup the claim.

Overall this paper is confusing and premature.",0
"Summary: There are many different pruning techniques to reduce memory footprint of CNN models, and those techniques have different granularities (layer, maps, kernel or intra kernel), pruning ratio and sparsity of representation. The work proposes a method to choose the best pruning masks out to many trials. Tested on CIFAR-10, SVHN and MNIST.

Pros:
Proposes a method to choose pruning mask out of N trials. 
Analysis on different pruning methods.

Cons & Questions:
“The proposed strategy selects the best pruned network through N random pruning trials. This approach enables one to select pruning mask in one shot and is simpler than the multi-step technique.” How can one get the best pruning mask in one shot if you ran N random pruning trials? (answered)
Missing tests of the approach with bigger CNN: like AlexNet, VGG, GoogLeNet or ResNet. (extended to VGG ok)
Since reducing model size for embedded systems is the final goal, then showing how much memory space in MB is saved with the proposed technique compared with other approaches like Han et al. (2015) would be good.

Misc:
Typo in figure 6 a) caption: “Featuer” (corrected)",0
"Summary: There are many different pruning techniques to reduce memory footprint of CNN models, and those techniques have different granularities (layer, maps, kernel or intra kernel), pruning ratio and sparsity of representation. The work proposes a method to choose the best pruning masks out to many trials. Tested on CIFAR-10, SVHN and MNIST.

Pros:
Proposes a method to choose pruning mask out of N trials. 
Analysis on different pruning methods.

Cons & Questions:
“The proposed strategy selects the best pruned network through N random pruning trials. This approach enables one to select pruning mask in one shot and is simpler than the multi-step technique.” How can one get the best pruning mask in one shot if you ran N random pruning trials? (answered)
Missing tests of the approach with bigger CNN: like AlexNet, VGG, GoogLeNet or ResNet. (extended to VGG ok)
Since reducing model size for embedded systems is the final goal, then showing how much memory space in MB is saved with the proposed technique compared with other approaches like Han et al. (2015) would be good.

Misc:
Typo in figure 6 a) caption: “Featuer” (corrected)",0
"The paper introduces a lightweight network for semantic segmentation that combines several acceleration ideas.
As indicated in my preliminary question, the authors do not make the case about why any of the techniques they propose is beyond what we know already: factorizing filters into alternating 1-D convolutions, using low-rank kernels, or any of the newer inception network architectures.

I have had a hard time figuring out what is the take-home message of this paper. All of these ideas are known, and have proven their worth for detection. If a paper is going to be accepted for applying them to semantic segmentation, then in the next conference another paper should be accepted for applying them to normal estimation, another to saliency estimation and so on. 

As the authors mention in their preliminary review:
""I agree that most improvements from classification architectures are straightforward to apply to object segmentation, and that's exactly what we've done - our network is based on current state of the art models. Instead of repeating most of the discussion on factorizing filters, etc., that has been discussed in a lot of papers already, we have decided that it's much more valuable to describe in depth the choices that are related to segmentation only - these are the most important contributions of our paper.""

I do not see however any in-depth discussion of certain choices - e.g. an analysis of how certain choices influence performance or speed. Instead all one gets are some statements ""these gave a significant accuracy boost"" ""this helped a lot"", ""that did not help"", ""this turned out to work much better than that"" . This is not informative - and is more like an informal chat rather than an in-depth discussion. 

If novelty is not that important, and it is only performance or speed that matter, I am still not convinced.
The authors only compare to [1,2] (SegNet) in terms of both accuracy and speed. I cannot see the reason why they do so, and they do not really justify it. According to the authors' evaluation, [1] requires ~1 sec. per frame,  while Deeplab v2, without the DenseCRF, runs at 5-8fps. 
(",0
"The paper introduces a lightweight network for semantic segmentation that combines several acceleration ideas.
As indicated in my preliminary question, the authors do not make the case about why any of the techniques they propose is beyond what we know already: factorizing filters into alternating 1-D convolutions, using low-rank kernels, or any of the newer inception network architectures.

I have had a hard time figuring out what is the take-home message of this paper. All of these ideas are known, and have proven their worth for detection. If a paper is going to be accepted for applying them to semantic segmentation, then in the next conference another paper should be accepted for applying them to normal estimation, another to saliency estimation and so on. 

As the authors mention in their preliminary review:
""I agree that most improvements from classification architectures are straightforward to apply to object segmentation, and that's exactly what we've done - our network is based on current state of the art models. Instead of repeating most of the discussion on factorizing filters, etc., that has been discussed in a lot of papers already, we have decided that it's much more valuable to describe in depth the choices that are related to segmentation only - these are the most important contributions of our paper.""

I do not see however any in-depth discussion of certain choices - e.g. an analysis of how certain choices influence performance or speed. Instead all one gets are some statements ""these gave a significant accuracy boost"" ""this helped a lot"", ""that did not help"", ""this turned out to work much better than that"" . This is not informative - and is more like an informal chat rather than an in-depth discussion. 

If novelty is not that important, and it is only performance or speed that matter, I am still not convinced.
The authors only compare to [1,2] (SegNet) in terms of both accuracy and speed. I cannot see the reason why they do so, and they do not really justify it. According to the authors' evaluation, [1] requires ~1 sec. per frame,  while Deeplab v2, without the DenseCRF, runs at 5-8fps. 
(",0
"This paper makes three main methodological contributions:
 - definition of Neural Feature (NF) as the pixel average of the top N images that highly activation a neuron
 - ranking of neurons based on color selectivity
 - ranking of neurons based on class selectivity

The main weaknesses of the paper are that none of the methodological contributions are very significant, and no singularly significant result arises from the application of the methods.

However, the main strengths of the paper are its assortment of moderately-sized interesting conclusions about the basic behavior of neural nets. For example, a few are:
 - “Indexing on class selectivity neurons we found highly class selective neurons like digital-clock at conv2, cardoon at conv3 and ladybug at conv5, much before the fully connected layers.” As far as I know, this had not been previously reported.
 - Color selective neurons are found even in higher layers. (25% color selectivity in conv5)
 - “our main color axis emerge (black-white, blue-yellow, orange-cyan and cyan- magenta). Curiously, these two observations correlate with evidences in the human visual system (Shapley & Hawken (2011)).” Great observation!

Overall, I’d recommend the paper be accepted, because although it’s difficult to predict at this time, there’s a fair chance that one of the “smaller conclusions” would turn out to be important in hindsight a few years hence.


Other small comments:
 - The cite for “Learning to generate chairs…” is wrong (first two authors combined resulting in a confusing cite)

 - What exactly is the Color Selectivity Index computing? The Opponent Color Space isn’t well defined and it wasn’t previously familiar to me. Intuitively it seems to be selecting for units that respond to a constant color, but the highest color selectivity NF in Fig 5 i for a unit with two colors, not one. Finally, the very last unit (lowest color selectivity) is almost the same edge pattern, but with white -> black instead of blue -> orange. Why are these considered to be so drastically different? This should probably be more clearly described.

 - For the sake of argument, imagine a mushroom sensitive neuron in conv5 that fires highly for mushrooms of *any* color but not for anything else. If the dataset contains only red-capped mushrooms, would the color selectivity index for this neuron be high or low? If it is high, it’s somewhat misleading because the unit itself actually isn’t color selective; the dataset just happens only to have red mushrooms in it. (It’s a subtle point but worth considering and probably discussing in the paper)",0
"This paper makes three main methodological contributions:
 - definition of Neural Feature (NF) as the pixel average of the top N images that highly activation a neuron
 - ranking of neurons based on color selectivity
 - ranking of neurons based on class selectivity

The main weaknesses of the paper are that none of the methodological contributions are very significant, and no singularly significant result arises from the application of the methods.

However, the main strengths of the paper are its assortment of moderately-sized interesting conclusions about the basic behavior of neural nets. For example, a few are:
 - “Indexing on class selectivity neurons we found highly class selective neurons like digital-clock at conv2, cardoon at conv3 and ladybug at conv5, much before the fully connected layers.” As far as I know, this had not been previously reported.
 - Color selective neurons are found even in higher layers. (25% color selectivity in conv5)
 - “our main color axis emerge (black-white, blue-yellow, orange-cyan and cyan- magenta). Curiously, these two observations correlate with evidences in the human visual system (Shapley & Hawken (2011)).” Great observation!

Overall, I’d recommend the paper be accepted, because although it’s difficult to predict at this time, there’s a fair chance that one of the “smaller conclusions” would turn out to be important in hindsight a few years hence.


Other small comments:
 - The cite for “Learning to generate chairs…” is wrong (first two authors combined resulting in a confusing cite)

 - What exactly is the Color Selectivity Index computing? The Opponent Color Space isn’t well defined and it wasn’t previously familiar to me. Intuitively it seems to be selecting for units that respond to a constant color, but the highest color selectivity NF in Fig 5 i for a unit with two colors, not one. Finally, the very last unit (lowest color selectivity) is almost the same edge pattern, but with white -> black instead of blue -> orange. Why are these considered to be so drastically different? This should probably be more clearly described.

 - For the sake of argument, imagine a mushroom sensitive neuron in conv5 that fires highly for mushrooms of *any* color but not for anything else. If the dataset contains only red-capped mushrooms, would the color selectivity index for this neuron be high or low? If it is high, it’s somewhat misleading because the unit itself actually isn’t color selective; the dataset just happens only to have red mushrooms in it. (It’s a subtle point but worth considering and probably discussing in the paper)",0
"Unfortunately, the paper is not clear enough for me to understand what is being proposed. At a high-level the authors seem to propose a generalization of the standard layered neural architecture (of which MLPs are a special case), based on arbitrary nodes which communicate via messages. The paper then goes on to show that their layer-free architecture can perform the same computation as a standard MLP. This logic appears circular. The low level details of the method are also confusing: while the authors seem to be wanting to move away from layers based on matrix-vector products, Algorithm 4 nevertheless resorts to matrix-vector products for the forward and backwards pass. Although the implementation relies on asynchronously communicating nodes, the “locking” nature of the computation makes the two entirely equivalent.",0
"Unfortunately, the paper is not clear enough for me to understand what is being proposed. At a high-level the authors seem to propose a generalization of the standard layered neural architecture (of which MLPs are a special case), based on arbitrary nodes which communicate via messages. The paper then goes on to show that their layer-free architecture can perform the same computation as a standard MLP. This logic appears circular. The low level details of the method are also confusing: while the authors seem to be wanting to move away from layers based on matrix-vector products, Algorithm 4 nevertheless resorts to matrix-vector products for the forward and backwards pass. Although the implementation relies on asynchronously communicating nodes, the “locking” nature of the computation makes the two entirely equivalent.",0
The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference.,0
The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference.,0
"In this paper, the author proposed an approach for feature combination of two embeddings v1 and v2. This is done by first computing the pairwise combinations of the elements of v1 and v2 (with complicated nonlinearity), and then pick the K-Max as the output vector. For triple (or higher-order) combinations, two (or more) consecutive pairwise combinations are performed to yield the final representations. It seems that the approach is not directly related to categorical data, and can be applied to any embeddings (even if they are not one-hot). So is there any motivation that brings about this particular approach? What is the connection? 

There are many papers with similar ideas. CCPM (A convolutional click prediction model) that the authors have compared against, also proposes very similar network structure (conv + K-max + conv + K-max). In the paper, the author does not mention their conceptual similarity and difference versus CCPM. Compact Bilinear Pooling,",0
"In this paper, the author proposed an approach for feature combination of two embeddings v1 and v2. This is done by first computing the pairwise combinations of the elements of v1 and v2 (with complicated nonlinearity), and then pick the K-Max as the output vector. For triple (or higher-order) combinations, two (or more) consecutive pairwise combinations are performed to yield the final representations. It seems that the approach is not directly related to categorical data, and can be applied to any embeddings (even if they are not one-hot). So is there any motivation that brings about this particular approach? What is the connection? 

There are many papers with similar ideas. CCPM (A convolutional click prediction model) that the authors have compared against, also proposes very similar network structure (conv + K-max + conv + K-max). In the paper, the author does not mention their conceptual similarity and difference versus CCPM. Compact Bilinear Pooling,",0
"A new sparse coding model is introduced that learns features jointly with their transformations. It is found that inference over per-image transformation variables is hard, so the authors suggest tying these variables across all data points, turning them into global parameters, and using multiple transformations for each feature. Furthermore, it is suggested to use a tree of transformations, where each path down the tree generates a feature by multiplying the root feature by the transformations associated with the edges. The one-layer tree model achieves similar reconstruction error as traditional sparse coding, while using fewer parameters.

This is a nice addition to the literature on sparse coding and the literature on learning transformation models. The authors identify and deal with a difficult inference problem that can occur in transformation models. That said, I am skeptical about the usefulness of the general approach. The authors take it as a given that “learning sparse features and transformations jointly” is an important goal in itself, but this is never really argued or demonstrated with experiments. It doesn’t seem like this method enables new applications, extends our understanding of learning what/where pathways in the brain, or improve our ability to model natural images.

The authors claim that the model extracts pose information, but although the model explicitly captures the transformation that relates different features in a tree, at test time, inference is only performed over the (sparse) coefficient associated with each (feature, transformation) combination, just like in sparse coding. It is not clear what we gain by knowing that each coefficient is associated with a transformation, especially since there are many models that do this general “what / where” split.

It would be good to check that the x_{v->b} actually change significantly from their initialization values. The loss surface still looks pretty bad even for tied transformations, so they may actually not move much. Does the proposed model work better according to some measure, compared to a model where x_{v->b} are fixed and chosen from some reasonable range of parameter values (either randomly or spaced evenly)?

One of the conceptually interesting aspects of the paper is the idea of a tree of transformations, but the advantage of deeper trees is never demonstrated convincingly. It looks like the authors have only just gotten this approach to work on toy data with vertical and horizontal bars.

Finally, it is not clear how the method could be extended to have multiple layers. The transformation operators T can be defined in the first layer because they act on the input space, but the same cannot be done in the learned feature space. It is also not clear how the pose information should be further processed in a hierarchical manner, or how learning in a deep version should work.

In summary, I do not recommend this paper for publication, because it is not clear what problem is being solved, the method is only moderately novel and the novel aspects are not convincingly shown to be beneficial.",0
"A new sparse coding model is introduced that learns features jointly with their transformations. It is found that inference over per-image transformation variables is hard, so the authors suggest tying these variables across all data points, turning them into global parameters, and using multiple transformations for each feature. Furthermore, it is suggested to use a tree of transformations, where each path down the tree generates a feature by multiplying the root feature by the transformations associated with the edges. The one-layer tree model achieves similar reconstruction error as traditional sparse coding, while using fewer parameters.

This is a nice addition to the literature on sparse coding and the literature on learning transformation models. The authors identify and deal with a difficult inference problem that can occur in transformation models. That said, I am skeptical about the usefulness of the general approach. The authors take it as a given that “learning sparse features and transformations jointly” is an important goal in itself, but this is never really argued or demonstrated with experiments. It doesn’t seem like this method enables new applications, extends our understanding of learning what/where pathways in the brain, or improve our ability to model natural images.

The authors claim that the model extracts pose information, but although the model explicitly captures the transformation that relates different features in a tree, at test time, inference is only performed over the (sparse) coefficient associated with each (feature, transformation) combination, just like in sparse coding. It is not clear what we gain by knowing that each coefficient is associated with a transformation, especially since there are many models that do this general “what / where” split.

It would be good to check that the x_{v->b} actually change significantly from their initialization values. The loss surface still looks pretty bad even for tied transformations, so they may actually not move much. Does the proposed model work better according to some measure, compared to a model where x_{v->b} are fixed and chosen from some reasonable range of parameter values (either randomly or spaced evenly)?

One of the conceptually interesting aspects of the paper is the idea of a tree of transformations, but the advantage of deeper trees is never demonstrated convincingly. It looks like the authors have only just gotten this approach to work on toy data with vertical and horizontal bars.

Finally, it is not clear how the method could be extended to have multiple layers. The transformation operators T can be defined in the first layer because they act on the input space, but the same cannot be done in the learned feature space. It is also not clear how the pose information should be further processed in a hierarchical manner, or how learning in a deep version should work.

In summary, I do not recommend this paper for publication, because it is not clear what problem is being solved, the method is only moderately novel and the novel aspects are not convincingly shown to be beneficial.",0
"This paper explores the use of Open Bigrams as a target representation of words, for application to handwriting image recognition. 

Pros:
- The use of OBs is novel and interesting.
- Clearly written and explained.

Cons:
- No comparison to previous state of the art, only with author-generated results. 
- More ablation studies needed -- i.e. fill in Table3 with rnn0,1 rnn0,1,2 rnn0,1' etc etc. It is not clear where the performance is coming from, as it seems that it is single character modelling (0) and word endings (') that are actually beneficial.
- While the use of Open bigrams is novel, there are works which use bag of bigrams and ngrams as models which are not really compared to or explored. E.g.",0
"This paper explores the use of Open Bigrams as a target representation of words, for application to handwriting image recognition. 

Pros:
- The use of OBs is novel and interesting.
- Clearly written and explained.

Cons:
- No comparison to previous state of the art, only with author-generated results. 
- More ablation studies needed -- i.e. fill in Table3 with rnn0,1 rnn0,1,2 rnn0,1' etc etc. It is not clear where the performance is coming from, as it seems that it is single character modelling (0) and word endings (') that are actually beneficial.
- While the use of Open bigrams is novel, there are works which use bag of bigrams and ngrams as models which are not really compared to or explored. E.g.",0
"Strengths

- interesting to explore the connection between ReLU DNN and simplified SFNN
- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally
- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)


Weaknesses

-no results are reported on real tasks with large training set

-not clear exploration on the scalability of the learning methods when training data becomes larger

-when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in “Pattern Recognition and Computer Vision”, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as “explaining away”.

-would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers",0
"Strengths

- interesting to explore the connection between ReLU DNN and simplified SFNN
- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally
- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)


Weaknesses

-no results are reported on real tasks with large training set

-not clear exploration on the scalability of the learning methods when training data becomes larger

-when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in “Pattern Recognition and Computer Vision”, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as “explaining away”.

-would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers",0
"The authors propose ""information dropout"", a variation of dropout with an information theoretic interpretation. A dropout layer limits the amount of information that can be passed through it, and the authors quantify this using a variational bound. 

It remains unclear why such an information bottleneck is a good idea from a theoretical standpoint. Bayesian interpretations lend a theoretical basis to parameter noise, but activation noise has no such motivation. The information bottleneck indeed limits the information that can be passed through, but there is no rigorous argument for why this should improve generalization.

The experiments are not convincing. The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.",0
"The authors propose ""information dropout"", a variation of dropout with an information theoretic interpretation. A dropout layer limits the amount of information that can be passed through it, and the authors quantify this using a variational bound. 

It remains unclear why such an information bottleneck is a good idea from a theoretical standpoint. Bayesian interpretations lend a theoretical basis to parameter noise, but activation noise has no such motivation. The information bottleneck indeed limits the information that can be passed through, but there is no rigorous argument for why this should improve generalization.

The experiments are not convincing. The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.",0
"I agree with the other reviewer that the application areas are limited in the paper. I agree with the overall sentiment of the paper to evaluate effectiveness of some of the more recent techniques in this area, in conjunction with the recurrent networks. 

The paper advertises itself as a method (or a list of methods) of improving the recurrent baselines when performing experiments, however fails (or not shown) to generalize to other tasks. Effectiveness of these methods need to be shown across a wide variety of tasks if we intend to replace traditional baselines in general, rather than a specific subset of applications.

I like the desire to evaluate many of the recent techniques and having many replications of experiments towards this end (which is a strong point of the paper). However, whether there are synergies of some of the enhancements with sentiment analysis or not, we cannot see from these results. It would be interesting to see whether some of these results generalize across a wide variety of tasks.",0
"I agree with the other reviewer that the application areas are limited in the paper. I agree with the overall sentiment of the paper to evaluate effectiveness of some of the more recent techniques in this area, in conjunction with the recurrent networks. 

The paper advertises itself as a method (or a list of methods) of improving the recurrent baselines when performing experiments, however fails (or not shown) to generalize to other tasks. Effectiveness of these methods need to be shown across a wide variety of tasks if we intend to replace traditional baselines in general, rather than a specific subset of applications.

I like the desire to evaluate many of the recent techniques and having many replications of experiments towards this end (which is a strong point of the paper). However, whether there are synergies of some of the enhancements with sentiment analysis or not, we cannot see from these results. It would be interesting to see whether some of these results generalize across a wide variety of tasks.",0
"This work proposes a model that can learn short binary codes via paragraph vectors to allow fast retrieval of documents. The experiments show that this is superior to semantic hashing. The approach is simple and not very technically interesting. For a code size of 128, the loss compared to a continuous paragraph vector seems moderate.

The paper asks the reader to refer to the Salakhutdinov and Hinton paper for the baseline numbers but I think they should be placed in the paper for easy reference. For simplicity, the paper could show the precision at 12.5%, 25% and 50% recall for the proposed model and semantic hashing. It also seems that the semantic hashing paper shows results on RCV2 and not RCV1. RCV1 is twice the size of RCV2 and is English only so it seems that these results are not comparable. It would be interesting to see how many binary bits are required to match the performance of the continuous representation. A comparison to the continuous PV-DBOW trained with bigrams would also make it a more fair comparison.

Figure 7 in the paper shows a loss from using the real-binary PV-DBOW. It seems that if a user needed high quality ranking after the retrieval stage and they could afford the extra space and computation, then it would be better for them to use a standard PV-DBOW to obtain the continuous representation at that stage.

Minor comments:
First line after the introduction: is sheer -> is the sheer
4th line from the bottom of P1: words embeddings -> word embeddings
In table 1: What does code size refer to for PV-DBOW? Is this the number of elements in the continuous vector?
5th line from the bottom of P5: W -> We
5th line after section 3.1: covers wide -> covers a wide",0
"This work proposes a model that can learn short binary codes via paragraph vectors to allow fast retrieval of documents. The experiments show that this is superior to semantic hashing. The approach is simple and not very technically interesting. For a code size of 128, the loss compared to a continuous paragraph vector seems moderate.

The paper asks the reader to refer to the Salakhutdinov and Hinton paper for the baseline numbers but I think they should be placed in the paper for easy reference. For simplicity, the paper could show the precision at 12.5%, 25% and 50% recall for the proposed model and semantic hashing. It also seems that the semantic hashing paper shows results on RCV2 and not RCV1. RCV1 is twice the size of RCV2 and is English only so it seems that these results are not comparable. It would be interesting to see how many binary bits are required to match the performance of the continuous representation. A comparison to the continuous PV-DBOW trained with bigrams would also make it a more fair comparison.

Figure 7 in the paper shows a loss from using the real-binary PV-DBOW. It seems that if a user needed high quality ranking after the retrieval stage and they could afford the extra space and computation, then it would be better for them to use a standard PV-DBOW to obtain the continuous representation at that stage.

Minor comments:
First line after the introduction: is sheer -> is the sheer
4th line from the bottom of P1: words embeddings -> word embeddings
In table 1: What does code size refer to for PV-DBOW? Is this the number of elements in the continuous vector?
5th line from the bottom of P5: W -> We
5th line after section 3.1: covers wide -> covers a wide",0
"This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.

The paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. 

The authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.

The supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.

In the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.

Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015.",0
"This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.

The paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. 

The authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.

The supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.

In the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.

Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015.",0
"In this paper a novel approach for anomaly detection is considered for the task of intrusion detection based on system call sequence.
The system call sequence is regarded as a language, and multiple lstm-rnn language models are trained and ensembled. 
Diversity in the ensemble is achieved by choosing different hyper parameters for each lstm-LM.
The combination of the LMs is done by averaging transformations of the likelihoods. 

I really like the fact that no attack data is used during training, and I like the LM and ensemble approach. 
The only high level drawbacks I have are the following, which might have a simple answer as I'm not an expert in this field:

- Relaying of system calls seems weak: If the attacker has access to some ""normal"" sequences of system calls, all she can fool the system by interleaving its malicious system calls with normal ones, in a way that will artificially raise the likelihood of the sequence. 
- A few lines covering other anomaly detection tasks, where RNNs are used, can be added to the introduction, to give a better idea about the novelty of the approach.",0
"In this paper a novel approach for anomaly detection is considered for the task of intrusion detection based on system call sequence.
The system call sequence is regarded as a language, and multiple lstm-rnn language models are trained and ensembled. 
Diversity in the ensemble is achieved by choosing different hyper parameters for each lstm-LM.
The combination of the LMs is done by averaging transformations of the likelihoods. 

I really like the fact that no attack data is used during training, and I like the LM and ensemble approach. 
The only high level drawbacks I have are the following, which might have a simple answer as I'm not an expert in this field:

- Relaying of system calls seems weak: If the attacker has access to some ""normal"" sequences of system calls, all she can fool the system by interleaving its malicious system calls with normal ones, in a way that will artificially raise the likelihood of the sequence. 
- A few lines covering other anomaly detection tasks, where RNNs are used, can be added to the introduction, to give a better idea about the novelty of the approach.",0
"7

Summary:
This paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method’s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.

Review:
Variational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.

As the authors point out, “VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA”. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.

The derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?

In Section 3 the authors claim that “if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data”. This is not correct, a model which hasn’t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.

Minor:
In the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.",0
"7

Summary:
This paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method’s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.

Review:
Variational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful.

As the authors point out, “VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA”. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.

The derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix?

In Section 3 the authors claim that “if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data”. This is not correct, a model which hasn’t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence.

Minor:
In the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.",0
"This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. 
The paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. 

The authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. 
The experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.

The authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.


The paper contains errors:

- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!

- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. 

- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. 

- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. 


It is not clear to me why the author say for LVMs such as GPLVM that ""the latent space is learned a priority with clean training data"". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  


It is not clear what the authors mean in the paper by ""pre-training"" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training.",0
"This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. 
The paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. 

The authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. 
The experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.

The authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.


The paper contains errors:

- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!

- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. 

- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. 

- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. 


It is not clear to me why the author say for LVMs such as GPLVM that ""the latent space is learned a priority with clean training data"". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  


It is not clear what the authors mean in the paper by ""pre-training"" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training.",0
"Overall I think this is an interesting paper which shows empirical performance improvement over baselines. However, my main concern with the paper is regarding its technical depth, as the gist of the paper can be summarized as the following: instead of keeping the batch norm mean and bias estimation over the whole model, estimate them on a per-domain basis. I am not sure if this is novel, as this is a natural extension of the original batch normalization paper. Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper.

Detailed comments:

Section 3.1: I respectfully disagree that the core idea of BN is to align the distribution of training data. It does this as a side effect, but the major purpose of BN is to properly control the scale of the gradient so we can train very deep models without the problem of vanishing gradients. It is plausible that intermediate features from different datasets naturally show as different groups in a t-SNE embedding. This is not the particular feature of batch normalization: visualizing a set of intermediate features with AlexNet and one gets the same results. So the premise in section 3.1 is not accurate.

Section 3.3: I have the same concern as the other reviewer. It seems to be quite detatched from the general idea of AdaBN. Equation 2 presents an obvious argument that the combined BN-fully_connected layer forms a linear transform, which is true in the original BN case and in this case as well. I do not think it adds much theoretical depth to the paper. (In general the novelty of this paper seems low)

Experiments:

- section 4.3.1 is not an accurate measure of the ""effectiveness"" of the proposed method, but a verification of a simple fact: previously, we normalize the source domain features into a Gaussian distribution. the proposed method is explicitly normalizing the target domain features into the same Gaussian distribution as well. So, it is obvious that the KL divergence between these two distributions are closer - in fact, one is *explicitly* making them close. However, this does not directly correlate to the effectiveness of the final classification performance.

- section 4.3.2: the sensitivity analysis is a very interesting read, as it suggests that only a very few number of images are needed to account for the domain shift in the AdaBN parameter estimation. This seems to suggest that a single ""whitening"" operation is already good enough to offset the domain bias (in both cases shown, a single batch is sufficient to recover about 80% of the performance gain, although I cannot get data for even smaller number of examples from the figure). It would thus be useful to have a comparison between these approaches, and also a detailed analysis of the effect from each layer of the model - the current analysis seems a bit thin.",0
"Overall I think this is an interesting paper which shows empirical performance improvement over baselines. However, my main concern with the paper is regarding its technical depth, as the gist of the paper can be summarized as the following: instead of keeping the batch norm mean and bias estimation over the whole model, estimate them on a per-domain basis. I am not sure if this is novel, as this is a natural extension of the original batch normalization paper. Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper.

Detailed comments:

Section 3.1: I respectfully disagree that the core idea of BN is to align the distribution of training data. It does this as a side effect, but the major purpose of BN is to properly control the scale of the gradient so we can train very deep models without the problem of vanishing gradients. It is plausible that intermediate features from different datasets naturally show as different groups in a t-SNE embedding. This is not the particular feature of batch normalization: visualizing a set of intermediate features with AlexNet and one gets the same results. So the premise in section 3.1 is not accurate.

Section 3.3: I have the same concern as the other reviewer. It seems to be quite detatched from the general idea of AdaBN. Equation 2 presents an obvious argument that the combined BN-fully_connected layer forms a linear transform, which is true in the original BN case and in this case as well. I do not think it adds much theoretical depth to the paper. (In general the novelty of this paper seems low)

Experiments:

- section 4.3.1 is not an accurate measure of the ""effectiveness"" of the proposed method, but a verification of a simple fact: previously, we normalize the source domain features into a Gaussian distribution. the proposed method is explicitly normalizing the target domain features into the same Gaussian distribution as well. So, it is obvious that the KL divergence between these two distributions are closer - in fact, one is *explicitly* making them close. However, this does not directly correlate to the effectiveness of the final classification performance.

- section 4.3.2: the sensitivity analysis is a very interesting read, as it suggests that only a very few number of images are needed to account for the domain shift in the AdaBN parameter estimation. This seems to suggest that a single ""whitening"" operation is already good enough to offset the domain bias (in both cases shown, a single batch is sufficient to recover about 80% of the performance gain, although I cannot get data for even smaller number of examples from the figure). It would thus be useful to have a comparison between these approaches, and also a detailed analysis of the effect from each layer of the model - the current analysis seems a bit thin.",0
"Strengths
-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. 
-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules.
-- x50 less memory usage than AlexNet, keeping similar accuracy 
-- strong experimental results

Weaknesses
--Would be nice to test Sqeezenet on multiple tasks

--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the “by-pass” architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more rigorously on theoretical analysis?",0
"Strengths
-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. 
-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules.
-- x50 less memory usage than AlexNet, keeping similar accuracy 
-- strong experimental results

Weaknesses
--Would be nice to test Sqeezenet on multiple tasks

--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the “by-pass” architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more rigorously on theoretical analysis?",0
"CONTRIBUTIONS
When training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.

NOVELTY
Thresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.

MISSING CITATIONS
Prior work has explored low-precision arithmetic for recurrent neural network language models:

Hubara et al, “Quantized Neural Networks: Training Neural Networks with
Low Precision Weights and Activations”,",0
"CONTRIBUTIONS
When training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.

NOVELTY
Thresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.

MISSING CITATIONS
Prior work has explored low-precision arithmetic for recurrent neural network language models:

Hubara et al, “Quantized Neural Networks: Training Neural Networks with
Low Precision Weights and Activations”,",0
"The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.

However, it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement, especially literature review and experiment analysis.",0
"The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.

However, it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement, especially literature review and experiment analysis.",0
"This paper proposes a novel approach ParMAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models which is based on the composition of multiple processing layers (i.e., deep nets). The basic idea of MAC to optimise the nested objective function, which is traditionally learned using methods based on the chain-rule gradients but inconvenient and is hard to parallelise, is to break nested functional relationships judiciously by introducing new variables ( the auxiliary coordinates) as equality constraints, and then to optimise a penalised function using alternating optimisation over the original parameters (W step) and over the coordinates (Z step).  The minimisation (W step) updates the parameters by splitting the nested model into independent submodels and training them using existing algorithms, and the coordination (Z step) ensures that corresponding inputs and outputs of submodels eventually match.  In this paper, the basic assumptions of ParMAC are that with large datasets in distributed systems, it is imperative to minimise data movement over the network because of the communication time generally far exceeds the computation time in modern architectures. Thus, the authors propose the ParMAC to translate the parallelism inherent in MAC into a distributed system by data parallelism and model parallelism. They also analyse its parallel speedup and convergence, and demonstrated it with MPI-based implementation to optimise binary autoencoders. The proposed ParMAC is tested on 3 colour image retrieval datasets. 

The organization of the paper is well written, and the presentation is clear. My questions are included in the following:
- The MAC framework solves the original problem approximately. If people use the sigmoid function to smooth the stepwise function, the naive optimization methods can be easier applied. What is the difference between these two? Or why do we want to use a new approach to solve it?
- The authors do not compare their ParMAC model with other distributed approaches for the same nested function optimization problem.",0
"This paper proposes a novel approach ParMAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models which is based on the composition of multiple processing layers (i.e., deep nets). The basic idea of MAC to optimise the nested objective function, which is traditionally learned using methods based on the chain-rule gradients but inconvenient and is hard to parallelise, is to break nested functional relationships judiciously by introducing new variables ( the auxiliary coordinates) as equality constraints, and then to optimise a penalised function using alternating optimisation over the original parameters (W step) and over the coordinates (Z step).  The minimisation (W step) updates the parameters by splitting the nested model into independent submodels and training them using existing algorithms, and the coordination (Z step) ensures that corresponding inputs and outputs of submodels eventually match.  In this paper, the basic assumptions of ParMAC are that with large datasets in distributed systems, it is imperative to minimise data movement over the network because of the communication time generally far exceeds the computation time in modern architectures. Thus, the authors propose the ParMAC to translate the parallelism inherent in MAC into a distributed system by data parallelism and model parallelism. They also analyse its parallel speedup and convergence, and demonstrated it with MPI-based implementation to optimise binary autoencoders. The proposed ParMAC is tested on 3 colour image retrieval datasets. 

The organization of the paper is well written, and the presentation is clear. My questions are included in the following:
- The MAC framework solves the original problem approximately. If people use the sigmoid function to smooth the stepwise function, the naive optimization methods can be easier applied. What is the difference between these two? Or why do we want to use a new approach to solve it?
- The authors do not compare their ParMAC model with other distributed approaches for the same nested function optimization problem.",0
"Summary
===
This paper presents tic-tac-toe as toy problem for investigating CNNs.
A dataset is created containing tic-tac-toe boards where one player is one
move away from winning and a CNN is trained to label boards according
to (1) the player who can win (2 choices) and (2) the position they may move
to win (9 choices), resulting in 18 labels. The CNN evaluated in this paper
performs perfectly at the task and the paper's goal is to inspect how the
CNN works.

The fundamental mechanism for this inspection is Class Activation
Mapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention
in the CNN. These implicit attention maps (localization heat maps) are used to
derive actions (which square each player should move). The attention maps  

(1) attend to squares in the tic-tac-toe board rather than arbitrary
blobs, despite the fact that one square in a board has uniform color, and

(2) they can be used to pick correct (winning) actions.

This experiment are used to support assertions that the network understands
(1) chess (tic-tac-toe) boards
(2) a rule for winning tic-tac-toe
(3) that there are two players.

Some follow up experiments indicate similar results under various renderings
of the tic-tac-toe boards and an incomplete training regime.


More Clarifying Questions
===

* I am not quite sure precisely how CAM is implemented here. In the original CAM
one must identify a class of interest to visualize (e.g., cat or dog). I don't
think this paper identifies such a choice. How is one of the 18 possible classes
chosen for creating the CAM visualization and through that visualization
choosing an action?

* How was the test set for this dataset for the table 1 results created?
How many of the final 1029 states were used for test and was the
distribution of labels the same in train and test?

* How is RCO computed? Is rank correlation or Pearson correlation used?
If Pearson correlation is used then it may be good to consider rank correlation,
as argued in ""Human Attention in Visual Question Answering: Do Humans and
Deep Networks Look at the Same Regions?"" by Das et. al. in EMNLP 2016.
In table 1, what does the 10^3 next to RCO mean?


Pros
===

* The proposed method, deriving an action to take from the result of a
visualization technique, is very novel.

* This paper provides an experiment that clearly shows a CNN relying on context
to make accurate predictions.

* The use of a toy tic-tac-toe domain to study attention in CNNs
(implicit or otherwise) is a potentially fruitful setting that may
lead to better understanding of implicit and maybe explicit attention mechanisms.


Cons
===

* This work distinguishes between predictions about ""what will happen""
(will the white player win?) and ""what to do"" (where should the white
player move to win?). The central idea is generalization from ""what will happen""
to ""what to do"" indicates concept learning (sec. 2.1). Why should an ability to
act be any more indicative of a learned concept than an ability to predict
future states. I see a further issue with the presentation of this approach and
a potential correctness problem:

1. (correctness)
In the specific setting proposed I see no difference between ""what to do""
and ""what will happen.""

Suppose one created labels dictating ""what to do"" for each example in the
proposed dataset. How would these differ from the labels of ""what will happen""
in the proposed dataset? In this case ""what will happen"" labels include
both player identity (who wins) and board position (which position they move
to win). Wouldn't the ""what to do"" labels need to indicate board position?
They could also chosen to indicate player identity, which would make them
identical to the ""what will happen"" labels (both 18-way softmaxes).

2. (presentation)
I think this distinction would usually be handled by the Reinforcement Learning
framework, but the proposed method is not presented in that framework or
related to an RL based approach. In RL ""what will happen"" is the reward an
agent will receive for making a particular action and ""what to do"" is the
action an agent should take. From this point of view, generalization from
""what will happen"" to ""what to do"" is not a novel thing to study.

Alternate models include:
    * A deep Q network (Mnih. et. al. 2015) could predict the value of
      every possible action where an action is a (player, board position) tuple.
    * The argmax of the current model's softmax could be used as an action
      prediction.
The deep Q network approach need not be implemented, but differences between
methods should be explained because of the uniqueness of the proposed approach.


* Comparison to work that uses visualization to investigate deep RL networks
is missing. In particular, other work in RL has used Simonyan et. al.
(arXiv 2013) style saliency maps to investigate network behavior. For example, 
""Dueling Network Architectures for Deep Reinforcement Learning"" by Wang et. al.
in (ICML 2016) uses saliency maps to identify differences between their
state-value and advantage networks. In ""Graying the black box:
Understanding DQNs"" by Zahavy et. al. (ICML 2016) these saliency maps are
also used to analyze network behavior.


* In section 2.3, saliency maps of Simonyan et. al. are said to not be able to
activate on grid squares because they have constant intensity, yet no empirical
or theoretical evidence is provided for this claim.

On a related note, what precisely is the notion of information referenced in
section 2.3 and why is it relevant? Is it entropy of the distribution of pixel
intensities in a patch? To me it seems that any measure which depends only
on one patch is irrelevant because the methods discussed (e.g., saliency maps)
depend on context as well as the intensities within a patch.


* The presentation in the paper would be improved if the results in section 7
were presented along with relevant discussion in preceding sections.


Overall Evaluation
===
The experiments presented here are novel, but I am not sure they are very
significant or offer clear conclusions. The methods and goals are not presented
clearly and lack the broader relevant context mentioned above. Furthermore, I
find the lines of thought mentioned in the Cons section possibly incorrect
or incomplete. As detailed with further clarifying questions, upon closer
inspection I do not see how some aspects of the proposed approach were
implemented, so my opinion may change with further details.",0
"Summary
===
This paper presents tic-tac-toe as toy problem for investigating CNNs.
A dataset is created containing tic-tac-toe boards where one player is one
move away from winning and a CNN is trained to label boards according
to (1) the player who can win (2 choices) and (2) the position they may move
to win (9 choices), resulting in 18 labels. The CNN evaluated in this paper
performs perfectly at the task and the paper's goal is to inspect how the
CNN works.

The fundamental mechanism for this inspection is Class Activation
Mapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention
in the CNN. These implicit attention maps (localization heat maps) are used to
derive actions (which square each player should move). The attention maps  

(1) attend to squares in the tic-tac-toe board rather than arbitrary
blobs, despite the fact that one square in a board has uniform color, and

(2) they can be used to pick correct (winning) actions.

This experiment are used to support assertions that the network understands
(1) chess (tic-tac-toe) boards
(2) a rule for winning tic-tac-toe
(3) that there are two players.

Some follow up experiments indicate similar results under various renderings
of the tic-tac-toe boards and an incomplete training regime.


More Clarifying Questions
===

* I am not quite sure precisely how CAM is implemented here. In the original CAM
one must identify a class of interest to visualize (e.g., cat or dog). I don't
think this paper identifies such a choice. How is one of the 18 possible classes
chosen for creating the CAM visualization and through that visualization
choosing an action?

* How was the test set for this dataset for the table 1 results created?
How many of the final 1029 states were used for test and was the
distribution of labels the same in train and test?

* How is RCO computed? Is rank correlation or Pearson correlation used?
If Pearson correlation is used then it may be good to consider rank correlation,
as argued in ""Human Attention in Visual Question Answering: Do Humans and
Deep Networks Look at the Same Regions?"" by Das et. al. in EMNLP 2016.
In table 1, what does the 10^3 next to RCO mean?


Pros
===

* The proposed method, deriving an action to take from the result of a
visualization technique, is very novel.

* This paper provides an experiment that clearly shows a CNN relying on context
to make accurate predictions.

* The use of a toy tic-tac-toe domain to study attention in CNNs
(implicit or otherwise) is a potentially fruitful setting that may
lead to better understanding of implicit and maybe explicit attention mechanisms.


Cons
===

* This work distinguishes between predictions about ""what will happen""
(will the white player win?) and ""what to do"" (where should the white
player move to win?). The central idea is generalization from ""what will happen""
to ""what to do"" indicates concept learning (sec. 2.1). Why should an ability to
act be any more indicative of a learned concept than an ability to predict
future states. I see a further issue with the presentation of this approach and
a potential correctness problem:

1. (correctness)
In the specific setting proposed I see no difference between ""what to do""
and ""what will happen.""

Suppose one created labels dictating ""what to do"" for each example in the
proposed dataset. How would these differ from the labels of ""what will happen""
in the proposed dataset? In this case ""what will happen"" labels include
both player identity (who wins) and board position (which position they move
to win). Wouldn't the ""what to do"" labels need to indicate board position?
They could also chosen to indicate player identity, which would make them
identical to the ""what will happen"" labels (both 18-way softmaxes).

2. (presentation)
I think this distinction would usually be handled by the Reinforcement Learning
framework, but the proposed method is not presented in that framework or
related to an RL based approach. In RL ""what will happen"" is the reward an
agent will receive for making a particular action and ""what to do"" is the
action an agent should take. From this point of view, generalization from
""what will happen"" to ""what to do"" is not a novel thing to study.

Alternate models include:
    * A deep Q network (Mnih. et. al. 2015) could predict the value of
      every possible action where an action is a (player, board position) tuple.
    * The argmax of the current model's softmax could be used as an action
      prediction.
The deep Q network approach need not be implemented, but differences between
methods should be explained because of the uniqueness of the proposed approach.


* Comparison to work that uses visualization to investigate deep RL networks
is missing. In particular, other work in RL has used Simonyan et. al.
(arXiv 2013) style saliency maps to investigate network behavior. For example, 
""Dueling Network Architectures for Deep Reinforcement Learning"" by Wang et. al.
in (ICML 2016) uses saliency maps to identify differences between their
state-value and advantage networks. In ""Graying the black box:
Understanding DQNs"" by Zahavy et. al. (ICML 2016) these saliency maps are
also used to analyze network behavior.


* In section 2.3, saliency maps of Simonyan et. al. are said to not be able to
activate on grid squares because they have constant intensity, yet no empirical
or theoretical evidence is provided for this claim.

On a related note, what precisely is the notion of information referenced in
section 2.3 and why is it relevant? Is it entropy of the distribution of pixel
intensities in a patch? To me it seems that any measure which depends only
on one patch is irrelevant because the methods discussed (e.g., saliency maps)
depend on context as well as the intensities within a patch.


* The presentation in the paper would be improved if the results in section 7
were presented along with relevant discussion in preceding sections.


Overall Evaluation
===
The experiments presented here are novel, but I am not sure they are very
significant or offer clear conclusions. The methods and goals are not presented
clearly and lack the broader relevant context mentioned above. Furthermore, I
find the lines of thought mentioned in the Cons section possibly incorrect
or incomplete. As detailed with further clarifying questions, upon closer
inspection I do not see how some aspects of the proposed approach were
implemented, so my opinion may change with further details.",0
"SUMMARY 
This paper studies the expressive power of deep neural networks under various related measures of expressivity. 
It discusses how these measures relate to the `trajectory length', which is shown to depend exponentially on the depth of the network, in expectation (at least experimentally, at an intuitive level, or theoretically under certain assumptions). 
The paper also emphasises the importance of the weights in the earlier layers of the network, as these have a larger influence on the represented classes of functions, and demonstrates this in an experimental setting. 

PROS 
The paper further advances on topics related to the expressive power of feedforward neural networks with piecewise linear activation functions, in particular elaborating on the relations between various points of view. 

CONS 
The paper further advances and elaborates on interesting topics, but to my appraisal it does not contribute significantly new aspects to the discussion. 

COMMENTS
- The paper is a bit long (especially the appendix) and seems to have been written a bit in a rush. 
Overall the main points are presented clearly, but the results and conclusions could be clearer about the assumptions / experimental vs theoretical nature. 
The connection to previous works could also be clearer. 

- On page 2 one finds the statement ``Furthermore, architectures are often compared via ‘hardcoded’ weight values -- a specific function that can be represented efficiently by one architecture is shown to only be inefficiently approximated by another.'' 

This is partially true, but it neglects important parts of the discussion conducted in the cited papers. 
In particular, the paper [Montufar, Pascanu, Cho, Bengio 2014] discusses not one hard coded function, but classes of functions with a given number of linear regions. 
That paper shows that deep networks generically* produce functions with at least a given number of linear regions, while shallow networks never do. 
* Generically meaning that, after fixing the number of parameters, any function represented by the network, for parameter values form an open, positive -measure, neighbourhood, belongs to the class of functions which have at least a certain number of linear regions. 
In particular, such statements can be directly interpreted in terms of networks with random weights. 

- One of the measures for expressivity discussed in the present paper is the number of Dichotomies. In statistical learning theory, this notion is used to define the VC-dimension. In that context, a high value is associated with a high statistical complexity, meaning that picking a good hypothesis requires more data. 

- On page 2 one finds the statement ``We discover and prove the underlying reason for this – all three measures are directly proportional to a fourth quantity, trajectory length.'' 
The expected trajectory length increasing exponentially with depth can be interpreted as the increase (or decrease) in the scale by a composition of the form a*...*a x, which scales the inputs by a^d. Such a scaling by itself certainly is not an underlying cause for an increase in the number of dichotomies or activation patterns or transitions. Here it seems that at least the assumptions on the considered types of trajectories also play an important role. 
This is probably related to another observation from page 4: ``if the variance of the bias is comparatively too large... then we no longer see exponential growth.''

OTHER SPECIFIC COMMENTS 
In Theorem 1 
- Here it would be good to be more specific about ``random neural network'', i.e., fixed connectivity structure with random weights, and also about the kind of one-dimensional trajectory, i.e., finite in length, closed, differentiable almost everywhere, etc. 

- The notation ``g \geq O(f)'' used in the theorem reads literally as |g| \geq \leq k |f| for some k>0, for large enough arguments. It could also be read as g being not smaller than some function that is bounded above by f, which holds for instance whenever g\geq 0. 
For expressing asymptotic lower bounds one can use the notation \Omega (see",0
"SUMMARY 
This paper studies the expressive power of deep neural networks under various related measures of expressivity. 
It discusses how these measures relate to the `trajectory length', which is shown to depend exponentially on the depth of the network, in expectation (at least experimentally, at an intuitive level, or theoretically under certain assumptions). 
The paper also emphasises the importance of the weights in the earlier layers of the network, as these have a larger influence on the represented classes of functions, and demonstrates this in an experimental setting. 

PROS 
The paper further advances on topics related to the expressive power of feedforward neural networks with piecewise linear activation functions, in particular elaborating on the relations between various points of view. 

CONS 
The paper further advances and elaborates on interesting topics, but to my appraisal it does not contribute significantly new aspects to the discussion. 

COMMENTS
- The paper is a bit long (especially the appendix) and seems to have been written a bit in a rush. 
Overall the main points are presented clearly, but the results and conclusions could be clearer about the assumptions / experimental vs theoretical nature. 
The connection to previous works could also be clearer. 

- On page 2 one finds the statement ``Furthermore, architectures are often compared via ‘hardcoded’ weight values -- a specific function that can be represented efficiently by one architecture is shown to only be inefficiently approximated by another.'' 

This is partially true, but it neglects important parts of the discussion conducted in the cited papers. 
In particular, the paper [Montufar, Pascanu, Cho, Bengio 2014] discusses not one hard coded function, but classes of functions with a given number of linear regions. 
That paper shows that deep networks generically* produce functions with at least a given number of linear regions, while shallow networks never do. 
* Generically meaning that, after fixing the number of parameters, any function represented by the network, for parameter values form an open, positive -measure, neighbourhood, belongs to the class of functions which have at least a certain number of linear regions. 
In particular, such statements can be directly interpreted in terms of networks with random weights. 

- One of the measures for expressivity discussed in the present paper is the number of Dichotomies. In statistical learning theory, this notion is used to define the VC-dimension. In that context, a high value is associated with a high statistical complexity, meaning that picking a good hypothesis requires more data. 

- On page 2 one finds the statement ``We discover and prove the underlying reason for this – all three measures are directly proportional to a fourth quantity, trajectory length.'' 
The expected trajectory length increasing exponentially with depth can be interpreted as the increase (or decrease) in the scale by a composition of the form a*...*a x, which scales the inputs by a^d. Such a scaling by itself certainly is not an underlying cause for an increase in the number of dichotomies or activation patterns or transitions. Here it seems that at least the assumptions on the considered types of trajectories also play an important role. 
This is probably related to another observation from page 4: ``if the variance of the bias is comparatively too large... then we no longer see exponential growth.''

OTHER SPECIFIC COMMENTS 
In Theorem 1 
- Here it would be good to be more specific about ``random neural network'', i.e., fixed connectivity structure with random weights, and also about the kind of one-dimensional trajectory, i.e., finite in length, closed, differentiable almost everywhere, etc. 

- The notation ``g \geq O(f)'' used in the theorem reads literally as |g| \geq \leq k |f| for some k>0, for large enough arguments. It could also be read as g being not smaller than some function that is bounded above by f, which holds for instance whenever g\geq 0. 
For expressing asymptotic lower bounds one can use the notation \Omega (see",0
"The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short:

1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms. 

2. The definition (1),  and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms).

3.  It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for algorithm tuning. 

At the least, many more empirical comparisons should be provided to convince me that the universality holds across a broad range of algorithms.",0
"The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short:

1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms. 

2. The definition (1),  and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms).

3.  It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for algorithm tuning. 

At the least, many more empirical comparisons should be provided to convince me that the universality holds across a broad range of algorithms.",0
"The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). 

Using an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures. 

Although good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned  k values are mostly very small and not varying much across layers. Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments.",0
"The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). 

Using an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures. 

Although good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned  k values are mostly very small and not varying much across layers. Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments.",0
"This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.
Comments
1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.
2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited. This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.
3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.

Overall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the paper could be of interest to people scaling up learning. I would encourage the author to extend the method to the cases of non-linear objective function which could make it more interesting to the ICLR community",0
"This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.
Comments
1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.
2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited. This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.
3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.

Overall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the paper could be of interest to people scaling up learning. I would encourage the author to extend the method to the cases of non-linear objective function which could make it more interesting to the ICLR community",0
I was holding off on this review hoping to get the missing details from the code at,0
I was holding off on this review hoping to get the missing details from the code at,0
"This work proposes to use visualization of gradients to further understand the importance of features (i.e. pixels) for visual classification. Overall, this presented visualizations are interesting, however, the approach is very ad hoc. The authors do not explain why visualizing regular gradients isn't correlated with the importance of features relevant to the given visual category and proceed to the interior gradient approach. 

One particular question with regular gradients at features that form the spatial support of the visual class. Is it the case that the gradients of the features that are confident of the prediction remain low, while those with high uncertainty will have strong gradients?

With regards to the interior gradients, it is unclear how the scaling parameter \alpha affects the feature importance and how it is related to attention.

Finally, does this model use batch normalization?",0
"This work proposes to use visualization of gradients to further understand the importance of features (i.e. pixels) for visual classification. Overall, this presented visualizations are interesting, however, the approach is very ad hoc. The authors do not explain why visualizing regular gradients isn't correlated with the importance of features relevant to the given visual category and proceed to the interior gradient approach. 

One particular question with regular gradients at features that form the spatial support of the visual class. Is it the case that the gradients of the features that are confident of the prediction remain low, while those with high uncertainty will have strong gradients?

With regards to the interior gradients, it is unclear how the scaling parameter \alpha affects the feature importance and how it is related to attention.

Finally, does this model use batch normalization?",0
"This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs. The authors present a convincing set of results over many translation tasks and compare with very competitive baselines. I also appreciate the detailed report on training and generation speed. I find it's very interesting when position embeddings turn out to be hugely important (beside residual connections); unfortunately, there is little analysis to shed more lights on this aspect and perhaps compare other ways of capturing positions (a wild guess might be to use embeddings that represent some form of relative positions). The only concern I have (similar to the other reviewer) is that this paper perhaps fits better in an NLP conference.

One minor comment: it's slight strange that this well-executed paper doesn't have a single figure on the proposed architecture :) It will also be even better to draw a figure for the biLSTM architecture as well (it does take some effort to understand the last paragraph in Section 2, especially the part on having a linear layer to compute z).",0
"This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs. The authors present a convincing set of results over many translation tasks and compare with very competitive baselines. I also appreciate the detailed report on training and generation speed. I find it's very interesting when position embeddings turn out to be hugely important (beside residual connections); unfortunately, there is little analysis to shed more lights on this aspect and perhaps compare other ways of capturing positions (a wild guess might be to use embeddings that represent some form of relative positions). The only concern I have (similar to the other reviewer) is that this paper perhaps fits better in an NLP conference.

One minor comment: it's slight strange that this well-executed paper doesn't have a single figure on the proposed architecture :) It will also be even better to draw a figure for the biLSTM architecture as well (it does take some effort to understand the last paragraph in Section 2, especially the part on having a linear layer to compute z).",0
"The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net.
Although the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well.
The proposed approach relates to Batch Norm and weight decay.
Experiments are given on ""low-shot"" settting.
There seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?
Regarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don't put in Table 1. Why?
Overall, the idea is simple but feels like preliminary: while it is supposed to be a ""soft BN"", BN itself gets better performance than feature penalty, and both together give even better results. Is something still missing in the explanation?

-- edits after revised version:

Thank you for adding more information to the paper. I feel it is still too long but hopefully you can reduce it to 9 pages as promised. However, I'm still not convinced the paper is ready to be accepted, mainly for the following reasons:
- on Omniglot, the paper is still significantly far from the current state of the art.
- the new experiments do not really confirm/infirm the relationship with BN.
- you added an explanation of why FP works for low-shot setting, by showing it controls the VC dimension and hence is good to control overfitting with a small number of training examples, but this discussion is basic and does not really shed more light than the obvious.
I'm pushing up your score from 4 to 5 for the improved version, but I still think it is below acceptance level.",0
"The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net.
Although the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well.
The proposed approach relates to Batch Norm and weight decay.
Experiments are given on ""low-shot"" settting.
There seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?
Regarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don't put in Table 1. Why?
Overall, the idea is simple but feels like preliminary: while it is supposed to be a ""soft BN"", BN itself gets better performance than feature penalty, and both together give even better results. Is something still missing in the explanation?

-- edits after revised version:

Thank you for adding more information to the paper. I feel it is still too long but hopefully you can reduce it to 9 pages as promised. However, I'm still not convinced the paper is ready to be accepted, mainly for the following reasons:
- on Omniglot, the paper is still significantly far from the current state of the art.
- the new experiments do not really confirm/infirm the relationship with BN.
- you added an explanation of why FP works for low-shot setting, by showing it controls the VC dimension and hence is good to control overfitting with a small number of training examples, but this discussion is basic and does not really shed more light than the obvious.
I'm pushing up your score from 4 to 5 for the improved version, but I still think it is below acceptance level.",0
"This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea).

1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of “robust RL”. A good place to start might be with the work of Shie Mannor.

2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as “catastrophic” if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I’d want to see plots for “average reward” in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for entering these otherwise invisible “danger states”?

3) a somewhat ad-hoc solution, that introduces new domain-specific hyperparameters (k_r, k_lambda and lambda) a second deep network and and two additional replay memories. In terms of results, I’m also unsure whether I can trust the results, given the long-standing track-record of cart-pole being fully solved by many methods: is DQN an outlier here? Or is the convnet not an appropriate function-approximator? Actually: which exact variant “state-of-the-art” variant of DQN are you using?

The good idea that I encourage the authors to pursue further is D_d, this set of rare but dangerous states, that should be kept around in some form. I see it as an ingredient for continual learning that most typical methods lack -- it is also one of the big differences between RL and supervised learning, where such states would generally be discarded as outliers.

Given my comment a couple of weeks ago, and the prompt response (“we implemented expected SARSA”), I would have expected that the paper had been revised with the new results by now? In any case, I’m open to discussing all these points and revising my opinion based on an updated version of the paper.

Minor comment: the bibliography is done sloppily, with missing years, conference venues and missing/misspelled author lists, e.g. “Sergey et al. Levine”. I also think it is good form to cite the actual conference publications instead of arXiv where applicable.",0
"This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea).

1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of “robust RL”. A good place to start might be with the work of Shie Mannor.

2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as “catastrophic” if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I’d want to see plots for “average reward” in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for entering these otherwise invisible “danger states”?

3) a somewhat ad-hoc solution, that introduces new domain-specific hyperparameters (k_r, k_lambda and lambda) a second deep network and and two additional replay memories. In terms of results, I’m also unsure whether I can trust the results, given the long-standing track-record of cart-pole being fully solved by many methods: is DQN an outlier here? Or is the convnet not an appropriate function-approximator? Actually: which exact variant “state-of-the-art” variant of DQN are you using?

The good idea that I encourage the authors to pursue further is D_d, this set of rare but dangerous states, that should be kept around in some form. I see it as an ingredient for continual learning that most typical methods lack -- it is also one of the big differences between RL and supervised learning, where such states would generally be discarded as outliers.

Given my comment a couple of weeks ago, and the prompt response (“we implemented expected SARSA”), I would have expected that the paper had been revised with the new results by now? In any case, I’m open to discussing all these points and revising my opinion based on an updated version of the paper.

Minor comment: the bibliography is done sloppily, with missing years, conference venues and missing/misspelled author lists, e.g. “Sergey et al. Levine”. I also think it is good form to cite the actual conference publications instead of arXiv where applicable.",0
"This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers. Additionally they release a Python open source dataset. As expected this augmentation, the fixed attention policy, improves the perplexity of the model. It seems important to dig a bit deeper into these results and show the contribution of different token types to the achieve perplexity. This is alluded to in the text, but a more thorough comparison would be welcome. The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty --- for example the Maddison and Tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope.",0
"This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers. Additionally they release a Python open source dataset. As expected this augmentation, the fixed attention policy, improves the perplexity of the model. It seems important to dig a bit deeper into these results and show the contribution of different token types to the achieve perplexity. This is alluded to in the text, but a more thorough comparison would be welcome. The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty --- for example the Maddison and Tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope.",0
"ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper:

1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission ""Singularity of Hessian in Deep Learning"") even in cases where the optimization is not challenging. Therefore, I don't agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.

2- Hessian at zero initial point: The explanation of why we should be interested in Hessain at zero initial point is not acceptable. The zero initial point is not interesting because it is a very particular point that cannot tell us about the Hessian during optimization.",0
"ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper:

1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission ""Singularity of Hessian in Deep Learning"") even in cases where the optimization is not challenging. Therefore, I don't agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.

2- Hessian at zero initial point: The explanation of why we should be interested in Hessain at zero initial point is not acceptable. The zero initial point is not interesting because it is a very particular point that cannot tell us about the Hessian during optimization.",0
"This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.

Although the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in ""A Simple Word Embedding Model for Lexical Substitution"" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.

In addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: 
*",0
"This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.

Although the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in ""A Simple Word Embedding Model for Lexical Substitution"" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.

In addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: 
*",0
"This paper presents a generative model for binary images.  Images are composed by placing a set of binary features at locations in the image.  These features are OR'd together to produce an image.  In a hierarchical variant, features/classes can have a set of possible templates, one of which can be active.  Variables are defined to control which template is present in each layer.  A joint probability distribution over both the feature appearance and instance/location variables is defined.

Overall, the goal of this work is interesting -- it would be satisfying if semantically meaningful features could be extracted, allowing compositionality in a generative model of images.  However, it isn't clear this would necessarily result from the proposed process.
Why would the learned features (building blocks) necessarily semantically meaningful?  In the motivating example of text, rather than discovering letters, features could correspond to many other sub-units (parts of letters), or other features lacking direct semantic meaning.

The current instantiation of the model is limited.  It models binary image patterns.  The experiments are done on synthetic data and MNIST digits.  The method recovers the structure and is effective at classification on synthetic data that are directly compositional.  On the MNIST data, the test errors are quite large, and worse than a CNN except when synthetic data corruption is added.  Further work to enhance the ability of the method to handle natural images or naturally occuring data variation would enhance the paper.",0
"This paper presents a generative model for binary images.  Images are composed by placing a set of binary features at locations in the image.  These features are OR'd together to produce an image.  In a hierarchical variant, features/classes can have a set of possible templates, one of which can be active.  Variables are defined to control which template is present in each layer.  A joint probability distribution over both the feature appearance and instance/location variables is defined.

Overall, the goal of this work is interesting -- it would be satisfying if semantically meaningful features could be extracted, allowing compositionality in a generative model of images.  However, it isn't clear this would necessarily result from the proposed process.
Why would the learned features (building blocks) necessarily semantically meaningful?  In the motivating example of text, rather than discovering letters, features could correspond to many other sub-units (parts of letters), or other features lacking direct semantic meaning.

The current instantiation of the model is limited.  It models binary image patterns.  The experiments are done on synthetic data and MNIST digits.  The method recovers the structure and is effective at classification on synthetic data that are directly compositional.  On the MNIST data, the test errors are quite large, and worse than a CNN except when synthetic data corruption is added.  Further work to enhance the ability of the method to handle natural images or naturally occuring data variation would enhance the paper.",0
"The authors explore the idea of deep-learning a static analyzer. They do it with a toy programming language and a very simplified analysis problem -- just checking if all variables are initalized.

While the idea is interesting and might be developped into a tool in the future, the toy task presented in this paper is too simple to warrant an ICLR submission. Just detecting whether a variable is initialized in a string is a toy algorihtmic task, similar to the ones solved in a number of paper in recent years by models such as the Neural Turing Machine, Stack RNNs, Neural GPU, or Differentiable Neural Computer. All these architectures perform almost perfectly on a number of algorithmic tasks, so it is highly probable that they would also solve this one. Unluckily, the authors only compare to much more basic models, such as HMMs. Since the code for many of the above-mentioned models is available online, a paper without these baselines is not ready for ICLR. Moreover, there is a risk that existing models already solve this problem very well, making the contribution unclear.",0
"The authors explore the idea of deep-learning a static analyzer. They do it with a toy programming language and a very simplified analysis problem -- just checking if all variables are initalized.

While the idea is interesting and might be developped into a tool in the future, the toy task presented in this paper is too simple to warrant an ICLR submission. Just detecting whether a variable is initialized in a string is a toy algorihtmic task, similar to the ones solved in a number of paper in recent years by models such as the Neural Turing Machine, Stack RNNs, Neural GPU, or Differentiable Neural Computer. All these architectures perform almost perfectly on a number of algorithmic tasks, so it is highly probable that they would also solve this one. Unluckily, the authors only compare to much more basic models, such as HMMs. Since the code for many of the above-mentioned models is available online, a paper without these baselines is not ready for ICLR. Moreover, there is a risk that existing models already solve this problem very well, making the contribution unclear.",0
"This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.  Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system. 

I think the paper is well motivated. However, there are several concerns:
1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as",0
"This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.  Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system. 

I think the paper is well motivated. However, there are several concerns:
1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as",0
"Because the authors did not respond to reviewer feedback, I am maintaining my original review score.

-----

This paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account. However, the model is restricted to using a linear decoder. Experimental results are positive but not convincing.

Strengths:
- The authors target a worthwhile and challenging problem: incorporating the modeling of uncertainty over hidden states with the power of flexible neural net-like models.
- The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states is clever. Combined with the Gaussian distribution over hidden states, the resulting regularization term is simple and differentiable.
- This general approach -- focusing on writing down the problem as a neural network-like loss function -- seems robust and flexible and could be combined with other approaches, including variants of variational autoencoders.

Weaknesses:
- The presentation is a muddled, especially the model definition in Sec. 3.3. The authors introduce four variants of their model with different combinations of decoder (with and without variance term) and linear vs. MLP transition function. It appears that the 2,2 variant is generally better but not on all metrics and often by small margins. This makes drawing a solid conclusions difficult: what each component of the loss contributes, whether and how the nonlinear transition function helps and how much, how in practice the model should be applied, etc. I would suggest two improvements to the manuscript: (1) focus on the main 2,2 variant in Sec. 3.3 (with the hypothesis that it should perform best) and make the simpler variants additional ""baselines"" described in a paragraph in Sec. 4.1; (2) perform more thorough experiments with larger data sets to make a stronger case for the superiority of this approach.
- The authors only allude to learning (with references to gradient descent and ADAM during model description) in this framework. Inference gets its one subsection but only one sentence that ends in an ellipsis (?).
- It's unclear what is the purpose of introducing the inequality in Eq. 9.
- Experimental results are not convincing: given the size of the data, the differences vs. the RNN and KF baselines is probably not significant, and these aren't particularly strong baselines (especially if it is in fact an RNN and not an LSTM or GRU).
- The position of this paper is unclear with respect to variational autoencoders and related models. Recurrent variants of VAEs (e.g., Krishnan, et al., 2015) seem to achieve most of the same goals as far as uncertainty modeling is concerned. It seems like those could easily be extended to model relationships between time series using the simple regularization strategy used here. Same goes for Johnson, et al., 2016 (mentioned in separate question).

This is a valuable research direction with some intriguing ideas and interesting preliminary results. I would suggest that the authors restructure this manuscript a bit, striving for clarity of model description similar to the papers cited above and providing greater detail about learning and inference. They also need to perform more thorough experiments and present results that tell a clear story about the strengths and weaknesses of this approach.",0
"Because the authors did not respond to reviewer feedback, I am maintaining my original review score.

-----

This paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account. However, the model is restricted to using a linear decoder. Experimental results are positive but not convincing.

Strengths:
- The authors target a worthwhile and challenging problem: incorporating the modeling of uncertainty over hidden states with the power of flexible neural net-like models.
- The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states is clever. Combined with the Gaussian distribution over hidden states, the resulting regularization term is simple and differentiable.
- This general approach -- focusing on writing down the problem as a neural network-like loss function -- seems robust and flexible and could be combined with other approaches, including variants of variational autoencoders.

Weaknesses:
- The presentation is a muddled, especially the model definition in Sec. 3.3. The authors introduce four variants of their model with different combinations of decoder (with and without variance term) and linear vs. MLP transition function. It appears that the 2,2 variant is generally better but not on all metrics and often by small margins. This makes drawing a solid conclusions difficult: what each component of the loss contributes, whether and how the nonlinear transition function helps and how much, how in practice the model should be applied, etc. I would suggest two improvements to the manuscript: (1) focus on the main 2,2 variant in Sec. 3.3 (with the hypothesis that it should perform best) and make the simpler variants additional ""baselines"" described in a paragraph in Sec. 4.1; (2) perform more thorough experiments with larger data sets to make a stronger case for the superiority of this approach.
- The authors only allude to learning (with references to gradient descent and ADAM during model description) in this framework. Inference gets its one subsection but only one sentence that ends in an ellipsis (?).
- It's unclear what is the purpose of introducing the inequality in Eq. 9.
- Experimental results are not convincing: given the size of the data, the differences vs. the RNN and KF baselines is probably not significant, and these aren't particularly strong baselines (especially if it is in fact an RNN and not an LSTM or GRU).
- The position of this paper is unclear with respect to variational autoencoders and related models. Recurrent variants of VAEs (e.g., Krishnan, et al., 2015) seem to achieve most of the same goals as far as uncertainty modeling is concerned. It seems like those could easily be extended to model relationships between time series using the simple regularization strategy used here. Same goes for Johnson, et al., 2016 (mentioned in separate question).

This is a valuable research direction with some intriguing ideas and interesting preliminary results. I would suggest that the authors restructure this manuscript a bit, striving for clarity of model description similar to the papers cited above and providing greater detail about learning and inference. They also need to perform more thorough experiments and present results that tell a clear story about the strengths and weaknesses of this approach.",0
"The topic of the paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are too incremental. Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do. However neither the methodology not the results are novel / surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.

I’m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors.",0
"The topic of the paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are too incremental. Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do. However neither the methodology not the results are novel / surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.

I’m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors.",0
"This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians. It uses an AlexNet (CaffeNet) backbone architecture modified to include batch normalization. Experimental results are presented on the INRIA and ETH datasets.

Pros
- The paper is clearly written and easy to follow

Cons
- The paper's two contributions are too minor to merit publication
- Experimental results should include at least the Caltech pedestrian dataset but likely also the KITTI pedestrian dataset
- Recent work from ECCV 2016 [a], with superior results and much more experimental evaluation, is not cited or discussed

My rating is due primarily to the lack luster contributions. The first claimed contribution is the use of EdgeBoxes as proposals for pedestrian detection. Unless the result of this choice produced a truly surprising experimental result, this is simply too minor to be considered a contribution. Moreover, if this choice is important, then the paper should justify it by showing that other proposal methods (of which there are a great many in addition to Selective Search and Edge Boxes) are worse performing in some regard (speed, accuracy, memory, etc.). The second claimed contribution is the use of batch normalization (BN) in their network architecture. There is a case to be made that BN hasn't been explored in Fast R-CNN. However, if the goal of the paper was to thoroughly explore BN + Fast R-CNN, then why focus narrowly on pedestrian detection? Instead, it should focus more broadly on generic object category detection for which there are well established Fast R-CNN baselines on PASCAL VOC and COCO. The use of BN + Fast R-CNN only for pedestrian detection does not provide much signal about this choice. There are also potential technical issues that are not discussed. BN is typically avoided in Fast R-CNN because the batch size seen by most of the network is usually only one or two images. This is likely too few images for the naive application of BN.


[a] ""Is Faster R-CNN Doing Well for Pedestrian Detection?"" Zhang et al.",0
"This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians. It uses an AlexNet (CaffeNet) backbone architecture modified to include batch normalization. Experimental results are presented on the INRIA and ETH datasets.

Pros
- The paper is clearly written and easy to follow

Cons
- The paper's two contributions are too minor to merit publication
- Experimental results should include at least the Caltech pedestrian dataset but likely also the KITTI pedestrian dataset
- Recent work from ECCV 2016 [a], with superior results and much more experimental evaluation, is not cited or discussed

My rating is due primarily to the lack luster contributions. The first claimed contribution is the use of EdgeBoxes as proposals for pedestrian detection. Unless the result of this choice produced a truly surprising experimental result, this is simply too minor to be considered a contribution. Moreover, if this choice is important, then the paper should justify it by showing that other proposal methods (of which there are a great many in addition to Selective Search and Edge Boxes) are worse performing in some regard (speed, accuracy, memory, etc.). The second claimed contribution is the use of batch normalization (BN) in their network architecture. There is a case to be made that BN hasn't been explored in Fast R-CNN. However, if the goal of the paper was to thoroughly explore BN + Fast R-CNN, then why focus narrowly on pedestrian detection? Instead, it should focus more broadly on generic object category detection for which there are well established Fast R-CNN baselines on PASCAL VOC and COCO. The use of BN + Fast R-CNN only for pedestrian detection does not provide much signal about this choice. There are also potential technical issues that are not discussed. BN is typically avoided in Fast R-CNN because the batch size seen by most of the network is usually only one or two images. This is likely too few images for the naive application of BN.


[a] ""Is Faster R-CNN Doing Well for Pedestrian Detection?"" Zhang et al.",0
"The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.

I have two main concerns. One is the lack of comparisons to similar recently proposed methods - ""Learning Step Size Controllers for Robust Neural Network Training"" by Daniel et al. and ""Learning to learn by gradient descent by gradient descent"" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?

My second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:",0
"The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.

I have two main concerns. One is the lack of comparisons to similar recently proposed methods - ""Learning Step Size Controllers for Robust Neural Network Training"" by Daniel et al. and ""Learning to learn by gradient descent by gradient descent"" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?

My second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:",0
"The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions. As such one gives ""complementary viewpoints"" of the input to the subsequent layers, which can be thought of as performing ensembling/expert combination within the model, rather than using an ensemble of networks. 

For this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a ""foreground"" and a ""background"" subset, and append side-losses that force them to be zero on background and foreground pixels respectively. 

They demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a ""vanilla"" baseline that does not use these losses.

I enjoyed reading the paper because the idea is simple, smart, and seems to be effective. 
But there are a few concerns;
-firstly, the way of doing this seems very particular to vision. In vision one knows that masking the features (during both training and testing) helps, e.g.",0
"The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions. As such one gives ""complementary viewpoints"" of the input to the subsequent layers, which can be thought of as performing ensembling/expert combination within the model, rather than using an ensemble of networks. 

For this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a ""foreground"" and a ""background"" subset, and append side-losses that force them to be zero on background and foreground pixels respectively. 

They demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a ""vanilla"" baseline that does not use these losses.

I enjoyed reading the paper because the idea is simple, smart, and seems to be effective. 
But there are a few concerns;
-firstly, the way of doing this seems very particular to vision. In vision one knows that masking the features (during both training and testing) helps, e.g.",0
"This paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf’s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.

The model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.

Overall the paper could use some rewriting especially the notations in section 3. The experiments are well executed and they definitely get good results. The heat maps at the end are very insightful. 

Comments

This contributions of this paper would be much stronger if it showed improvements in a practical applications such as Question Answering (although the paper clearly mentions that this technique could be applied to improve QA)
In section 3, it is unclear why the authors refer the entity as a ‘topic'. This makes the text a little confusing since a topic can also be associated with something abstract, but in this case the topic is always a freebase entity. 
Is it really necessary to predict a fact at every step before generating a word. In other words, how many distinct facts on average does the model choose to generate a sentence. Intuitively a natural language sentence would be describe few facts about an entity. If the fact generation step could be avoided (by adding a latent variable which decides if the fact should be generated or not), the model will also be faster.
In equation 2, the model has to make a hard decision to choose the fact. For this to be end to end trained, every word needs to be annotated with a corresponding fact which might not be always a realistic scenario. For e.g., in domains such as social media text.
Learning position embeddings for copying knowledge words seems a little counter-intuitive. Does the sequence of knowledge words follow any particular structure like word O_2 is always the last name (e.g. Obama).
It would also be nice to compare to char-level LM's which inherently solves the unknown token problem.",0
"This paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf’s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.

The model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.

Overall the paper could use some rewriting especially the notations in section 3. The experiments are well executed and they definitely get good results. The heat maps at the end are very insightful. 

Comments

This contributions of this paper would be much stronger if it showed improvements in a practical applications such as Question Answering (although the paper clearly mentions that this technique could be applied to improve QA)
In section 3, it is unclear why the authors refer the entity as a ‘topic'. This makes the text a little confusing since a topic can also be associated with something abstract, but in this case the topic is always a freebase entity. 
Is it really necessary to predict a fact at every step before generating a word. In other words, how many distinct facts on average does the model choose to generate a sentence. Intuitively a natural language sentence would be describe few facts about an entity. If the fact generation step could be avoided (by adding a latent variable which decides if the fact should be generated or not), the model will also be faster.
In equation 2, the model has to make a hard decision to choose the fact. For this to be end to end trained, every word needs to be annotated with a corresponding fact which might not be always a realistic scenario. For e.g., in domains such as social media text.
Learning position embeddings for copying knowledge words seems a little counter-intuitive. Does the sequence of knowledge words follow any particular structure like word O_2 is always the last name (e.g. Obama).
It would also be nice to compare to char-level LM's which inherently solves the unknown token problem.",0
"This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.

The main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.

Regarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.

Regarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance. 

Overall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence.",0
"This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.

The main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.

Regarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.

Regarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance. 

Overall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence.",0
"This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval. In other words, the network is off-the-shelf and solely acts as a feature extractor. The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as ""traditional wisdom"".

Specifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on. 

While this type of experimental study is reasonable and well motivated, it suffers from a huge problem. Namely it ""ignores"" 2 major recent works that are in direct contradictions with many claims of the paper ([a] ""End-to-end Learning of Deep Visual Representations for Image Retrieval"" by  Gordo et al. and [b] ""CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples"" by Radenović et al., both ECCV'16 papers). These works have shown that training for retrieval can be achieved with a siamese architectures and have demonstrated outstanding performance. As a result, many claims and findings of the paper are either outdated, questionable or just wrong.

Here are some of the misleading claims: 

  - ""Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years.""
  Until [a] (not cited), the state-of-the-art was still largely dominated by sparse invariant features based methods (see last Table in [a]).
  
  - ""the proposed method [...] outperforms the state-of-the-art methods on four typical datasets""
  That is not true, for the same reasons than above, and also because the state-of-the-art is now dominated by [a] and [b].
  
  - ""Also in situations where a large numbers of training samples are not available, instance retrieval using unsupervised method is still preferable and may be the only option."".
  This is a questionable opinion. The method exposed in ""End-to-end Learning of Deep Visual Representations for Image Retrieval"" by Gordo et al. outperforms the state-of-the-art on the UKB dataset (3.84 without QE or DBA) whereas it was trained for landmarks retrieval and not objects, i.e. in a different retrieval context. This demonstrates that in spite of insufficient training data, training is still possible and beneficial.

  - Finally, most findings are not even new or surprising (e.g. aggregate several regions in a multi-scale manner was already achieved by Tolias at al, etc.). So the interest of the paper is limited overall.

In addition, there are some problems in the experiments. For instance, the tuning experiments are only conducted on the Oxford dataset and using a single network (VGG-19), whereas it is not clear whether these conditions are well representative of all datasets and all networks (it is well known that the Oxford dataset behaves very differently than the Holidays dataset, for instance). In addition, tuning is performed very aggressively, making it look like the authors are tuning on the test set (e.g. see Table 3). 

To conclude, the paper is one year too late with respect to recent developments in the state of the art.",0
"This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval. In other words, the network is off-the-shelf and solely acts as a feature extractor. The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as ""traditional wisdom"".

Specifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on. 

While this type of experimental study is reasonable and well motivated, it suffers from a huge problem. Namely it ""ignores"" 2 major recent works that are in direct contradictions with many claims of the paper ([a] ""End-to-end Learning of Deep Visual Representations for Image Retrieval"" by  Gordo et al. and [b] ""CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples"" by Radenović et al., both ECCV'16 papers). These works have shown that training for retrieval can be achieved with a siamese architectures and have demonstrated outstanding performance. As a result, many claims and findings of the paper are either outdated, questionable or just wrong.

Here are some of the misleading claims: 

  - ""Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years.""
  Until [a] (not cited), the state-of-the-art was still largely dominated by sparse invariant features based methods (see last Table in [a]).
  
  - ""the proposed method [...] outperforms the state-of-the-art methods on four typical datasets""
  That is not true, for the same reasons than above, and also because the state-of-the-art is now dominated by [a] and [b].
  
  - ""Also in situations where a large numbers of training samples are not available, instance retrieval using unsupervised method is still preferable and may be the only option."".
  This is a questionable opinion. The method exposed in ""End-to-end Learning of Deep Visual Representations for Image Retrieval"" by Gordo et al. outperforms the state-of-the-art on the UKB dataset (3.84 without QE or DBA) whereas it was trained for landmarks retrieval and not objects, i.e. in a different retrieval context. This demonstrates that in spite of insufficient training data, training is still possible and beneficial.

  - Finally, most findings are not even new or surprising (e.g. aggregate several regions in a multi-scale manner was already achieved by Tolias at al, etc.). So the interest of the paper is limited overall.

In addition, there are some problems in the experiments. For instance, the tuning experiments are only conducted on the Oxford dataset and using a single network (VGG-19), whereas it is not clear whether these conditions are well representative of all datasets and all networks (it is well known that the Oxford dataset behaves very differently than the Holidays dataset, for instance). In addition, tuning is performed very aggressively, making it look like the authors are tuning on the test set (e.g. see Table 3). 

To conclude, the paper is one year too late with respect to recent developments in the state of the art.",0
"This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave, but the replies were not so convincing. 

This paper was actually difficult for me to follow. For instance, in Figure 1, a bicluster matrix is constructed as an outer product of $h$ and $w$. $h$ is a hidden unit, but what is $w$? I could not find any definition. Furthermore, I could not know how $h$ is estimated in this method. Therefore, I do NOT understand how this method performs biclustering. 

Totally, I am not sure that this paper is suitable for publication. 

Prons:
Empirical performance is good.

Cons:
Novelty of the proposed method
Some description in the paper is unclear.",0
"This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave, but the replies were not so convincing. 

This paper was actually difficult for me to follow. For instance, in Figure 1, a bicluster matrix is constructed as an outer product of $h$ and $w$. $h$ is a hidden unit, but what is $w$? I could not find any definition. Furthermore, I could not know how $h$ is estimated in this method. Therefore, I do NOT understand how this method performs biclustering. 

Totally, I am not sure that this paper is suitable for publication. 

Prons:
Empirical performance is good.

Cons:
Novelty of the proposed method
Some description in the paper is unclear.",0
"In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs. For example, the authors highlight the well known but often not sufficiently appreciated fact that the probability mass of high dimensional Gaussian distributions concentrates near a thin hyper-shell with a certain radius. They therefore propose to use spherical interpolations (great arcs) instead of the commonly used linear interpolations. In a similar spirit they propose a visualisation for analogies and techniques to reinforce structure in VAE latent spaces.

I find it hard to give clear recommendation for this paper: On the one hand I enjoyed reading it and I might want use some of the proposals (e.g. spherical interpolations; J-diagrams) in future work of mine. On the other hand, it’s obvious that this paper is not a typical machine learning paper; it does not propose a new model, or training method, or provide (theoretical/empirical) insight and it does not have the scientific quality and depth I’ve seen in many other ICLR submissions. But it does more than just describing useful “tricks”. And all things considered I think this paper deserves a wider audience (but  I'm not convinced that ICLR is the right venue)",0
"In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs. For example, the authors highlight the well known but often not sufficiently appreciated fact that the probability mass of high dimensional Gaussian distributions concentrates near a thin hyper-shell with a certain radius. They therefore propose to use spherical interpolations (great arcs) instead of the commonly used linear interpolations. In a similar spirit they propose a visualisation for analogies and techniques to reinforce structure in VAE latent spaces.

I find it hard to give clear recommendation for this paper: On the one hand I enjoyed reading it and I might want use some of the proposals (e.g. spherical interpolations; J-diagrams) in future work of mine. On the other hand, it’s obvious that this paper is not a typical machine learning paper; it does not propose a new model, or training method, or provide (theoretical/empirical) insight and it does not have the scientific quality and depth I’ve seen in many other ICLR submissions. But it does more than just describing useful “tricks”. And all things considered I think this paper deserves a wider audience (but  I'm not convinced that ICLR is the right venue)",0
"This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.

My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. 
The authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  
My suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term ""DeepRL"" seems arbitrary.

On the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. 
It's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.",0
"This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.

My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. 
The authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  
My suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term ""DeepRL"" seems arbitrary.

On the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. 
It's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.",0
"This work proposes to iteratively improve a sentence that has been generated from another MT system (in this case, a phrase-based system). The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word, and predicts the current target word. During testing, the gold words are replaced with the generated words. While this is an interesting area of research, I am not convinced by the proposed approach, and experimental evidence is lacking.

Under the current framework, it is all but impossible for the model to do anything more than a rudimentary word replacement (e.g. it cannot change ""I went to the fridge even though I was not hungry"" to ""Although I was not hungry, I went to the fridge""). The fact that only 0.6 words are edited on average supports this. 

Specific comments:
- It would be interesting to see what the improvements are if the baseline model is a neural system.
- It seems strange (to me at least) that T^i and L(y^{-i|k}) only look at a window of 2k words. It means that when making the decision to change the i-th word, the model does not know what was generated outside of the window? 
- Relatedly, the idea of changing individual words based on local (i.e. word-level) scores seems counterintuitive. Given that we have the full generated sentence, don't we want a global score? Scoring at the sentence-level could also make room for non-greedy search strategies, which could potentially facilitate richer edits.
- How does the approach compare to a model that simply re-ranks the k-best output?
- Instead of editing, did you consider learning an encoder-decoder that takes in x, y_g, and generates y_ref? When decoding you can attend to both x and y_g.

Minor comments:
- Iteratively improving a generated text was also explored in",0
"This work proposes to iteratively improve a sentence that has been generated from another MT system (in this case, a phrase-based system). The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word, and predicts the current target word. During testing, the gold words are replaced with the generated words. While this is an interesting area of research, I am not convinced by the proposed approach, and experimental evidence is lacking.

Under the current framework, it is all but impossible for the model to do anything more than a rudimentary word replacement (e.g. it cannot change ""I went to the fridge even though I was not hungry"" to ""Although I was not hungry, I went to the fridge""). The fact that only 0.6 words are edited on average supports this. 

Specific comments:
- It would be interesting to see what the improvements are if the baseline model is a neural system.
- It seems strange (to me at least) that T^i and L(y^{-i|k}) only look at a window of 2k words. It means that when making the decision to change the i-th word, the model does not know what was generated outside of the window? 
- Relatedly, the idea of changing individual words based on local (i.e. word-level) scores seems counterintuitive. Given that we have the full generated sentence, don't we want a global score? Scoring at the sentence-level could also make room for non-greedy search strategies, which could potentially facilitate richer edits.
- How does the approach compare to a model that simply re-ranks the k-best output?
- Instead of editing, did you consider learning an encoder-decoder that takes in x, y_g, and generates y_ref? When decoding you can attend to both x and y_g.

Minor comments:
- Iteratively improving a generated text was also explored in",0
"This paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations.

Technical issues:

The move from (1) to (2) is problematic. Yes it is a lower bound, but by igoring H(Z), equation (2) ignores the fact that H(Z) will potentially vary more significantly that H(Z|Y). As a result of removing H(Z), the objective (2) encourages Z that are low entropy as the H(Z) term is ignored, doubly so as low entropy Z results in low entropy Z|Y. Yes the -H(X|Z) mitigates against a complete entropy collapse for H(Z), but it still neglects critical terms. In fact one might wonder if this is the reason that semantic noise addition needs to be done anyway, just to push up the entropy of Z to stop it reducing too much.

In (3) arbitrary balancing paramters lamda_1 and lambda_2 are introduced ex-nihilo - they were not there in (2). This is not ever justified.

Then in (5), a further choice is made by simply adding L_{NLL} to the objective. But in the supervised case, the targets are known and so turn up in H(Z|Y). Hence now H(Z|Y) should be conditioned on the targets. However instead another objective is added again without justification, and the conditional entropy of Z is left disconnected from the data it is to be conditioned on. One might argue the C(X,Y,Z) simply acts as a prior on the networks (and hence implicitly on the weights) that we consider, which is then combined with a likelihood term, but this case is not made. In fact there is no explicit probabilistic or information theoretic motivation for the chosen objective.

Given these issues, it is then not too surprising that some further things need to be done, such as semantic noise addition to actually get things working properly. It may be the form of noise addition is a good idea, but given the troublesome objective being used in the first place, it is very hard to draw conclusions.

In summary, substantially better theoretical justification of the chosen model is needed, before any reasonable conclusion on the semantic noise modelling can be made.",0
"This paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations.

Technical issues:

The move from (1) to (2) is problematic. Yes it is a lower bound, but by igoring H(Z), equation (2) ignores the fact that H(Z) will potentially vary more significantly that H(Z|Y). As a result of removing H(Z), the objective (2) encourages Z that are low entropy as the H(Z) term is ignored, doubly so as low entropy Z results in low entropy Z|Y. Yes the -H(X|Z) mitigates against a complete entropy collapse for H(Z), but it still neglects critical terms. In fact one might wonder if this is the reason that semantic noise addition needs to be done anyway, just to push up the entropy of Z to stop it reducing too much.

In (3) arbitrary balancing paramters lamda_1 and lambda_2 are introduced ex-nihilo - they were not there in (2). This is not ever justified.

Then in (5), a further choice is made by simply adding L_{NLL} to the objective. But in the supervised case, the targets are known and so turn up in H(Z|Y). Hence now H(Z|Y) should be conditioned on the targets. However instead another objective is added again without justification, and the conditional entropy of Z is left disconnected from the data it is to be conditioned on. One might argue the C(X,Y,Z) simply acts as a prior on the networks (and hence implicitly on the weights) that we consider, which is then combined with a likelihood term, but this case is not made. In fact there is no explicit probabilistic or information theoretic motivation for the chosen objective.

Given these issues, it is then not too surprising that some further things need to be done, such as semantic noise addition to actually get things working properly. It may be the form of noise addition is a good idea, but given the troublesome objective being used in the first place, it is very hard to draw conclusions.

In summary, substantially better theoretical justification of the chosen model is needed, before any reasonable conclusion on the semantic noise modelling can be made.",0
"This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.

My major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone. Second, there have been several other compression schemes involving pruning, re-training and vector-quantization [e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups. Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited. The advantage of the given method - other then possibly reducing the time it takes to compress the network - is thus unclear. In particular, taking a pre-trained network as a starting point for a quantized model that is subsequently fine-tuned might not take much longer to process then the method given here (but maybe the authors can quantify this?). Finally, much of the speed-up and memory reduction in the VGG-model seems to arise from the three fully-connected layers, in particular the last one. The speed-up in the convolutional layers is comparably small, making me wonder how well the method would work in all-convolutional networks such as the Inception architecture.

[1] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding,",0
"This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.

My major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone. Second, there have been several other compression schemes involving pruning, re-training and vector-quantization [e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups. Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited. The advantage of the given method - other then possibly reducing the time it takes to compress the network - is thus unclear. In particular, taking a pre-trained network as a starting point for a quantized model that is subsequently fine-tuned might not take much longer to process then the method given here (but maybe the authors can quantify this?). Finally, much of the speed-up and memory reduction in the VGG-model seems to arise from the three fully-connected layers, in particular the last one. The speed-up in the convolutional layers is comparably small, making me wonder how well the method would work in all-convolutional networks such as the Inception architecture.

[1] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding,",0
"This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.

- Do the reported decoding times take into account the vocabulary reduction step?
- Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge?
- The proposed methods are helpful because of the difficulties induced by using a word-level model. But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem.",0
"This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.

- Do the reported decoding times take into account the vocabulary reduction step?
- Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge?
- The proposed methods are helpful because of the difficulties induced by using a word-level model. But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem.",0
"The primary point made by this paper is that given certain architectural characteristics of multi-GPU systems, namely the use of bi-directional PCI-E for communication and the integration of two independent DMA engines on recent GPU devices (providing support for simultaneous independent communications), and given the characteristics of the communications patterns required by synchronous SGD trainers for deep neural networks, namely that the messages are large, dense, and have a fixed length, it makes sense to design communication collectives such as broadcast, reduce, and allreduce specifically for the use case of synchronous SGD training on a multi-GPU system.  The paper describes the implementation of these three collectives (broadcast, reduce, and allreduce) using a linear pipelining (LP) scheme on a (logical) ring topology.  The paper compares the LP collectives to two alternatives:  collectives based on a minimal spanning tree (MST) topology and collectives based on bidirectional exchange (BE).  First, a theoretical comparison is made using a standard cost model used in the high performance computing community.  When assumptions based on multi-GPU system architecture (very low latency for messages) and on the communication characteristics of synchronous SGD training (very large messages) are integrated into the model, the paper finds that the LP collectives should be less costly than BE collectives by a factor of 2 and less costly than MST collectives by a factor of log(p), where p is the number of GPUs being used.  Second, an empirical comparison is performed in which (1) the time required to perform each of the different collectives on a 4-device (k40m) system is measured as a function of message size and (2) the time required to perform each of the different collectives with a 200 MB message length is measured as a function of the number of devices in the system.  These measurements show that the LP-based collectives are consistently the fastest.  Third, DNN training experiments with AlexNet and GoogLeNet are performed on a 4-device system using three different synchronous SGD algorithms with the different implementations of the collectives (a total of 6 different algorithms in all).  Measurements of the communication and computation costs show that the LP collectives reduce communication costs without affecting computation costs (as expected).  Measurements of the convergence of the training loss as a function of time for the two DNN architectures show that use of the LP collectives leads to faster training.

While the theory says that the costs of LP collectives should be invariant to the number of devices in a multi-GPU system, the empirical work shows that in practice this does not hold going from 4 to 5 devices (in the tested configuration) because in a 5-device system messages must traverse the QPI.  Are there other practical considerations that the authors are aware of that affect the scaling of the LP collectives?  If so, these should be mentioned in the paper.

In the sentence ""Worringen (2003) proposed a pipeline collective model in shared memory environment for CPU data, but communications of different MPI processes sharing the same CPU memory bus within the same CPU socket."" I really can't figure out what the words after ""but communications of different MPI processes"" are trying to convey.  This sentence is not comprehensible.

""Please note the latency term is log pα, which is the smallest among algorithms in Table.1. Therefore, MST only suits for high frequent short messages.""  The claim that MST collectives are only suitable for high-frequency, short messages does not follow from the statement that MST collectives have the smallest latency term.  You also need to consider the way the cost scales with message size (the bandwidth term).  If the MST collectives had a better bandwidth term than the other collectives, then they would also be superior for large messages.

""Let’s take an appropriate block size b to ensure n/b ≪ α.""  This looks wrong, since n > b.  Should it be b/n ≪ α?

""However, the parameters are not consistent after several iterations due to the precision issues of float multiplications in Gradient Update.""  Are you sure the inconsistency in weight estimates across devices is due to multiplication?  I would expect that it would be due to gradients being accumulated in different orders; that is, because floating point addition is not commutative.

I recommend replacing the term ""sub-gradients"" in this paper with ""partial gradients.""  In the optimization literature, the term ""sub-gradient"" has a very specific meaning that differs from this paper's use of the term (see",0
"The primary point made by this paper is that given certain architectural characteristics of multi-GPU systems, namely the use of bi-directional PCI-E for communication and the integration of two independent DMA engines on recent GPU devices (providing support for simultaneous independent communications), and given the characteristics of the communications patterns required by synchronous SGD trainers for deep neural networks, namely that the messages are large, dense, and have a fixed length, it makes sense to design communication collectives such as broadcast, reduce, and allreduce specifically for the use case of synchronous SGD training on a multi-GPU system.  The paper describes the implementation of these three collectives (broadcast, reduce, and allreduce) using a linear pipelining (LP) scheme on a (logical) ring topology.  The paper compares the LP collectives to two alternatives:  collectives based on a minimal spanning tree (MST) topology and collectives based on bidirectional exchange (BE).  First, a theoretical comparison is made using a standard cost model used in the high performance computing community.  When assumptions based on multi-GPU system architecture (very low latency for messages) and on the communication characteristics of synchronous SGD training (very large messages) are integrated into the model, the paper finds that the LP collectives should be less costly than BE collectives by a factor of 2 and less costly than MST collectives by a factor of log(p), where p is the number of GPUs being used.  Second, an empirical comparison is performed in which (1) the time required to perform each of the different collectives on a 4-device (k40m) system is measured as a function of message size and (2) the time required to perform each of the different collectives with a 200 MB message length is measured as a function of the number of devices in the system.  These measurements show that the LP-based collectives are consistently the fastest.  Third, DNN training experiments with AlexNet and GoogLeNet are performed on a 4-device system using three different synchronous SGD algorithms with the different implementations of the collectives (a total of 6 different algorithms in all).  Measurements of the communication and computation costs show that the LP collectives reduce communication costs without affecting computation costs (as expected).  Measurements of the convergence of the training loss as a function of time for the two DNN architectures show that use of the LP collectives leads to faster training.

While the theory says that the costs of LP collectives should be invariant to the number of devices in a multi-GPU system, the empirical work shows that in practice this does not hold going from 4 to 5 devices (in the tested configuration) because in a 5-device system messages must traverse the QPI.  Are there other practical considerations that the authors are aware of that affect the scaling of the LP collectives?  If so, these should be mentioned in the paper.

In the sentence ""Worringen (2003) proposed a pipeline collective model in shared memory environment for CPU data, but communications of different MPI processes sharing the same CPU memory bus within the same CPU socket."" I really can't figure out what the words after ""but communications of different MPI processes"" are trying to convey.  This sentence is not comprehensible.

""Please note the latency term is log pα, which is the smallest among algorithms in Table.1. Therefore, MST only suits for high frequent short messages.""  The claim that MST collectives are only suitable for high-frequency, short messages does not follow from the statement that MST collectives have the smallest latency term.  You also need to consider the way the cost scales with message size (the bandwidth term).  If the MST collectives had a better bandwidth term than the other collectives, then they would also be superior for large messages.

""Let’s take an appropriate block size b to ensure n/b ≪ α.""  This looks wrong, since n > b.  Should it be b/n ≪ α?

""However, the parameters are not consistent after several iterations due to the precision issues of float multiplications in Gradient Update.""  Are you sure the inconsistency in weight estimates across devices is due to multiplication?  I would expect that it would be due to gradients being accumulated in different orders; that is, because floating point addition is not commutative.

I recommend replacing the term ""sub-gradients"" in this paper with ""partial gradients.""  In the optimization literature, the term ""sub-gradient"" has a very specific meaning that differs from this paper's use of the term (see",0
"This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a PoE.
I find the paper very unclear. I tried to find a proper definition of the joint model p(x,z) but could not extract this from the text. The proposed “EM-like” algorithm should then also follow directly from this definition. At this point I do not see if such as definition even exists. In other words, is there is an objective function on which the iterates of the proposed algorithm are guaranteed to improve on the train data?
We also note that the “product of unifac models” from Hinton tries to do something very similar where only a subset of the experts will get activated to generate the input:",0
"This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a PoE.
I find the paper very unclear. I tried to find a proper definition of the joint model p(x,z) but could not extract this from the text. The proposed “EM-like” algorithm should then also follow directly from this definition. At this point I do not see if such as definition even exists. In other words, is there is an objective function on which the iterates of the proposed algorithm are guaranteed to improve on the train data?
We also note that the “product of unifac models” from Hinton tries to do something very similar where only a subset of the experts will get activated to generate the input:",0
"The authors introduce a new memory model which allows memory access in O(log n) time.

Pros:
* The paper is well written and everything is clear.
* It's a new model and I'm not aware of a similar model.
* It's clear that memory access time is an issue for longer sequences and it is clear how this model solves this problem.

Cons:
* The motivation for O(log n) access time is to be able to use the model on very long sequences. While it is clear from the definition that the computation time is low because of its design, it is not clear that the model will really generalize well to very long sequences.
* The model was also not tested on any real-world task.

I think such experiments should be added to show whether the model really works on long sequences and real-world tasks, otherwise it is not clear if this is a useful model.",0
"The authors introduce a new memory model which allows memory access in O(log n) time.

Pros:
* The paper is well written and everything is clear.
* It's a new model and I'm not aware of a similar model.
* It's clear that memory access time is an issue for longer sequences and it is clear how this model solves this problem.

Cons:
* The motivation for O(log n) access time is to be able to use the model on very long sequences. While it is clear from the definition that the computation time is low because of its design, it is not clear that the model will really generalize well to very long sequences.
* The model was also not tested on any real-world task.

I think such experiments should be added to show whether the model really works on long sequences and real-world tasks, otherwise it is not clear if this is a useful model.",0
"This paper was easy to read, the main idea was presented very clearly.

The main points of the paper (and my concerns are below) can be summarized as follows:
1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn't it only because you are somehow serialize the communication? And it would be maybe much faster if a ""MPI_Reduce"" would be used (even if we wait for the slowest guy)?
2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain.
3.they propose to take gradient from the first ""N"" workers out of ""N+b"" 
workers available. My concern here is that they focused only on the 
workers, but what if the ""parameter server"" will became to slow? What 
if the parameter server would be the bottleneck? How would you address 
this situation? But still if the number of nodes (N) is not large, and 
the deep DNN is used, I can imagine that the communciation will not 
take more than 30% of the run-time.


My largest concern is with the experiments. Different batch size 
implies that different learning rate should be chosen, right? How did 
you tune the learning rates and other parameters for e.g. Figure 5 you 
provide some formulas in (A2) but clearly this can bias your Figures, 
right? meaning, that if you tune ""\gamma, \beta"" for each N, it could 
be somehow more representative? also it would be nicer if you run the 
experiment many times and then report average, best and worst case 
behaviour. because now it can be just coinsidence, right?",0
"This paper was easy to read, the main idea was presented very clearly.

The main points of the paper (and my concerns are below) can be summarized as follows:
1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn't it only because you are somehow serialize the communication? And it would be maybe much faster if a ""MPI_Reduce"" would be used (even if we wait for the slowest guy)?
2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain.
3.they propose to take gradient from the first ""N"" workers out of ""N+b"" 
workers available. My concern here is that they focused only on the 
workers, but what if the ""parameter server"" will became to slow? What 
if the parameter server would be the bottleneck? How would you address 
this situation? But still if the number of nodes (N) is not large, and 
the deep DNN is used, I can imagine that the communciation will not 
take more than 30% of the run-time.


My largest concern is with the experiments. Different batch size 
implies that different learning rate should be chosen, right? How did 
you tune the learning rates and other parameters for e.g. Figure 5 you 
provide some formulas in (A2) but clearly this can bias your Figures, 
right? meaning, that if you tune ""\gamma, \beta"" for each N, it could 
be somehow more representative? also it would be nicer if you run the 
experiment many times and then report average, best and worst case 
behaviour. because now it can be just coinsidence, right?",0
"Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts which are chosen with a hard switch at run-time. This work is applied to the task of predictive different driving behaviors from human drivers, and combines behaviors at test time, often switching behaviors within seconds. Prediction loss is lower than the similar but non-competitive architecture used as a baseline.
It is not very clear how to interpret the results, what is the real impact of the model. If behaviors switch very often, can this really be seen as choosing the best driving mode for a given situation? Maybe the motivation needs to be rephrased a little to be more convincing?
The competitive approach presented is interesting but not really novel, thus the impact of this paper for a conference such as ICLR may be limited.",0
"Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts which are chosen with a hard switch at run-time. This work is applied to the task of predictive different driving behaviors from human drivers, and combines behaviors at test time, often switching behaviors within seconds. Prediction loss is lower than the similar but non-competitive architecture used as a baseline.
It is not very clear how to interpret the results, what is the real impact of the model. If behaviors switch very often, can this really be seen as choosing the best driving mode for a given situation? Maybe the motivation needs to be rephrased a little to be more convincing?
The competitive approach presented is interesting but not really novel, thus the impact of this paper for a conference such as ICLR may be limited.",0
"This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network. This in and of itself is not novel, nor is the idea of optimizing this by adagrad. Though it's weird that the paper explicitly derives the gradient and suggests doing alternating adagrad steps instead of the more standard adagrad steps; it's unclear whether this matters at all for performance. The main trick responsible for increasing the efficiency of this model is the candidate label sampling, which is done in a relatively standard way by sampling labels proportionally to their frequency in the dataset.

Given that neither the model nor the training strategy is novel, it's surprising that the results are better than the state-of-the-art in quality and efficiency (though non-asymptotic efficiency claims are always questionable since implementation effort trades off fairly well against performance). I feel like this paper doesn't quite meet the bar.",0
"This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network. This in and of itself is not novel, nor is the idea of optimizing this by adagrad. Though it's weird that the paper explicitly derives the gradient and suggests doing alternating adagrad steps instead of the more standard adagrad steps; it's unclear whether this matters at all for performance. The main trick responsible for increasing the efficiency of this model is the candidate label sampling, which is done in a relatively standard way by sampling labels proportionally to their frequency in the dataset.

Given that neither the model nor the training strategy is novel, it's surprising that the results are better than the state-of-the-art in quality and efficiency (though non-asymptotic efficiency claims are always questionable since implementation effort trades off fairly well against performance). I feel like this paper doesn't quite meet the bar.",0
"The authors propose to sample from VAEs through a Markov chain [z_t ~ q(z|x=x_{t-1}), x_t ~ p(x|z=z_t)]. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results. The qualitative difference between regular sampling and this Gibbs chain is not very convincing, judging from the figures. It would be a great workshop paper (perhaps more), if the authors fix the notation, fix the discussion to related work, and produce more convincing (perhaps simply upscaled?) figures.

Comments: 
 - Rezende et al's (2014) original VAE paper already discusses the Markov chain, which is ignored in this paper
 - Notation is nonstandard / confusing. At page 1, it’s unclear what the authors mean with “p(x|z) which is approximated as q(x|z)”.
- It’s also not clear what’s meant with q(z). At page 2, q(z) is called the learned distribution, while p(z) can in general also be a learned distribution.
- It’s not true that it’s impossible to draw samples from q(z): one can sample x ~ q(x) from the dataset, then draw z ~ q(z|x).
- It's not explained whether the analysis only applies to continuous observed spaces, or also discrete observed spaces
- Figures 3 and 4 are not very convincing.",0
"The authors propose to sample from VAEs through a Markov chain [z_t ~ q(z|x=x_{t-1}), x_t ~ p(x|z=z_t)]. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results. The qualitative difference between regular sampling and this Gibbs chain is not very convincing, judging from the figures. It would be a great workshop paper (perhaps more), if the authors fix the notation, fix the discussion to related work, and produce more convincing (perhaps simply upscaled?) figures.

Comments: 
 - Rezende et al's (2014) original VAE paper already discusses the Markov chain, which is ignored in this paper
 - Notation is nonstandard / confusing. At page 1, it’s unclear what the authors mean with “p(x|z) which is approximated as q(x|z)”.
- It’s also not clear what’s meant with q(z). At page 2, q(z) is called the learned distribution, while p(z) can in general also be a learned distribution.
- It’s not true that it’s impossible to draw samples from q(z): one can sample x ~ q(x) from the dataset, then draw z ~ q(z|x).
- It's not explained whether the analysis only applies to continuous observed spaces, or also discrete observed spaces
- Figures 3 and 4 are not very convincing.",0
"This paper proposed to use GAN for encrypted communications.

In section 2, the authors proposed a 3 part neural network trained to encode and decode data. This model does not have any practical value except paving the way for describing the next model in section 3: it is strictly worse than any provable cryptography system.

In section 3, the authors designed a task where they want to hide part of the data, which has correlated fields, while publishing the rest. However, I'm having trouble thinking of an application where this system is better than simply decorrelating the data and encrypting the fields one wants to hide with a provable cryptography system while publishing the rest in plain text.",0
"This paper proposed to use GAN for encrypted communications.

In section 2, the authors proposed a 3 part neural network trained to encode and decode data. This model does not have any practical value except paving the way for describing the next model in section 3: it is strictly worse than any provable cryptography system.

In section 3, the authors designed a task where they want to hide part of the data, which has correlated fields, while publishing the rest. However, I'm having trouble thinking of an application where this system is better than simply decorrelating the data and encrypting the fields one wants to hide with a provable cryptography system while publishing the rest in plain text.",0
"This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments:


The usage of P(f_i^1 | f_i^2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (“probability” can be replaced with another word).

I would like to know more about how the method is using the “batch statistics” (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch.

Shouldn’t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1? Have the authors tried any other value?

I think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter).

The STL results are quite impressive, but CIFAR-10 maybe not so much. For CIFAR I’d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation. Have the authors considered this?


All in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try. I think I would’ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR).",0
"This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments:


The usage of P(f_i^1 | f_i^2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (“probability” can be replaced with another word).

I would like to know more about how the method is using the “batch statistics” (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch.

Shouldn’t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1? Have the authors tried any other value?

I think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter).

The STL results are quite impressive, but CIFAR-10 maybe not so much. For CIFAR I’d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation. Have the authors considered this?


All in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try. I think I would’ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR).",0
"This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. In this sense it is similar in spirit to scheduled sampling (Bengio et al) and SEARN (Daume et al) DAgger (Ross et al) which consider a ""roll-in"" mixture of the target and model distributions during training. It was clarified in the pre-review questions that these targets are generated on-line rather than from a lagged distribution, which I think makes the algorithm pseudocode somewhat misleading if I understand it correctly.

This is an incremental improvement on the idea of label softening/smoothing that has recently been revived, and so the novelty is not that high. The author points out that co-label similarity is better preserved by this method but it doesn't follow that this is causal re: regularization; a natural baseline would be a fixed, soft label distribution, as well as one where the softening/temperature of the label distribution is gradually reduced (as one would expect for this method to do as the model gets closer and closer to reproducing the target distribution).

It's an interesting and somewhat appealing idea but the case is not clearly made that this is all that useful. The dropout baselines for MNIST seem quite far from results already in the literature (Srivastava et al 2014 achieves 1.06% with a 3x1024 MLP with dropout and a simple max norm constraint; the dropout baselines here fail to break 1.3% which is rather high by contemporary standards on the permutation-invariant task), and results for CIFAR10 are quite far from the current state of the art, making it difficult to judge the contribution in light of other innovations. The largest benchmark considered is SVHN where the reported accuracies are quite bad indeed; SOTA for single net performance has been less than half the reported error rates for 3-4 years now. It's unclear what conclusions can be drawn about how this would help (or even hurt) in a better-tuned setting.

I have remaining reservations about data hygiene, namely reporting minimum test loss/maximum test accuracy rather than an unbiased method for model selection (minimum validation set error, for example). Relatedly, the regularization potential of early stopping on a validation set is not considered. See, e.g. the protocol in Goodfellow et al (2013).",0
"This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. In this sense it is similar in spirit to scheduled sampling (Bengio et al) and SEARN (Daume et al) DAgger (Ross et al) which consider a ""roll-in"" mixture of the target and model distributions during training. It was clarified in the pre-review questions that these targets are generated on-line rather than from a lagged distribution, which I think makes the algorithm pseudocode somewhat misleading if I understand it correctly.

This is an incremental improvement on the idea of label softening/smoothing that has recently been revived, and so the novelty is not that high. The author points out that co-label similarity is better preserved by this method but it doesn't follow that this is causal re: regularization; a natural baseline would be a fixed, soft label distribution, as well as one where the softening/temperature of the label distribution is gradually reduced (as one would expect for this method to do as the model gets closer and closer to reproducing the target distribution).

It's an interesting and somewhat appealing idea but the case is not clearly made that this is all that useful. The dropout baselines for MNIST seem quite far from results already in the literature (Srivastava et al 2014 achieves 1.06% with a 3x1024 MLP with dropout and a simple max norm constraint; the dropout baselines here fail to break 1.3% which is rather high by contemporary standards on the permutation-invariant task), and results for CIFAR10 are quite far from the current state of the art, making it difficult to judge the contribution in light of other innovations. The largest benchmark considered is SVHN where the reported accuracies are quite bad indeed; SOTA for single net performance has been less than half the reported error rates for 3-4 years now. It's unclear what conclusions can be drawn about how this would help (or even hurt) in a better-tuned setting.

I have remaining reservations about data hygiene, namely reporting minimum test loss/maximum test accuracy rather than an unbiased method for model selection (minimum validation set error, for example). Relatedly, the regularization potential of early stopping on a validation set is not considered. See, e.g. the protocol in Goodfellow et al (2013).",0
"This paper proposes to leverage ""surprisal"" as top-down signal in RNN. More specifically author uses the error corresponding to the previous prediction as an extra input at the current timestep in a LSTM.

The general idea of suprising-driven feedback is interesting for online prediction task. It is a simple enough idea that seems to bring some significant improvements. However, the paper in its current form has some important flaws.

- Overall, the paper writing could be improved. In particular, section 2.4 and 2.5 is composed mostly by the equations of the forward and backward propagation of feedback RNN and feedback LSTM. However, author provides no analysis along with those equations. It is therefore not clear what insight the author tries to express in those sections. In addition, feedback RNN is not evaluated in the experimental section, so it is not clear why feedback RNN is described.

- The experimental evaluation is limited. Only one dataset enwik8 is explored. I think it is necessary to try the idea on different datasets to see if feedback LSTM sees some consistent improvements.
Also, author claims state-of-art on enwik8, but hypernetwork, already cited in the paper, achieves better results (1.34 BPC, table 4 in the hypernetworks paper).

- Author only compares to methods that do not use last prediction error as extra signal. I would argue that a comparison with dynamic evaluation would be more fair. 
 Feedback LSTM uses prediction error as extra input in the forward prop, while dynamic evaluation  backprop it through the network and change the weight accordingly. Also they don't propagate the prediction error in the same way, they both leverage ""extra"" supervised information through the prediction errors.


In summary:
Pros: 
- Interesting idea
- Seems to improve performances

Cons:
- Paper writing
- Weak evaluation (only one dataset)
- Compare only with approaches that does not use the last-timestep error signal",0
"This paper proposes to leverage ""surprisal"" as top-down signal in RNN. More specifically author uses the error corresponding to the previous prediction as an extra input at the current timestep in a LSTM.

The general idea of suprising-driven feedback is interesting for online prediction task. It is a simple enough idea that seems to bring some significant improvements. However, the paper in its current form has some important flaws.

- Overall, the paper writing could be improved. In particular, section 2.4 and 2.5 is composed mostly by the equations of the forward and backward propagation of feedback RNN and feedback LSTM. However, author provides no analysis along with those equations. It is therefore not clear what insight the author tries to express in those sections. In addition, feedback RNN is not evaluated in the experimental section, so it is not clear why feedback RNN is described.

- The experimental evaluation is limited. Only one dataset enwik8 is explored. I think it is necessary to try the idea on different datasets to see if feedback LSTM sees some consistent improvements.
Also, author claims state-of-art on enwik8, but hypernetwork, already cited in the paper, achieves better results (1.34 BPC, table 4 in the hypernetworks paper).

- Author only compares to methods that do not use last prediction error as extra signal. I would argue that a comparison with dynamic evaluation would be more fair. 
 Feedback LSTM uses prediction error as extra input in the forward prop, while dynamic evaluation  backprop it through the network and change the weight accordingly. Also they don't propagate the prediction error in the same way, they both leverage ""extra"" supervised information through the prediction errors.


In summary:
Pros: 
- Interesting idea
- Seems to improve performances

Cons:
- Paper writing
- Weak evaluation (only one dataset)
- Compare only with approaches that does not use the last-timestep error signal",0
