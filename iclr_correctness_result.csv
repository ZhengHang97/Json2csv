SOUNDNESS_CORRECTNESS,comments
2,"This paper improves significantly upon the original NPI work, showing that the model generalizes far better when trained on traces in recursive form. The authors show better sample complexity and generalization results for addition and bubblesort programs, and add two new and more interesting tasks - topological sort and quicksort (added based on reviewer discussion). Furthermore, they actually *prove* that the algorithms learned by the model generalize perfectly, which to my knowledge is the first time this has been done in neural program induction."
2,"This paper improves significantly upon the original NPI work, showing that the model generalizes far better when trained on traces in recursive form. The authors show better sample complexity and generalization results for addition and bubblesort programs, and add two new and more interesting tasks - topological sort and quicksort (added based on reviewer discussion). Furthermore, they actually *prove* that the algorithms learned by the model generalize perfectly, which to my knowledge is the first time this has been done in neural program induction."
4,"This is the most convincing paper on image compression with deep neural networks that I have read so far. The paper is very well written, the use of the rate-distortion theory in the objective fits smoothly in the framework. The paper is compared to a reasonable baseline (JPEG2000, as opposed to previous papers only considering JPEG). I would expect this paper to have a very good impact. 

Yes, please include results on Lena/Barbara/Baboon (sorry, not Gibbons), along with state-of-the-art references with more classical methods such as the one I mentioned in my questions. I think it is important to clearly state how NN compare to best previous methods. From the submitted version, I still don't know how both categories of methods are positioned. "
5,"This is a nice paper that demonstrates an end-to-end trained image compression and decompression system, which achieves better bit-rate vs quality trade-offs than established image compression algorithms (like JPEG-2000). In addition to showing the efficacy of 'deep learning' for a new application, a key contribution of the paper is the introduction of a differentiable version of ""rate"" function, which the authors show can be used for effective training with different rate-distortion trade-offs. I expect this will have impact beyond the compression application itself---for other tasks that might benefit from differentiable approximations to similar functions.

The authors provided a thoughtful response to my pre-review question. I would still argue that to minimize distortion under a fixed range and quantization, a sufficiently complex network would learn automatically produce  codes within a fixed range with the highest-possible entropy (i.e., it would meet the upper bound). But the second argument is convincing---doing so forces a specific ""form"" on how the compressor output is used, which to match the effective compression of the current system, would require a more complex network that is able to carry out the computations currently being done by a separate variable rate encoder used to store q.
"
4,"This is the most convincing paper on image compression with deep neural networks that I have read so far. The paper is very well written, the use of the rate-distortion theory in the objective fits smoothly in the framework. The paper is compared to a reasonable baseline (JPEG2000, as opposed to previous papers only considering JPEG). I would expect this paper to have a very good impact. 

Yes, please include results on Lena/Barbara/Baboon (sorry, not Gibbons), along with state-of-the-art references with more classical methods such as the one I mentioned in my questions. I think it is important to clearly state how NN compare to best previous methods. From the submitted version, I still don't know how both categories of methods are positioned. "
5,"This is a nice paper that demonstrates an end-to-end trained image compression and decompression system, which achieves better bit-rate vs quality trade-offs than established image compression algorithms (like JPEG-2000). In addition to showing the efficacy of 'deep learning' for a new application, a key contribution of the paper is the introduction of a differentiable version of ""rate"" function, which the authors show can be used for effective training with different rate-distortion trade-offs. I expect this will have impact beyond the compression application itself---for other tasks that might benefit from differentiable approximations to similar functions.

The authors provided a thoughtful response to my pre-review question. I would still argue that to minimize distortion under a fixed range and quantization, a sufficiently complex network would learn automatically produce  codes within a fixed range with the highest-possible entropy (i.e., it would meet the upper bound). But the second argument is convincing---doing so forces a specific ""form"" on how the compressor output is used, which to match the effective compression of the current system, would require a more complex network that is able to carry out the computations currently being done by a separate variable rate encoder used to store q.
"
3,"This work presents an LSTM based meta-learning framework to learn the optimization algorithm of a another learning algorithm (here a NN).
The paper is globally well written and the presentation of the main material is clear. The crux of the paper: drawing the parallel between Robbins Monroe update rule and the LSTM update rule and exploit it to satisfy the two main desiderata of few shot learning (1- quick acquisition of new knowledge, 2- slower extraction of general transferable knowledge) is intriguing. 

Several tricks re-used from (Andrychowicz et al. 2016)  such as parameter sharing and normalization, and novel design choices (specific implementation of batch normalization) are well  motivated. 
The experiments are convincing. This is a strong paper. My only concerns/questions are the following:

1. Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough.
2. It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this?
3. The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs:
     - Samy Bengio PhD thesis (1989) is all about this ;-)
     - Use of genetic programming for the search of a new learning rule for neural networks (S. Bengio, Y. Bengio, and J. Cloutier. 1994)
     - I am convince Schmidhuber has done something, make sure you find it and update related work section.  

Overall, I like the paper. I believe the discussed material is relevant to a wide audience at ICLR.  
"
3,"This paper describes a new approach to meta learning by interpreting the SGD update rule as gated recurrent model with trainable parameters. The idea is original and important for research related to transfer learning. The paper has a clear structure, but clarity could be improved at some points.

Pros:

- An interesting and feasible approach to meta-learning
- Competitive results and proper comparison to state-of-the-art
- Good recommendations for practical systems

Cons:

- The analogy would be closer to GRUs than LSTMs
- The description of the data separation in meta sets is hard to follow and could be visualized
- The experimental evaluation is only partly satisfying, especially the effect of the parameters of i_t and f_t would be of interest
- Fig 2 doesn't have much value

Remarks:

- Small typo in 3.2: ""This means each coordinate has it"" -> its

> We plan on releasing the code used in our evaluation experiments.

This would certainly be a major plus."
3,"This work presents an LSTM based meta-learning framework to learn the optimization algorithm of a another learning algorithm (here a NN).
The paper is globally well written and the presentation of the main material is clear. The crux of the paper: drawing the parallel between Robbins Monroe update rule and the LSTM update rule and exploit it to satisfy the two main desiderata of few shot learning (1- quick acquisition of new knowledge, 2- slower extraction of general transferable knowledge) is intriguing. 

Several tricks re-used from (Andrychowicz et al. 2016)  such as parameter sharing and normalization, and novel design choices (specific implementation of batch normalization) are well  motivated. 
The experiments are convincing. This is a strong paper. My only concerns/questions are the following:

1. Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough.
2. It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this?
3. The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs:
     - Samy Bengio PhD thesis (1989) is all about this ;-)
     - Use of genetic programming for the search of a new learning rule for neural networks (S. Bengio, Y. Bengio, and J. Cloutier. 1994)
     - I am convince Schmidhuber has done something, make sure you find it and update related work section.  

Overall, I like the paper. I believe the discussed material is relevant to a wide audience at ICLR.  
"
3,"This paper describes a new approach to meta learning by interpreting the SGD update rule as gated recurrent model with trainable parameters. The idea is original and important for research related to transfer learning. The paper has a clear structure, but clarity could be improved at some points.

Pros:

- An interesting and feasible approach to meta-learning
- Competitive results and proper comparison to state-of-the-art
- Good recommendations for practical systems

Cons:

- The analogy would be closer to GRUs than LSTMs
- The description of the data separation in meta sets is hard to follow and could be visualized
- The experimental evaluation is only partly satisfying, especially the effect of the parameters of i_t and f_t would be of interest
- Fig 2 doesn't have much value

Remarks:

- Small typo in 3.2: ""This means each coordinate has it"" -> its

> We plan on releasing the code used in our evaluation experiments.

This would certainly be a major plus."
4,"SUMMARY 
This paper addresses important questions about the difficulties in training generative adversarial networks. It discusses consequences of using an asymmetric divergence function and sources of instability in training GANs. Then it proposes an alternative using a smoothening approach. 

PROS 
Theory, good questions, nice answers. 
Makes an interesting use of concepts form analysis and differential topology. 
Proposes avenues to avoid instability in GANs. 

CONS 
A bit too long, technical. Some parts and consequences still need to be further developed (which is perfectly fine for future work). 

MINOR COMMENTS

- Section 2.1 Maybe shorten this section a bit. E.g., move all proofs to the appendix. 

- Section 3 provides a nice, intuitive, simple solution. 

- On page 2 second bullet. This also means that P_g is smaller than the data distribution in some other x, which in turn will make the KL divergence non zero. 

- On page 2, ``for not generating plausibly looking pictures'' should be ``for generating not plausibly looking pictures''.  

- Lemma 1 would also hold in more generality. 

- Theorem 2.1 seems to be basic analysis. (In other words, a reference could spare the proof). 

- In Theorem 2.4, it would be good to remind the reader about p(z). 

- Lemma 2 seems to be basic analysis. (In other words, a reference could spare the proof). 
Specify the domain of the random variables. 

- relly - > rely 

- Theorem 2.2 the closed manifolds have boundary or not? (already in the questions)

- Corollary 2.1, ``assumptions of Theorem 1.3''. I could not find Theorem 1.3. 

- Theorem 2.5 ``Therefore'' -> `Then'? 

- Theorem 2.6 ``Is a... '' -> `is a' ? 

- The number of the theorems is confusing. 
"
4,"SUMMARY 
This paper addresses important questions about the difficulties in training generative adversarial networks. It discusses consequences of using an asymmetric divergence function and sources of instability in training GANs. Then it proposes an alternative using a smoothening approach. 

PROS 
Theory, good questions, nice answers. 
Makes an interesting use of concepts form analysis and differential topology. 
Proposes avenues to avoid instability in GANs. 

CONS 
A bit too long, technical. Some parts and consequences still need to be further developed (which is perfectly fine for future work). 

MINOR COMMENTS

- Section 2.1 Maybe shorten this section a bit. E.g., move all proofs to the appendix. 

- Section 3 provides a nice, intuitive, simple solution. 

- On page 2 second bullet. This also means that P_g is smaller than the data distribution in some other x, which in turn will make the KL divergence non zero. 

- On page 2, ``for not generating plausibly looking pictures'' should be ``for generating not plausibly looking pictures''.  

- Lemma 1 would also hold in more generality. 

- Theorem 2.1 seems to be basic analysis. (In other words, a reference could spare the proof). 

- In Theorem 2.4, it would be good to remind the reader about p(z). 

- Lemma 2 seems to be basic analysis. (In other words, a reference could spare the proof). 
Specify the domain of the random variables. 

- relly - > rely 

- Theorem 2.2 the closed manifolds have boundary or not? (already in the questions)

- Corollary 2.1, ``assumptions of Theorem 1.3''. I could not find Theorem 1.3. 

- Theorem 2.5 ``Therefore'' -> `Then'? 

- Theorem 2.6 ``Is a... '' -> `is a' ? 

- The number of the theorems is confusing. 
"
4,"In this paper, a referential game is proposed between two agents. Both agents observe two images. The first agent, called the sender, receive a binary target variable (t) and must send a symbol (message) to the second agent, called the receiver, such that this agent can recover the target. The agents both get a reward, if the receiver agent can predict the target. The paper proposes to parametrize the agents as neural networks - with pretrained representations of the images as feature vectors - and train them using REINFORCE. In this setting, it is shown that the agents converge to  optimal policies and that their learned communications (e.g. the symbolic code transmitted from the sender to the receiver) have some meaningful concepts. In addition to this, the paper presents experiments on a variant of the game grounded on different image classes. In this setting, the agents appear to learn even more meaningful concepts. Finally, multi-game setup is proposed, where the sender agent is alternating between playing the game before and playing a supervised learning task (classifying images). Not surprisingly, when anchored to the supervised learning task, the symbolic communications have even more meaningful concepts.

Learning shared representations for communication in a multi-agent setup is an interesting research direction to explore. This is a much harder task compared to standard supervised learning or single-agent reinforcement learning tasks, which justifies starting with a relatively simple task. To the best of my knowledge, the approach of first learning communication between two agents and then grounding this communication in human language is novel. As the authors remark, this may be an alternative paradigm to standard sequence-to-sequence models which tend to focus on statistical properties of language rather than their functional aspects. I believe the contributions of the proposed task and framework, and the analysis and visualization of what the communicated tokens represent is a useful stepping stone for future work. For this reason, I think the paper should be accepted.



Other comments:
- How is the target (t) incorporated into the sender networks? Please clarify this.
- Table 1 and Table 2 use percentage (%) values differently. In the first, percentages seem to be written in the interval [0, 100], and in the second in the interval [0, 1]. Please correct this. Perhaps related to this, in Table 1, the column ""obs-chance purity"" seems to have extremely small values. I assume this was mistake?
- ""assest"" -> ""assess""
- ""usufal"" -> ""usual"""
4,"In this paper, a referential game is proposed between two agents. Both agents observe two images. The first agent, called the sender, receive a binary target variable (t) and must send a symbol (message) to the second agent, called the receiver, such that this agent can recover the target. The agents both get a reward, if the receiver agent can predict the target. The paper proposes to parametrize the agents as neural networks - with pretrained representations of the images as feature vectors - and train them using REINFORCE. In this setting, it is shown that the agents converge to  optimal policies and that their learned communications (e.g. the symbolic code transmitted from the sender to the receiver) have some meaningful concepts. In addition to this, the paper presents experiments on a variant of the game grounded on different image classes. In this setting, the agents appear to learn even more meaningful concepts. Finally, multi-game setup is proposed, where the sender agent is alternating between playing the game before and playing a supervised learning task (classifying images). Not surprisingly, when anchored to the supervised learning task, the symbolic communications have even more meaningful concepts.

Learning shared representations for communication in a multi-agent setup is an interesting research direction to explore. This is a much harder task compared to standard supervised learning or single-agent reinforcement learning tasks, which justifies starting with a relatively simple task. To the best of my knowledge, the approach of first learning communication between two agents and then grounding this communication in human language is novel. As the authors remark, this may be an alternative paradigm to standard sequence-to-sequence models which tend to focus on statistical properties of language rather than their functional aspects. I believe the contributions of the proposed task and framework, and the analysis and visualization of what the communicated tokens represent is a useful stepping stone for future work. For this reason, I think the paper should be accepted.



Other comments:
- How is the target (t) incorporated into the sender networks? Please clarify this.
- Table 1 and Table 2 use percentage (%) values differently. In the first, percentages seem to be written in the interval [0, 100], and in the second in the interval [0, 1]. Please correct this. Perhaps related to this, in Table 1, the column ""obs-chance purity"" seems to have extremely small values. I assume this was mistake?
- ""assest"" -> ""assess""
- ""usufal"" -> ""usual"""
5,"The paper reports that ""[a]fter the controller RNN is done training, we take the best RNN cell according to the lowest validation perplexity and then run a grid search over learning rate, weight initialization, dropout rates and decay epoch. The best cell found was then run with three different configurations and sizes to increase its capacity.""

Is it possible for you to include (or provide here) the hyperparameters and type of dropout (i.e. recurrent dropout, embedding dropout, ...) used? Without them, replication would at best require a great deal of trial and error. As with ""Recurrent Neural Network Regularization"" (Zaremba et al., 2014), releasing a base set of hyper parameters greatly assists in the future work of the field.

This will likely also be desired for the other experiments, such as character LM."
5,"This paper explores an important part of our field, that of automating architecture search. While the technique is currently computationally intensive, this trade-off will likely become better in the near future as technology continues to improve.

The paper covers both standard vision and text tasks and tackle many benchmark datasets, showing there are gains to be made by exploring beyond the standard RNN and CNN search space. While one would always want to see the technique applied to more datasets, this is already far more sufficient to show the technique is not only competitive with human architectural intuition but may even surpass it. This also suggests an approach to tailor the architecture to specific datasets without resulting in hand engineering at each stage.

This is a well written paper on an interesting topic with strong results. I recommend it be accepted."
5,"This paper proposed to use RL and RNN to design the architecture of networks for specific tasks. The idea of the paper is quite promising and the experimental results on two datasets show that method is solid.
The pros of the paper are:
1. The idea of using RNN to produce the description of the network and using RL to train the RNN is interesting and promising.
2. The generated architecture looks similar to what human designed, which shows that the human expertise and the generated network architectures are compatible.

The cons of the paper are:
1. The training time of the network is long, even with a lot of computing resources. 
2. The experiments did not provide the generality of the generated architectures. It would be nice to see the performances of the generated architecture on other similar but different datasets, especially the generated sequential models.

Overall, I believe this is a nice paper. But it need more experiments to show its potential advantage over the human designed models."
5,"The paper reports that ""[a]fter the controller RNN is done training, we take the best RNN cell according to the lowest validation perplexity and then run a grid search over learning rate, weight initialization, dropout rates and decay epoch. The best cell found was then run with three different configurations and sizes to increase its capacity.""

Is it possible for you to include (or provide here) the hyperparameters and type of dropout (i.e. recurrent dropout, embedding dropout, ...) used? Without them, replication would at best require a great deal of trial and error. As with ""Recurrent Neural Network Regularization"" (Zaremba et al., 2014), releasing a base set of hyper parameters greatly assists in the future work of the field.

This will likely also be desired for the other experiments, such as character LM."
5,"This paper explores an important part of our field, that of automating architecture search. While the technique is currently computationally intensive, this trade-off will likely become better in the near future as technology continues to improve.

The paper covers both standard vision and text tasks and tackle many benchmark datasets, showing there are gains to be made by exploring beyond the standard RNN and CNN search space. While one would always want to see the technique applied to more datasets, this is already far more sufficient to show the technique is not only competitive with human architectural intuition but may even surpass it. This also suggests an approach to tailor the architecture to specific datasets without resulting in hand engineering at each stage.

This is a well written paper on an interesting topic with strong results. I recommend it be accepted."
5,"This paper proposed to use RL and RNN to design the architecture of networks for specific tasks. The idea of the paper is quite promising and the experimental results on two datasets show that method is solid.
The pros of the paper are:
1. The idea of using RNN to produce the description of the network and using RL to train the RNN is interesting and promising.
2. The generated architecture looks similar to what human designed, which shows that the human expertise and the generated network architectures are compatible.

The cons of the paper are:
1. The training time of the network is long, even with a lot of computing resources. 
2. The experiments did not provide the generality of the generated architectures. It would be nice to see the performances of the generated architecture on other similar but different datasets, especially the generated sequential models.

Overall, I believe this is a nice paper. But it need more experiments to show its potential advantage over the human designed models."
4,"This paper presents an on-policy deep RL method with additional auxiliary intrinsic variables. 

- The method is a special case of an universal value function based approach and the authors do cite the correct references. Maybe the biggest claimed technical contribution of this paper is to distill many of the existing ideas to solve 3D navigation problems. I think the contributions should be more clearly stated in the abstract/intro

- I would have liked to see failure modes of this approach. Under what circumstances does the model have problems generalizing to changing goals? There are other conceptual problems -- since this is an on-policy method, there will be catastrophic forgetting if the agent dosen't repeatedly train on goals from the distant past. 

- Since the main contribution of this paper is to integrate several key ideas and show empirical advantage, I would have liked to see results on other domains like Atari (maybe using the ROM as intrinsic variables)

Overall, I think this paper does show clear empirical advantage of using the proposed underlying formulations and experimental insights from this paper might be valuable for future agents"
3,"The paper presents an on-policy method to predict future intrinsic measurements. All the experiments are performed in the game of Doom (vizDoom to be exact), and instead of just predicting win/loss or the number of frags (score), the authors trained their model to predict (a sequence of) triplets of (health, ammunition, frags), weighted by (a sequence of) ""goal"" triplets that they provided as input. Changing the weights of the goal triplet is a way to perform/guide exploration. At test time, one can act by maximizing the long term goal only.

The results are impressive, as this model won the 2016 vizDoom competition. The experimental section of the paper seems sound:
 - There are comparisons of DFP with A3C, DQN, and an attempt to compare with DSR (a recent similar approach from Kulkarni et al., 2016). DFP outperforms other approaches (or equal them when they reach a ceiling / optimum, as for A3C in scenario D1).
 - There is an ablation study that supports the thesis that all the ""added complexity"" of the paper's model is useful.

Predicting intrinsic motivation (Singh et al. 2004), auxiliary variables, and forward modelling, are well-studied domains of reinforcement learning. The version that I read (December 4th revision) adequately references prior work, even if it is not completely exhaustive. 

A few comments (nitpicks) on the form:
 - Doom is described as a 3D environment, whereas it is actually a 2D environment (the height is not a discriminative/actionable dimension) presented in (fake) 3D.
 - The use of ""P"" in (2) (and subsequently) may be misleading as it stands for prediction but not probability (as is normally the case for P).
 - The double use of ""j"" (admittedly, with different fonts) in (6) may be misleading.
 - Results tables could repeat the units of the measurements (in particular as they are heterogenous in Table 1).

I think that this paper is a clear accept. One could argue that experiments could be conducted on different environments or that the novelty is limited, but I feel that ""correct"" (no-nonsense, experimentally sound on Doom, appendix providing details for reproducibility) and ""milestone"" (vizDoom winner) papers should get published."
4,"This paper presents an on-policy deep RL method with additional auxiliary intrinsic variables. 

- The method is a special case of an universal value function based approach and the authors do cite the correct references. Maybe the biggest claimed technical contribution of this paper is to distill many of the existing ideas to solve 3D navigation problems. I think the contributions should be more clearly stated in the abstract/intro

- I would have liked to see failure modes of this approach. Under what circumstances does the model have problems generalizing to changing goals? There are other conceptual problems -- since this is an on-policy method, there will be catastrophic forgetting if the agent dosen't repeatedly train on goals from the distant past. 

- Since the main contribution of this paper is to integrate several key ideas and show empirical advantage, I would have liked to see results on other domains like Atari (maybe using the ROM as intrinsic variables)

Overall, I think this paper does show clear empirical advantage of using the proposed underlying formulations and experimental insights from this paper might be valuable for future agents"
3,"The paper presents an on-policy method to predict future intrinsic measurements. All the experiments are performed in the game of Doom (vizDoom to be exact), and instead of just predicting win/loss or the number of frags (score), the authors trained their model to predict (a sequence of) triplets of (health, ammunition, frags), weighted by (a sequence of) ""goal"" triplets that they provided as input. Changing the weights of the goal triplet is a way to perform/guide exploration. At test time, one can act by maximizing the long term goal only.

The results are impressive, as this model won the 2016 vizDoom competition. The experimental section of the paper seems sound:
 - There are comparisons of DFP with A3C, DQN, and an attempt to compare with DSR (a recent similar approach from Kulkarni et al., 2016). DFP outperforms other approaches (or equal them when they reach a ceiling / optimum, as for A3C in scenario D1).
 - There is an ablation study that supports the thesis that all the ""added complexity"" of the paper's model is useful.

Predicting intrinsic motivation (Singh et al. 2004), auxiliary variables, and forward modelling, are well-studied domains of reinforcement learning. The version that I read (December 4th revision) adequately references prior work, even if it is not completely exhaustive. 

A few comments (nitpicks) on the form:
 - Doom is described as a 3D environment, whereas it is actually a 2D environment (the height is not a discriminative/actionable dimension) presented in (fake) 3D.
 - The use of ""P"" in (2) (and subsequently) may be misleading as it stands for prediction but not probability (as is normally the case for P).
 - The double use of ""j"" (admittedly, with different fonts) in (6) may be misleading.
 - Results tables could repeat the units of the measurements (in particular as they are heterogenous in Table 1).

I think that this paper is a clear accept. One could argue that experiments could be conducted on different environments or that the novelty is limited, but I feel that ""correct"" (no-nonsense, experimentally sound on Doom, appendix providing details for reproducibility) and ""milestone"" (vizDoom winner) papers should get published."
3,"The paper presents an amortised MAP estimation method for SR problems. By learning a neural network which learns to project to an affine subspace of SR solutions which are consistent with the LR method the method enables finding propoer solutions with by using a variety of methods: GANs, noise assisted and density assisted optimisation.
Results are nicely demonstrated on several datasets.

I like the paper all in all, though I feel the writing can be polished by quite a bit and presentation should be made clearer. It was hard to follow at times and considering the subject matter is quite complicated making it clearer would help. Also, I would love to see some more analysis of the resulting the networks - what kind of features to they learn? "
3,"The paper presents an amortised MAP estimation method for SR problems. By learning a neural network which learns to project to an affine subspace of SR solutions which are consistent with the LR method the method enables finding propoer solutions with by using a variety of methods: GANs, noise assisted and density assisted optimisation.
Results are nicely demonstrated on several datasets.

I like the paper all in all, though I feel the writing can be polished by quite a bit and presentation should be made clearer. It was hard to follow at times and considering the subject matter is quite complicated making it clearer would help. Also, I would love to see some more analysis of the resulting the networks - what kind of features to they learn? "
3,"This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems to be the first implementation of a differentiable memory as graph: it is much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks, but it is still very preliminary work, and the representation of memory as a graph seems much more powerful than a stack. Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved later version. This original, technically accurate (within what I understood) and thought provoking paper is worth publishing.

The preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches. The performance on the bAbI task is comparable to the best memory networks, but still worse than more traditional rule induction (see "
3,"This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems to be the first implementation of a differentiable memory as graph: it is much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks, but it is still very preliminary work, and the representation of memory as a graph seems much more powerful than a stack. Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved later version. This original, technically accurate (within what I understood) and thought provoking paper is worth publishing.

The preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches. The performance on the bAbI task is comparable to the best memory networks, but still worse than more traditional rule induction (see "
5,"This paper proposes to investigate attention transfers between a teacher and a student network. 

Attention transfer is performed by minimising the l2 distance between the teacher/student attention maps at different layers, in addition to minimising the classification loss and optionally a knowledge distillation term.
Authors define several activation based attentions (sum of absolute feature values raise at the power p or max of values raised at the power p). They also propose a gradient based attention (derivative of the Loss w.r.t. inputs). 

They evaluate their approaches on several datasets (CIFAR, Cub/Scene, Imagenet) showing that attention transfers  does help improving the student network test performance.  However, the student networks performs worst than the teacher, even with attention.

Few remarks/questions:
- in section 3 authors  claim that networks with higher accuracy have a higher spatial correlation between the object and the attention map. While Figure 4 is compelling, it would be nice to have quantitative results showing that as well.
- how did you choose the hyperparameter values, it would be nice to see what is the impact of $\beta$.
- it would be nice to report teacher train and validation loss in Figure 7 b)
- from the experiments, it is not clear what at the pros/cons of the different attention maps
- AT does not lead to better result than the teacher. However, the student networks have less parameters. It would be interesting to characterise the corresponding speed-up. If you keep the same architecture between the student and the teacher, is there any benefit to the attention transfer?

In summary:
Pros:
- Clearly written and well motivated.
- Consistent improvement of the student with attention compared to the student alone.
Cons:
- Students have worst performances than the teacher models.
- It is not clear which attention to use in which case?
- Somewhat incremental novelty relatively to Fitnet
"
5,"This paper proposes to investigate attention transfers between a teacher and a student network. 

Attention transfer is performed by minimising the l2 distance between the teacher/student attention maps at different layers, in addition to minimising the classification loss and optionally a knowledge distillation term.
Authors define several activation based attentions (sum of absolute feature values raise at the power p or max of values raised at the power p). They also propose a gradient based attention (derivative of the Loss w.r.t. inputs). 

They evaluate their approaches on several datasets (CIFAR, Cub/Scene, Imagenet) showing that attention transfers  does help improving the student network test performance.  However, the student networks performs worst than the teacher, even with attention.

Few remarks/questions:
- in section 3 authors  claim that networks with higher accuracy have a higher spatial correlation between the object and the attention map. While Figure 4 is compelling, it would be nice to have quantitative results showing that as well.
- how did you choose the hyperparameter values, it would be nice to see what is the impact of $\beta$.
- it would be nice to report teacher train and validation loss in Figure 7 b)
- from the experiments, it is not clear what at the pros/cons of the different attention maps
- AT does not lead to better result than the teacher. However, the student networks have less parameters. It would be interesting to characterise the corresponding speed-up. If you keep the same architecture between the student and the teacher, is there any benefit to the attention transfer?

In summary:
Pros:
- Clearly written and well motivated.
- Consistent improvement of the student with attention compared to the student alone.
Cons:
- Students have worst performances than the teacher models.
- It is not clear which attention to use in which case?
- Somewhat incremental novelty relatively to Fitnet
"
5,"1) Summary

This paper proposes to tackle visual servoing (specifically target following) using spatial feature maps from convolutional networks pre-trained on general image classification tasks. The authors combine bilinear models of one-step dynamics of visual feature maps at multiple scales with a reinforcement learning algorithm to learn a servoing policy. This policy is learned by minimizing a regularized weighted average of distances to features predicted by the aforementioned model of visual dynamics.

2) Contributions

+ Controlled experiments in simulation quantifying the usefulness of pre-trained deep features for visual servoing.
+ Clear performance benefits with respect to many sensible baselines, including ones using ground truth bounding boxes.
+ Principled learning of multi-scale visual feature weights with an efficient trust-region fitted Q-iteration algorithm to handle the problem of distractors.
+ Good sample efficiency thanks to the choice of Q-function approximator and the model-based one-step visual feature dynamics.
+ Open source virtual city environment to benchmark visual servoing.

3) Suggestions for improvement

- More complex benchmark:
Although the environment is not just a toy synthetic one, the experiments would benefit greatly from more complex visual conditions (clutter, distractors, appearance and motion variety, environment richness and diversity, etc). At least, the realism and diversity of object appearances could be vastly improved by using a larger number of 3D car models, including more realistic and diverse ones that can be obtained from Google SketchUp for instance, and populating the environment with more distractor cars (in traffic or parked). This is important as the main desired quality of the approach is robustness to visual variations.

- End-to-end and representation learning:
Although the improvements are already significant in the current synthetic experiments, it would be interesting to measure the impact of end-to-end training (i.e. also fine-tuning the convnet), as it is possibly needed for better generalization in more challenging visual conditions. It would also allow to measure the benefit of deep representation learning for visual servoing, which would be relevant to ICLR (there is no representation learning so far, although the method can be straightforwardly adapted as the authors mention briefly).

- Reproducibility:
The formalism and algorithms are clearly explained, but there is a slightly overwhelming mass of practical tricks and implementation details described with varying levels of details throughout the paper and appendix. Grouping, simplifying, or reorganizing the exposition of these implementation details would help, but a better way would probably consist in only summarizing the most important ones in section and link to an open source implementation of the method for completeness.

- Typos:
p.2: ""learning is a relative[ly] recent addition""
p.2: ""be applied [to] directly learn""

4) Conclusion

In spite of the aforementioned limits of the experiments, this paper is interesting and solid, in part thanks to the excellent reply to the pre-review questions and the subsequent improved revision. This leads me to believe the authors are more than capable of following to a significant extent the aforementioned suggestions for improvement, thus leading to an even better paper."
5,"1) Summary

This paper proposes to tackle visual servoing (specifically target following) using spatial feature maps from convolutional networks pre-trained on general image classification tasks. The authors combine bilinear models of one-step dynamics of visual feature maps at multiple scales with a reinforcement learning algorithm to learn a servoing policy. This policy is learned by minimizing a regularized weighted average of distances to features predicted by the aforementioned model of visual dynamics.

2) Contributions

+ Controlled experiments in simulation quantifying the usefulness of pre-trained deep features for visual servoing.
+ Clear performance benefits with respect to many sensible baselines, including ones using ground truth bounding boxes.
+ Principled learning of multi-scale visual feature weights with an efficient trust-region fitted Q-iteration algorithm to handle the problem of distractors.
+ Good sample efficiency thanks to the choice of Q-function approximator and the model-based one-step visual feature dynamics.
+ Open source virtual city environment to benchmark visual servoing.

3) Suggestions for improvement

- More complex benchmark:
Although the environment is not just a toy synthetic one, the experiments would benefit greatly from more complex visual conditions (clutter, distractors, appearance and motion variety, environment richness and diversity, etc). At least, the realism and diversity of object appearances could be vastly improved by using a larger number of 3D car models, including more realistic and diverse ones that can be obtained from Google SketchUp for instance, and populating the environment with more distractor cars (in traffic or parked). This is important as the main desired quality of the approach is robustness to visual variations.

- End-to-end and representation learning:
Although the improvements are already significant in the current synthetic experiments, it would be interesting to measure the impact of end-to-end training (i.e. also fine-tuning the convnet), as it is possibly needed for better generalization in more challenging visual conditions. It would also allow to measure the benefit of deep representation learning for visual servoing, which would be relevant to ICLR (there is no representation learning so far, although the method can be straightforwardly adapted as the authors mention briefly).

- Reproducibility:
The formalism and algorithms are clearly explained, but there is a slightly overwhelming mass of practical tricks and implementation details described with varying levels of details throughout the paper and appendix. Grouping, simplifying, or reorganizing the exposition of these implementation details would help, but a better way would probably consist in only summarizing the most important ones in section and link to an open source implementation of the method for completeness.

- Typos:
p.2: ""learning is a relative[ly] recent addition""
p.2: ""be applied [to] directly learn""

4) Conclusion

In spite of the aforementioned limits of the experiments, this paper is interesting and solid, in part thanks to the excellent reply to the pre-review questions and the subsequent improved revision. This leads me to believe the authors are more than capable of following to a significant extent the aforementioned suggestions for improvement, thus leading to an even better paper."
5,"This paper proposes a nonparametric neural network model, which automatically learns the size of the model during the training process. The key idea is to randomly add zero units and use sparse regularizer to automatically null out the weights that are irrelevant. The idea sounds to be a random search approach over discrete space with the help of sparse regularization to eliminate useless units. This is an important problem and the paper gives interesting results. My main comments are listed below:

What is the additional computation complexity of the algorithm? The decomposition of each fan-in weights into a parallel component and an orthogonal component and the transformation into radial-angular coordinates may require a lot of extra computation time. The authors may need to discuss the extra amount of operations relative to the parametric neural network. Furthermore, it would be useful to show some running time experiments.

It is observed that nonparametric networks return small networks on the convex dataset so that it is inferior to parametric networks. Any insight on this?"
5,"This paper proposes a nonparametric neural network model, which automatically learns the size of the model during the training process. The key idea is to randomly add zero units and use sparse regularizer to automatically null out the weights that are irrelevant. The idea sounds to be a random search approach over discrete space with the help of sparse regularization to eliminate useless units. This is an important problem and the paper gives interesting results. My main comments are listed below:

What is the additional computation complexity of the algorithm? The decomposition of each fan-in weights into a parallel component and an orthogonal component and the transformation into radial-angular coordinates may require a lot of extra computation time. The authors may need to discuss the extra amount of operations relative to the parametric neural network. Furthermore, it would be useful to show some running time experiments.

It is observed that nonparametric networks return small networks on the convex dataset so that it is inferior to parametric networks. Any insight on this?"
5,"This paper proposes a simple method for pruning filters in two types of architecture to decrease the time for execution.

Pros:
- Impressively retains accuracy on popular models on ImageNet and Cifar10

Cons:
- There is no justification for for low L1 or L2 norm being a good selection criteria. There are two easy critical missing baselines of 1) randomly pruning filters, 2) pruning filters with low activation pattern norms on training set.
- There is no direct comparison to the multitude of other pruning and speedup methods.
- While FLOPs are reported, it is not clear what empirical speedup this method gives, which is what people interested in these methods care about. Wall-clock speedup is trivial to report, so the lack of wall-clock speedup is suspect.

"
5,"This paper proposes a simple method for pruning filters in two types of architecture to decrease the time for execution.

Pros:
- Impressively retains accuracy on popular models on ImageNet and Cifar10

Cons:
- There is no justification for for low L1 or L2 norm being a good selection criteria. There are two easy critical missing baselines of 1) randomly pruning filters, 2) pruning filters with low activation pattern norms on training set.
- There is no direct comparison to the multitude of other pruning and speedup methods.
- While FLOPs are reported, it is not clear what empirical speedup this method gives, which is what people interested in these methods care about. Wall-clock speedup is trivial to report, so the lack of wall-clock speedup is suspect.

"
5,"This paper trains a generative model which transforms noise into model samples by a gradual denoising process. It is similar to a generative model based on diffusion. Unlike the diffusion approach:
- It uses only a small number of denoising steps, and is thus far more computationally efficient.
- Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) | x), and then runs in the same direction as the generative model. This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data. (This also seems somewhat related to ladder networks.)
- There is no tractable variational bound on the log likelihood.

I liked the idea, and found the visual sample quality given a short chain impressive. The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks. It would be much more convincing to have a log likelihood comparison that doesn't depend on Parzen likelihoods.

Detailed comments follow:

Sec. 2:
""theta(0) the"" -> ""theta(0) be the""
""theta(t) the"" -> ""theta(t) be the""
""what we will be using"" -> ""which we will be doing""
I like that you infer q(z^0|x), and then run inference in the same order as the generative chain. This reminds me slightly of ladder networks.
""q*. Having learned"" -> ""q*. [paragraph break] Having learned""
Sec 3.3:
""learn to inverse"" -> ""learn to reverse""
Sec. 4:
""For each experiments"" -> ""For each experiment""
How sensitive are your results to infusion rate?
Sec. 5: ""appears to provide more accurate models"" I don't think you showed this -- there's no direct comparison to the Sohl-Dickstein paper.
Fig 4. -- neat!
"
5,"This paper trains a generative model which transforms noise into model samples by a gradual denoising process. It is similar to a generative model based on diffusion. Unlike the diffusion approach:
- It uses only a small number of denoising steps, and is thus far more computationally efficient.
- Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) | x), and then runs in the same direction as the generative model. This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data. (This also seems somewhat related to ladder networks.)
- There is no tractable variational bound on the log likelihood.

I liked the idea, and found the visual sample quality given a short chain impressive. The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks. It would be much more convincing to have a log likelihood comparison that doesn't depend on Parzen likelihoods.

Detailed comments follow:

Sec. 2:
""theta(0) the"" -> ""theta(0) be the""
""theta(t) the"" -> ""theta(t) be the""
""what we will be using"" -> ""which we will be doing""
I like that you infer q(z^0|x), and then run inference in the same order as the generative chain. This reminds me slightly of ladder networks.
""q*. Having learned"" -> ""q*. [paragraph break] Having learned""
Sec 3.3:
""learn to inverse"" -> ""learn to reverse""
Sec. 4:
""For each experiments"" -> ""For each experiment""
How sensitive are your results to infusion rate?
Sec. 5: ""appears to provide more accurate models"" I don't think you showed this -- there's no direct comparison to the Sohl-Dickstein paper.
Fig 4. -- neat!
"
4,"This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models.  A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images.  The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models.

This work is an important step in understanding the nonlinear response properties of visual neurons.  Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models.  So this presents an important challenge to the field.  If we even had *a* model that works, it would be a starting point.  So this work should be seen in that light.  The challenge now of course is to tease apart what the rnn is doing.  Perhaps it could now be pruned and simplified to see what parts are critical to performance.  It would have been nice to see such an analysis.   Nevertheless this result is a good first start and I think important for people to know about.

I am a bit confused about what is being called a ""movie.""  My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each.  But then it is stated that the ""frame rate"" is 1/8.33 ms.  I think this must refer to the refresh rate of the monitor, right?   

I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies.  Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.
"
4,"This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models.  A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images.  The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models.

This work is an important step in understanding the nonlinear response properties of visual neurons.  Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models.  So this presents an important challenge to the field.  If we even had *a* model that works, it would be a starting point.  So this work should be seen in that light.  The challenge now of course is to tease apart what the rnn is doing.  Perhaps it could now be pruned and simplified to see what parts are critical to performance.  It would have been nice to see such an analysis.   Nevertheless this result is a good first start and I think important for people to know about.

I am a bit confused about what is being called a ""movie.""  My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each.  But then it is stated that the ""frame rate"" is 1/8.33 ms.  I think this must refer to the refresh rate of the monitor, right?   

I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies.  Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.
"
3,"This paper is about using denoising autoencoders to improve performance in GANs. In particular, the features as determined by the discriminator, of images generated by the generator, are fed into a denoising AE and we try to have these be reconstructed well. I think it's an interesting idea to use this ""extra information"" -- namely the feature representations learned by the discriminator. It seems very much in the spirit of ICLR! My main concern, though, is that I'm not wholly convinced on the nature of the improvement. This method achieves higher inception scores than other methods in some cases, but I have a hard time interpreting these scores and thus a hard time getting excited by the results. In particular, the authors have not convinced me that the benefits outweigh the required additional sophistication both conceptually and implementation-wise (speaking of which, will code be released?). One thing I'd be curious to know is, how hard is it to get this thing to actually work? 

Also, I view GANs as a means to an end -- while I'm not particularly excited about generating realistic images (especially in 32x32), I'm very excited about the future potential of GAN-based systems. So it would have been nice to see these improvements in inception score translate into improvements in a more useful task. But this criticism could probably apply to many GAN papers and so perhaps isn't fair here. 

I do think the idea of exploiting ""extra information"" (like discriminator features) is interesting both inside and outside the context of this paper. "
5,"The authors present a way to complement the Gerative Adversarial Network traning procedure with an additional term based on denoising autoencoders. The use of denoising autoencoders is motivated by the observation that they implicitly capture the distribution of the data they were trained on. While sampling methods based denoising autoencoders alone don't amount to very interesting generative models (at least no-one could demonstrate otherwise), this paper shows that it can be combined successfully with generative adversarial networks.

My overall assessment of this paper is that it is well written, well reasoned, and presents a good idea motivated from first principles. I feel that the idea presented here is not revolutionary or a very radical departure from what has been done before, I would have liked a slightly more structured experiments section which focusses on and provides insights into the relative merits of different choices one could make (see pre-review questions for details), rather than focussing just on demonstrating that a chosen variant works.

In addition to this general review, I have already posted specific questions and criticism in the pre-review questions - thanks for the authors' responses. Based on those responses the area I am most uncomfortable about is whether the (Alain & Bengio, 2014) intuition about the denoising autoencoders is valid if it all happens in a nonlinear featurespace. If the denoiser function's behaviour ends up depending on the Jacobian of the nonlinear transformation Phi, another question is whether this dependence is exploitable by the optimization scheme."
3,"This paper is about using denoising autoencoders to improve performance in GANs. In particular, the features as determined by the discriminator, of images generated by the generator, are fed into a denoising AE and we try to have these be reconstructed well. I think it's an interesting idea to use this ""extra information"" -- namely the feature representations learned by the discriminator. It seems very much in the spirit of ICLR! My main concern, though, is that I'm not wholly convinced on the nature of the improvement. This method achieves higher inception scores than other methods in some cases, but I have a hard time interpreting these scores and thus a hard time getting excited by the results. In particular, the authors have not convinced me that the benefits outweigh the required additional sophistication both conceptually and implementation-wise (speaking of which, will code be released?). One thing I'd be curious to know is, how hard is it to get this thing to actually work? 

Also, I view GANs as a means to an end -- while I'm not particularly excited about generating realistic images (especially in 32x32), I'm very excited about the future potential of GAN-based systems. So it would have been nice to see these improvements in inception score translate into improvements in a more useful task. But this criticism could probably apply to many GAN papers and so perhaps isn't fair here. 

I do think the idea of exploiting ""extra information"" (like discriminator features) is interesting both inside and outside the context of this paper. "
5,"The authors present a way to complement the Gerative Adversarial Network traning procedure with an additional term based on denoising autoencoders. The use of denoising autoencoders is motivated by the observation that they implicitly capture the distribution of the data they were trained on. While sampling methods based denoising autoencoders alone don't amount to very interesting generative models (at least no-one could demonstrate otherwise), this paper shows that it can be combined successfully with generative adversarial networks.

My overall assessment of this paper is that it is well written, well reasoned, and presents a good idea motivated from first principles. I feel that the idea presented here is not revolutionary or a very radical departure from what has been done before, I would have liked a slightly more structured experiments section which focusses on and provides insights into the relative merits of different choices one could make (see pre-review questions for details), rather than focussing just on demonstrating that a chosen variant works.

In addition to this general review, I have already posted specific questions and criticism in the pre-review questions - thanks for the authors' responses. Based on those responses the area I am most uncomfortable about is whether the (Alain & Bengio, 2014) intuition about the denoising autoencoders is valid if it all happens in a nonlinear featurespace. If the denoiser function's behaviour ends up depending on the Jacobian of the nonlinear transformation Phi, another question is whether this dependence is exploitable by the optimization scheme."
4,"This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out. While none of the individual components proposed in this paper are new, I believe that the combination in this fashion is. Further, I appreciated the detailed analysis of model behaviour in section 3.

The main downside to this submission is in its relative weakness on the empirical front. Arguably there are more interesting tasks than sentiment analysis and k-way classification! Likewise, why waste 2/3 of a page on t-sne projections rather than use that space for further analysis?

While I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, I think this paper should be accepted: it's an interesting algorithm, nicely composed and very efficient, so it's reasonable to assume that other readers might have use for some of the ideas presented here."
5,"This paper proposes learning document embeddings as a sum of the constituent word embeddings, which are jointly learned and randomly dropped out ('corrupted') during training. While none of the pieces of this model are particularly novel, the result is an efficient learning algorithm for document representation with good empirical performance.

Joint training of word and document embeddings is not a new idea, nor is the idea of enforcing the document to be represented by the sum of its word embeddings (see, e.g. 'The Sum of Its Parts: Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert). Furthermore, the corruption mechanism is nothing other than traditional dropout on the input layer. Coupled with the word2vec-style loss and training methods, this paper offers little on the novelty front.

On the other hand, it is very efficient at generation time, requiring only an average of the word embeddings rather than a complicated inference step as in Doc2Vec. Moreover, by construction, the embedding captures salient global information about the document -- it captures specifically that information that aids in local-context prediction. For such a simple model, the performance on sentiment analysis and document classification is quite encouraging.

Overall, despite the lack of novelty, the simplicity, efficiency, and performance of this model make it worthy of wider readership and study, and I recommend acceptance."
4,"This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out. While none of the individual components proposed in this paper are new, I believe that the combination in this fashion is. Further, I appreciated the detailed analysis of model behaviour in section 3.

The main downside to this submission is in its relative weakness on the empirical front. Arguably there are more interesting tasks than sentiment analysis and k-way classification! Likewise, why waste 2/3 of a page on t-sne projections rather than use that space for further analysis?

While I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, I think this paper should be accepted: it's an interesting algorithm, nicely composed and very efficient, so it's reasonable to assume that other readers might have use for some of the ideas presented here."
5,"This paper proposes learning document embeddings as a sum of the constituent word embeddings, which are jointly learned and randomly dropped out ('corrupted') during training. While none of the pieces of this model are particularly novel, the result is an efficient learning algorithm for document representation with good empirical performance.

Joint training of word and document embeddings is not a new idea, nor is the idea of enforcing the document to be represented by the sum of its word embeddings (see, e.g. 'The Sum of Its Parts: Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert). Furthermore, the corruption mechanism is nothing other than traditional dropout on the input layer. Coupled with the word2vec-style loss and training methods, this paper offers little on the novelty front.

On the other hand, it is very efficient at generation time, requiring only an average of the word embeddings rather than a complicated inference step as in Doc2Vec. Moreover, by construction, the embedding captures salient global information about the document -- it captures specifically that information that aids in local-context prediction. For such a simple model, the performance on sentiment analysis and document classification is quite encouraging.

Overall, despite the lack of novelty, the simplicity, efficiency, and performance of this model make it worthy of wider readership and study, and I recommend acceptance."
3,"
This paper explores transfer in reinforcement learning between agents that may be morphologically distinct. The key idea is for the source and target agent to have learned a shared skill, and then to use this to construct abstract feature spaces to enable the transfer of a new unshared skill in the source agent to the target agent. The paper is related to much other work on transfer that uses shared latent spaces, such as CCA and its variants, including manifold alignment and kernel CCA. 


The paper reports on experiments using a simple physics simulator between robot arms consisting of three vs. four links. For comparison, a simple CCA based approach is shown, although it would have been preferable to see comparisons for something more current and up to date, such as manifold alignment or kernel CCA. A three layer neural net is used to construct the latent feature spaces. 

The problem of transfer in RL is extremely important, and receives less attention than it should. This work uses an interesting hypothesis of trying to construct transfer based on shared skills between source and target agent. This is a promising approach. However, the comparisons to related approaches is not very up to date, and the domains are fairly simplistic. There is little by way of theoretical development of the ideas using MDP theory. 
"
3,"The paper considers the problem of transferring skills between robots with different morphologies, in the context of agents that have to perform several tasks.  A core component of the proposed approach is to use a task-invariant future space, which can be shared between tasks & between agents.

Compared to previous work (Ammar et al. 2015), it seems the main contribution here is to assume that good correspondences in episodic tasks can be extracted through time alignment (Sec. 2).  This is an interesting hypothesis. There is also similarity to work by Raimalwala et al (2016), but the authors argue their method is better equipped to handle non-linear dynamics. These are two interesting hypotheses, however I dont see that they have been verified in the presented empirical results.  In particular, the question of the pairing correspondence seems crucial. What happens when the time alignment is not suitable. Is it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.

In general, more comparison to other transfer methods, including those listed in Sec.2, would be very valuable.  The addition of Sec.5.1 is definitely a right step in this direction, but represents a small portion of the recent work on transfer learning.  I appreciate that other methods transfer other pieces of information (e.g. the policy), but still if the end goal is better performance, what is worth transferring (in addition to how to do the transfer) should be a reasonable question to explore.

Overall, the paper tackles an important problem, but this is a very active area of research, and further comparison to other methods would be worthwhile.  The method proposed of transferring the representation is well motivated, cleanly described, and conceptually sound.  The assumption that time alignment can be used for the state pairing seems problematic, and should be further validated."
3,"
This paper explores transfer in reinforcement learning between agents that may be morphologically distinct. The key idea is for the source and target agent to have learned a shared skill, and then to use this to construct abstract feature spaces to enable the transfer of a new unshared skill in the source agent to the target agent. The paper is related to much other work on transfer that uses shared latent spaces, such as CCA and its variants, including manifold alignment and kernel CCA. 


The paper reports on experiments using a simple physics simulator between robot arms consisting of three vs. four links. For comparison, a simple CCA based approach is shown, although it would have been preferable to see comparisons for something more current and up to date, such as manifold alignment or kernel CCA. A three layer neural net is used to construct the latent feature spaces. 

The problem of transfer in RL is extremely important, and receives less attention than it should. This work uses an interesting hypothesis of trying to construct transfer based on shared skills between source and target agent. This is a promising approach. However, the comparisons to related approaches is not very up to date, and the domains are fairly simplistic. There is little by way of theoretical development of the ideas using MDP theory. 
"
3,"The paper considers the problem of transferring skills between robots with different morphologies, in the context of agents that have to perform several tasks.  A core component of the proposed approach is to use a task-invariant future space, which can be shared between tasks & between agents.

Compared to previous work (Ammar et al. 2015), it seems the main contribution here is to assume that good correspondences in episodic tasks can be extracted through time alignment (Sec. 2).  This is an interesting hypothesis. There is also similarity to work by Raimalwala et al (2016), but the authors argue their method is better equipped to handle non-linear dynamics. These are two interesting hypotheses, however I dont see that they have been verified in the presented empirical results.  In particular, the question of the pairing correspondence seems crucial. What happens when the time alignment is not suitable. Is it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.

In general, more comparison to other transfer methods, including those listed in Sec.2, would be very valuable.  The addition of Sec.5.1 is definitely a right step in this direction, but represents a small portion of the recent work on transfer learning.  I appreciate that other methods transfer other pieces of information (e.g. the policy), but still if the end goal is better performance, what is worth transferring (in addition to how to do the transfer) should be a reasonable question to explore.

Overall, the paper tackles an important problem, but this is a very active area of research, and further comparison to other methods would be worthwhile.  The method proposed of transferring the representation is well motivated, cleanly described, and conceptually sound.  The assumption that time alignment can be used for the state pairing seems problematic, and should be further validated."
5,"This work proposed a simple but strong baseline for parametric texture synthesis. In empirical experiments, samples generated by the baseline composed by multi-scale and random filters sometime rival the VGG-based model which has multi-layer and pre-trained filters. The authors concluded that texture synthesis does not necessarily depend on deep hierarchical representations or the learned feature maps. 
This work is indeed interesting and insightful. However, the conclusions are needed to be further testified (especially for deep hierarchical representations). Firstly, all of generated samples by both VGG and single layer model are not perfect and much worse than the results from non-parametric methods.  Besides VGG-based model seems to do better in inpainting task in Figure 7. Last but not least, would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently? "
3,"The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.

Figure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.

The main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.

The authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.

Overall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.



"
5,"This work proposed a simple but strong baseline for parametric texture synthesis. In empirical experiments, samples generated by the baseline composed by multi-scale and random filters sometime rival the VGG-based model which has multi-layer and pre-trained filters. The authors concluded that texture synthesis does not necessarily depend on deep hierarchical representations or the learned feature maps. 
This work is indeed interesting and insightful. However, the conclusions are needed to be further testified (especially for deep hierarchical representations). Firstly, all of generated samples by both VGG and single layer model are not perfect and much worse than the results from non-parametric methods.  Besides VGG-based model seems to do better in inpainting task in Figure 7. Last but not least, would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently? "
3,"The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.

Figure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.

The main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.

The authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.

Overall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.



"
5,"This paper presents a succinct argument that the principle of optimizing receptive field location and size in a simulated eye that can make saccades with respect to a classification error of images of data whose labels depend on variable-size and variable-location subimages, explains the existence of a foveal area in e.g. the primate retina.

The argument could be improved by using more-realistic image data and drawing more direct correspondence with the number, receptive field sizes and eccentricities of retinal cells in e.g. the macaque, but the authors would then face the challenge of identifying a loss function that is both biologically plausible and supportive of their claim.

The argument could also be improved by commenting on the timescales involved. Presumably the density of the foveal center depends on the number of of saccades allowed by the inference process, as well as the size of the target sub-images, and also has an impact on the overall classification accuracy.

Why does the classification error rate of dataset 2 remain stubbornly at 24%? This seems so high that the model may not be working the way wed like it to. It seems that the overall argument of the paper pre-supposes that the model can be trained to be a good classifier. If there are other training strategies or other models that work better and differently, then it raises the question of why do our eyes and visual cortex not work more like *those ones* if evolutionary pressures are applying the same pressure as our training objective.

Why does the model with zooming powers out-do the translation-only model on dataset 1 (where all target images are the same size) and tie the translation-only model dataset 2 (where the target images have different sizes, for which the zooming model should be tailor-made?). Between this strange tie and the high classification rate on Dataset 2, I wonder if maybe one or both models isnt being trained to its potential, which would undermine the overall claim.

Comparing this model to other attention models (e.g. spatial transformer networks, DRAW) would be irrelevant to what I take to be the main point of the paper, but it would address the potential concerns above that training just didnt go very well, or there was some problem with the model parameterization that could be easily fixed."
5,"This paper proposed a neural attention model which has a learnable and differentiable sampling lattice. The work is well motivated as few previous work focus on learning the sampling lattice but with a fixed lattice. This work is quite similar to Spatial Transformer Networks (Jaderberg 2015), but the sampling lattice is learned by the model. The experiments showed that the model can learn a meaning lattice to the visual search task where the sampling lattice looks similar to human being's. 

The main concern of the paper is that experiments are not sufficient. The paper only reports the results on a modified clustered MNIST dataset. It would be more interesting if the authors could conduct  the model on real datasets, such as Toronto Face dataset, CUB bird dataset and SVHN. For example, for the Face dataset, it would be nice if the model can learn to attend different parts of the face for expression recognition, or attend different part of birds for fine-grained classification. Since the authors replied in the pre-review question that the model can learn meaningful lattice on MSCOCO dataset, I think it would be better to add that results into the paper.

Another drawback of the model is that the paper only compare with different variants of itselves. I suggest that this paper should compare with  Spatial Transformer Networks, DRAW, etc., on the same dataset to show the advantage of the learned sampling lattice."
5,"This paper presents a succinct argument that the principle of optimizing receptive field location and size in a simulated eye that can make saccades with respect to a classification error of images of data whose labels depend on variable-size and variable-location subimages, explains the existence of a foveal area in e.g. the primate retina.

The argument could be improved by using more-realistic image data and drawing more direct correspondence with the number, receptive field sizes and eccentricities of retinal cells in e.g. the macaque, but the authors would then face the challenge of identifying a loss function that is both biologically plausible and supportive of their claim.

The argument could also be improved by commenting on the timescales involved. Presumably the density of the foveal center depends on the number of of saccades allowed by the inference process, as well as the size of the target sub-images, and also has an impact on the overall classification accuracy.

Why does the classification error rate of dataset 2 remain stubbornly at 24%? This seems so high that the model may not be working the way wed like it to. It seems that the overall argument of the paper pre-supposes that the model can be trained to be a good classifier. If there are other training strategies or other models that work better and differently, then it raises the question of why do our eyes and visual cortex not work more like *those ones* if evolutionary pressures are applying the same pressure as our training objective.

Why does the model with zooming powers out-do the translation-only model on dataset 1 (where all target images are the same size) and tie the translation-only model dataset 2 (where the target images have different sizes, for which the zooming model should be tailor-made?). Between this strange tie and the high classification rate on Dataset 2, I wonder if maybe one or both models isnt being trained to its potential, which would undermine the overall claim.

Comparing this model to other attention models (e.g. spatial transformer networks, DRAW) would be irrelevant to what I take to be the main point of the paper, but it would address the potential concerns above that training just didnt go very well, or there was some problem with the model parameterization that could be easily fixed."
5,"This paper proposed a neural attention model which has a learnable and differentiable sampling lattice. The work is well motivated as few previous work focus on learning the sampling lattice but with a fixed lattice. This work is quite similar to Spatial Transformer Networks (Jaderberg 2015), but the sampling lattice is learned by the model. The experiments showed that the model can learn a meaning lattice to the visual search task where the sampling lattice looks similar to human being's. 

The main concern of the paper is that experiments are not sufficient. The paper only reports the results on a modified clustered MNIST dataset. It would be more interesting if the authors could conduct  the model on real datasets, such as Toronto Face dataset, CUB bird dataset and SVHN. For example, for the Face dataset, it would be nice if the model can learn to attend different parts of the face for expression recognition, or attend different part of birds for fine-grained classification. Since the authors replied in the pre-review question that the model can learn meaningful lattice on MSCOCO dataset, I think it would be better to add that results into the paper.

Another drawback of the model is that the paper only compare with different variants of itselves. I suggest that this paper should compare with  Spatial Transformer Networks, DRAW, etc., on the same dataset to show the advantage of the learned sampling lattice."
4,"# Review
This paper proposes five modifications to improve PixelCNN, a generative model with tractable likelihood. The authors empirically showed the impact of each of their proposed modifications using a series of ablation experiments. They also reported a new state-of-the-art result on CIFAR-10.
Improving generative models, especially for images, is an active research area and this paper definitely contributes to it.


# Pros
The authors motivate each modification well they proposed. They also used ablation experiments to show each of them is important.

The authors use a discretized mixture of logistic distributions to model the conditional distribution of a sub-pixel instead of a 256-way softmax. This allows to have a lower output dimension and to be better suited at learning ordinal relationships between sub-pixel values. The authors also mentioned it speeded up training time (less computation) as well as the convergence during the optimization of the model (as shown in Fig.6).

The authors make an interesting remark about how the dependencies between the color channels of a pixel are likely to be relatively simple and do not require a deep network to model. This allows them to have a simplified architecture where you don't have to separate out all feature maps in 3 groups depending on whether or not they can see the R/G/B sub-pixel of the current location.


# Cons
It is not clear to me what the predictive distribution for the green channel (and the blue) looks like. More precisely, how are the means of the mixture components linearly depending on the value of the red sub-pixel? I would have liked to see the equations for them.


# Minor Comments
In Fig.2 it is written ""Sequence of 6 layers"" but in the text (Section 2.4) it says 6 blocks of 5 ResNet layers. What is the remaining layer?
In Fig.2 what does the first ""green square -> blue square"" which isn't in the white rectangle represents?
Is there any reason why the mixture indicator is shared across all three channels?"
4,"# Review
This paper proposes five modifications to improve PixelCNN, a generative model with tractable likelihood. The authors empirically showed the impact of each of their proposed modifications using a series of ablation experiments. They also reported a new state-of-the-art result on CIFAR-10.
Improving generative models, especially for images, is an active research area and this paper definitely contributes to it.


# Pros
The authors motivate each modification well they proposed. They also used ablation experiments to show each of them is important.

The authors use a discretized mixture of logistic distributions to model the conditional distribution of a sub-pixel instead of a 256-way softmax. This allows to have a lower output dimension and to be better suited at learning ordinal relationships between sub-pixel values. The authors also mentioned it speeded up training time (less computation) as well as the convergence during the optimization of the model (as shown in Fig.6).

The authors make an interesting remark about how the dependencies between the color channels of a pixel are likely to be relatively simple and do not require a deep network to model. This allows them to have a simplified architecture where you don't have to separate out all feature maps in 3 groups depending on whether or not they can see the R/G/B sub-pixel of the current location.


# Cons
It is not clear to me what the predictive distribution for the green channel (and the blue) looks like. More precisely, how are the means of the mixture components linearly depending on the value of the red sub-pixel? I would have liked to see the equations for them.


# Minor Comments
In Fig.2 it is written ""Sequence of 6 layers"" but in the text (Section 2.4) it says 6 blocks of 5 ResNet layers. What is the remaining layer?
In Fig.2 what does the first ""green square -> blue square"" which isn't in the white rectangle represents?
Is there any reason why the mixture indicator is shared across all three channels?"
3,"Thank you for an interesting angle on highway and residual networks. This paper shows a new angle to how and what kind of representations are learnt at each layer in the aforementioned models. Due to residual information being provided at a periodic number of steps, each of the layers preserve feature identity which prevents lesioning unlike convolutional neural nets.                                 
                                                                                                                                                                                                          
Pros                                                                                                                                                                                                      
- the iterative unrolling view was extremely simple and intuitive, which was supported by theoretical results and reasonable assumptions.                                                                 
- Figure 3 gave a clear visualization for the iterative unrolling view                                                                                                                                    
                                                                                                                                                                                                          
Cons                                                                                                                                                                                                      
- Even though, the perspective is interesting few empirical results were shown to support the argument. The major experiments are image classification and language models trained on mutations of character-aware neural language models.                                                                                                                                                                         
- Figure 4 and 5 could be combined and enlarged to show the effects of batch normalization.   "
3,"This paper provides a new perspective to understanding the ResNet and Highway net. The new perspective assumes that the blocks inside the networks with residual or skip-connection are groups of successive layers with the same hidden size, which performs to iteratively refine their estimates of the same feature instead of generate new representations. Under this perspective, some contradictories with the traditional representation view induced by ResNet and Highway network and other paper can be well explained.

The pros of the paper are:
1. A novel perspective to understand the recent progress of neural network is proposed.
2. The paper provides a quantitatively experimentals to compare ResNet and Highway net, and shows contradict results with several claims from previous work. The authors also give discussions and explanations about the contradictories, which provides a good insight of the disadvantages and advantages between these two kind of networks.

The main cons of the paper is that the experiments are not sufficient. For example, since the main contribution of the paper is to propose the unrolled iterative estimation"" and the stage 4 of Figure 3 seems not follow the assumption of ""unrolled iterative estimation"" and the authors says: ""We note that stage four (with three blocks) appears to be underestimating the representation values, indicating a probable weak link in the architecture."". Thus, it would be much better to do experiments to show that under some condition, the performance of stage 4 can follow the assumption. 

Moreover, the paper should provide more experiments to show the evidence of ""unrolled iterative estimation"", not comparing ResNet with Highway Net. The lack of experiments on this point is the main concern from myself.

"
3,"Thank you for an interesting angle on highway and residual networks. This paper shows a new angle to how and what kind of representations are learnt at each layer in the aforementioned models. Due to residual information being provided at a periodic number of steps, each of the layers preserve feature identity which prevents lesioning unlike convolutional neural nets.                                 
                                                                                                                                                                                                          
Pros                                                                                                                                                                                                      
- the iterative unrolling view was extremely simple and intuitive, which was supported by theoretical results and reasonable assumptions.                                                                 
- Figure 3 gave a clear visualization for the iterative unrolling view                                                                                                                                    
                                                                                                                                                                                                          
Cons                                                                                                                                                                                                      
- Even though, the perspective is interesting few empirical results were shown to support the argument. The major experiments are image classification and language models trained on mutations of character-aware neural language models.                                                                                                                                                                         
- Figure 4 and 5 could be combined and enlarged to show the effects of batch normalization.   "
3,"This paper provides a new perspective to understanding the ResNet and Highway net. The new perspective assumes that the blocks inside the networks with residual or skip-connection are groups of successive layers with the same hidden size, which performs to iteratively refine their estimates of the same feature instead of generate new representations. Under this perspective, some contradictories with the traditional representation view induced by ResNet and Highway network and other paper can be well explained.

The pros of the paper are:
1. A novel perspective to understand the recent progress of neural network is proposed.
2. The paper provides a quantitatively experimentals to compare ResNet and Highway net, and shows contradict results with several claims from previous work. The authors also give discussions and explanations about the contradictories, which provides a good insight of the disadvantages and advantages between these two kind of networks.

The main cons of the paper is that the experiments are not sufficient. For example, since the main contribution of the paper is to propose the unrolled iterative estimation"" and the stage 4 of Figure 3 seems not follow the assumption of ""unrolled iterative estimation"" and the authors says: ""We note that stage four (with three blocks) appears to be underestimating the representation values, indicating a probable weak link in the architecture."". Thus, it would be much better to do experiments to show that under some condition, the performance of stage 4 can follow the assumption. 

Moreover, the paper should provide more experiments to show the evidence of ""unrolled iterative estimation"", not comparing ResNet with Highway Net. The lack of experiments on this point is the main concern from myself.

"
4,"This paper not only shows that a cache model on top of a pre-trained RNN can improve language modeling, but also illustrates a shortcoming of standard RNN models in that they are unable to capture this information themselves. Regardless of whether this is due to the small BPTT window (35 is standard) or an issue with the capability of the RNN itself, this is a useful insight. This technique is an interesting variation of memory augmented neural networks with a number of advantages to many of the standard memory augmented architectures.

They illustrate the neural cache model on not just the Penn Treebank but also WikiText-2 and WikiText-103, two datasets specifically tailored to illustrating long term dependencies with a more realistic vocabulary size. I have not seen the ability to refer up to 2000 words back previously.
I recommend this paper be accepted. There is additionally extensive analysis of the hyperparameters on these datasets, providing further insight.

I recommend this interesting and well analyzed paper be accepted."
4,"This paper not only shows that a cache model on top of a pre-trained RNN can improve language modeling, but also illustrates a shortcoming of standard RNN models in that they are unable to capture this information themselves. Regardless of whether this is due to the small BPTT window (35 is standard) or an issue with the capability of the RNN itself, this is a useful insight. This technique is an interesting variation of memory augmented neural networks with a number of advantages to many of the standard memory augmented architectures.

They illustrate the neural cache model on not just the Penn Treebank but also WikiText-2 and WikiText-103, two datasets specifically tailored to illustrating long term dependencies with a more realistic vocabulary size. I have not seen the ability to refer up to 2000 words back previously.
I recommend this paper be accepted. There is additionally extensive analysis of the hyperparameters on these datasets, providing further insight.

I recommend this interesting and well analyzed paper be accepted."
5,"This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style. 
+The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts. 
+This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper. 
+The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree. 
-It will be more interesting to show results in other domains such as texts and images. 
-In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain. "
5,"Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.

This paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.

Pros:
1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.

2. The proposed method produces visually appealing results on several datasets

3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task

4. The paper is well-written and easy to read

Cons:
1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)

2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.

3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.

I would also like to point out that using super-resolved outputs as opposed to the actual models outputs can produce a false impression of the visual quality of the transferred samples. Id suggest moving original outputs from the appendix into the main part."
5,"This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style. 
+The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts. 
+This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper. 
+The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree. 
-It will be more interesting to show results in other domains such as texts and images. 
-In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain. "
5,"Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.

This paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.

Pros:
1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.

2. The proposed method produces visually appealing results on several datasets

3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task

4. The paper is well-written and easy to read

Cons:
1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)

2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.

3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.

I would also like to point out that using super-resolved outputs as opposed to the actual models outputs can produce a false impression of the visual quality of the transferred samples. Id suggest moving original outputs from the appendix into the main part."
4,"This paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view. Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different view point.

While the idea is quite elegant and novel (I enjoy reading it), more experiments are needed to justify the approach. Probably the most important issue is that there is no baseline, e.g., what if we train the model with the image from the same viewpoint? It should be better than the proposed approach but how close are they? How the performance changes when we gradually change the viewpoint from third-person to first-person? Another important question is that maybe the network just blindly remembers the policy, in this case, the extracted feature could be artifacts of the input image that implicitly counts the time tick in some way (and thus domain-agonistic), but can still perform reasonable policy. Since the experiments are conduct in a synthetic environment, this might happen. An easy check is to run the algorithm on multiple viewpoint and/or with blurred/differently rendered images, and/or with random initial conditions.

Other ablation analysis is also needed. For example, I am not fully convinced by the gradient flipping trick used in Eqn. 5, and in the experiments there is no ablation analysis for that (GAN/EM style training versus gradient flipping trick). For the experiments, Fig. 4,5,6 does not have error bars and is not very convincing."
4,"This paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view. Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different view point.

While the idea is quite elegant and novel (I enjoy reading it), more experiments are needed to justify the approach. Probably the most important issue is that there is no baseline, e.g., what if we train the model with the image from the same viewpoint? It should be better than the proposed approach but how close are they? How the performance changes when we gradually change the viewpoint from third-person to first-person? Another important question is that maybe the network just blindly remembers the policy, in this case, the extracted feature could be artifacts of the input image that implicitly counts the time tick in some way (and thus domain-agonistic), but can still perform reasonable policy. Since the experiments are conduct in a synthetic environment, this might happen. An easy check is to run the algorithm on multiple viewpoint and/or with blurred/differently rendered images, and/or with random initial conditions.

Other ablation analysis is also needed. For example, I am not fully convinced by the gradient flipping trick used in Eqn. 5, and in the experiments there is no ablation analysis for that (GAN/EM style training versus gradient flipping trick). For the experiments, Fig. 4,5,6 does not have error bars and is not very convincing."
4,"This paper proposes an approach to character language modeling (CLMs) based on developing a domain specific language to represent CLMs. The experiments show mixed performance versus neural CLM approaches to modeling linux kernel data and wikipedia text, however the proposed DSL models are slightly more compact and fast to query as compared with neural CLMs. The proposed approach is difficult to understand overall and perhaps is aimed towards the sub-community already working on this sort of approach but lacks sufficient explanation for the ICLR audience. Critically the paper glosses over the major issues of demonstrating the proposed DSL is a valid probabilistic model and how training is performed to fit the model to data (there is clearly not a gradient-based training approach used). FInally the experiments feel incomplete without showing samples drawn from the generative model or analyzing the learned model to determine what it has learned. Overall I feel this paper does not describe the approach in enough depth for readers to understand or re-implement it.

Almost all of the model section is devoted to exposition of the DSL without specifying how probabilities are computed using this model and how training is performed. How are probabilities actually encoded? The DSL description seems to have only discrete decisions rather than probabilities.

Training is perhaps covered in previous papers but there needs to be some discussion of how it works here. Section 2.5 does not do enough to explain how training works or how any measure of optimality is achieved.

Given this model is quite a different hypothesis space from neural models or n-grams, looking and samples drawn from the model seems critical. The current experiments show it can score utterances relatively well but it would be very interesting if the model can sample more structured samples than neural approaches (for example long-range syntax constraints like brackets)"
5,"This paper introduces a novel method for language modeling which is suitable for both modeling programming language as well as natural language. The approach uses a program synthesis algorithm to search over program space and uses count-based estimation of the weights of the program. This is a departure from neural network-based approaches which rely on gradient descent, and thus are extremely slow to estimate. Count-based method such as regular n-gram models suffer because of their simplicity, i.e. not being able to model large context, and scaling badly as context increases. The proposed approach synthesizes programs using MCMC which learn context-sensitive probabilities using count-based estimation, and thus is both fast and able to model long-range context.

Experiments on a programming language datasets, the linux kernel corpus, show that this method is vastly better than both LSTM and n-gram language models. Experiments on the Wikipedia corpus show that the method is competitive, but not better, to SOTA models. Both estimation and query time are significantly better than LSTM LMs, and competitive to n-gram LMs.

It's debatable whether this paper is suitable for ICLR, due to ICLR's focus on neural network-based approaches. However, in the interest of diversity and novelty, such ""outside"" papers should be accepted to ICLR. This paper is likely to inspire more research into fusion of program synthesis and machine learning methods, which was a popular theme at NIPS 2016.

*Pros*
1. Novel approach.
2. Good results.

*Cons*
1. Some significant algorithmic details are not included in the paper. They should at least be included in an appendix for comprehensiveness.

*Comments*
1. Please include n-gram results in the table for Wikipedia results."
4,"This paper proposes an approach to character language modeling (CLMs) based on developing a domain specific language to represent CLMs. The experiments show mixed performance versus neural CLM approaches to modeling linux kernel data and wikipedia text, however the proposed DSL models are slightly more compact and fast to query as compared with neural CLMs. The proposed approach is difficult to understand overall and perhaps is aimed towards the sub-community already working on this sort of approach but lacks sufficient explanation for the ICLR audience. Critically the paper glosses over the major issues of demonstrating the proposed DSL is a valid probabilistic model and how training is performed to fit the model to data (there is clearly not a gradient-based training approach used). FInally the experiments feel incomplete without showing samples drawn from the generative model or analyzing the learned model to determine what it has learned. Overall I feel this paper does not describe the approach in enough depth for readers to understand or re-implement it.

Almost all of the model section is devoted to exposition of the DSL without specifying how probabilities are computed using this model and how training is performed. How are probabilities actually encoded? The DSL description seems to have only discrete decisions rather than probabilities.

Training is perhaps covered in previous papers but there needs to be some discussion of how it works here. Section 2.5 does not do enough to explain how training works or how any measure of optimality is achieved.

Given this model is quite a different hypothesis space from neural models or n-grams, looking and samples drawn from the model seems critical. The current experiments show it can score utterances relatively well but it would be very interesting if the model can sample more structured samples than neural approaches (for example long-range syntax constraints like brackets)"
5,"This paper introduces a novel method for language modeling which is suitable for both modeling programming language as well as natural language. The approach uses a program synthesis algorithm to search over program space and uses count-based estimation of the weights of the program. This is a departure from neural network-based approaches which rely on gradient descent, and thus are extremely slow to estimate. Count-based method such as regular n-gram models suffer because of their simplicity, i.e. not being able to model large context, and scaling badly as context increases. The proposed approach synthesizes programs using MCMC which learn context-sensitive probabilities using count-based estimation, and thus is both fast and able to model long-range context.

Experiments on a programming language datasets, the linux kernel corpus, show that this method is vastly better than both LSTM and n-gram language models. Experiments on the Wikipedia corpus show that the method is competitive, but not better, to SOTA models. Both estimation and query time are significantly better than LSTM LMs, and competitive to n-gram LMs.

It's debatable whether this paper is suitable for ICLR, due to ICLR's focus on neural network-based approaches. However, in the interest of diversity and novelty, such ""outside"" papers should be accepted to ICLR. This paper is likely to inspire more research into fusion of program synthesis and machine learning methods, which was a popular theme at NIPS 2016.

*Pros*
1. Novel approach.
2. Good results.

*Cons*
1. Some significant algorithmic details are not included in the paper. They should at least be included in an appendix for comprehensiveness.

*Comments*
1. Please include n-gram results in the table for Wikipedia results."
5,"The paper presents a learning algorithm for micromanagement of battle scenarios in real-time strategy games. It focuses on a complex sub-problem of the full RTS problem. The assumptions and restrictions made (greedy MDP, distance-based action encoding, etc.) are clear and make sense for this problem.

The main contribution of this paper is the zero-order optimization algorithm and how it is used for structured exploration. This is a nice new application of zero-order optimization meets deep learning for RL, quite well-motivated using similar arguments as DPG. The results show clear wins over vanilla Q-learning and REINFORCE, which is not hard to believe. Although RTS is a very interesting and challenging domain (certainly worthy as a domain of focused research!), it would have been nice to see results on other domains, mainly because it seems that this algorithm could be more generally applicable than just RTS games. Also, evaluation on such a complex domain makes it difficult to predict what other kinds of domains would benefit from this zero-order approach. Maybe the authors could add some text to clarify/motivate this.

There are a few seemingly arbitrary choices that are justified only by ""it worked in practice"". For example, using only the sign of w / Psi_{theta}(s^k, a^k). Again later: ""Also we neglected the argmax operation that chooses the actions"". I suppose this and dividing by t could keep things nicely within or close to [-1,1] ? It might make sense to try truncating/normalizing w/Psi; it seems that much information must be lost when only taking the sign. Also lines such as ""We did not extensively experiment with the structure of the network, but we found the maxpooling and tanh nonlinearity to be particularly important"" and claiming the importance of adagrad over RMSprop without elaboration or providing any details feels somewhat unsatisfactory and leaves the reader wondering why.. e.g. could these only be true in the RTS setup in this paper?

The presentation of the paper can be improved, as some ideas are presented without any context making it unnecessarily confusing. For example, when defining f(\tilde{s}, c) at the top of page 5, the w vector is not explained at all, so the reader is left wondering where it comes from or what its use is. This is explained later, of course, but one sentence on its role here would help contextualize its purpose (maybe refer later to the section where it is described fully). Also page 7: ""because we neglected that a single u is sampled for an entire episode""; actually, no, you did mention this in the text above and it's clear from the pseudo-code too.

""perturbated"" -> ""perturbed""

--- After response period: 

No rebuttal entered, therefore review remains unchanged."
5,"The paper presents a learning algorithm for micromanagement of battle scenarios in real-time strategy games. It focuses on a complex sub-problem of the full RTS problem. The assumptions and restrictions made (greedy MDP, distance-based action encoding, etc.) are clear and make sense for this problem.

The main contribution of this paper is the zero-order optimization algorithm and how it is used for structured exploration. This is a nice new application of zero-order optimization meets deep learning for RL, quite well-motivated using similar arguments as DPG. The results show clear wins over vanilla Q-learning and REINFORCE, which is not hard to believe. Although RTS is a very interesting and challenging domain (certainly worthy as a domain of focused research!), it would have been nice to see results on other domains, mainly because it seems that this algorithm could be more generally applicable than just RTS games. Also, evaluation on such a complex domain makes it difficult to predict what other kinds of domains would benefit from this zero-order approach. Maybe the authors could add some text to clarify/motivate this.

There are a few seemingly arbitrary choices that are justified only by ""it worked in practice"". For example, using only the sign of w / Psi_{theta}(s^k, a^k). Again later: ""Also we neglected the argmax operation that chooses the actions"". I suppose this and dividing by t could keep things nicely within or close to [-1,1] ? It might make sense to try truncating/normalizing w/Psi; it seems that much information must be lost when only taking the sign. Also lines such as ""We did not extensively experiment with the structure of the network, but we found the maxpooling and tanh nonlinearity to be particularly important"" and claiming the importance of adagrad over RMSprop without elaboration or providing any details feels somewhat unsatisfactory and leaves the reader wondering why.. e.g. could these only be true in the RTS setup in this paper?

The presentation of the paper can be improved, as some ideas are presented without any context making it unnecessarily confusing. For example, when defining f(\tilde{s}, c) at the top of page 5, the w vector is not explained at all, so the reader is left wondering where it comes from or what its use is. This is explained later, of course, but one sentence on its role here would help contextualize its purpose (maybe refer later to the section where it is described fully). Also page 7: ""because we neglected that a single u is sampled for an entire episode""; actually, no, you did mention this in the text above and it's clear from the pseudo-code too.

""perturbated"" -> ""perturbed""

--- After response period: 

No rebuttal entered, therefore review remains unchanged."
3,"This paper proposes to use an empirical Bayesian approach to learn the parameters of a neural network, and their priors.
A mixture model prior over the weights leads to a clustering effect in the weight posterior distributions (which are approximated with delta peaks). 
This clustering effect can exploited for parameter quantisation and compression of the network parameters.
The authors show that this leads to compression rates and predictive accuracy comparable to related approaches. 

Earlier work [Han et al. 2015] is based on a three-stage process of pruning small magnitude weights, clustering the remaining ones, and updating the cluster centres to optimise performance. The current work provides a more principled approach that does not have such an ad-hoc multi-stage structure, but a single iterative optimisation process.

A first experiment, described in section 6.1 shows that an empirical Bayes approach, without the use of hyper priors, already leads to a pronounced clustering effect and to setting many weights to zero. 
In particular a compression rate of 64.2 is obtained on the LeNet300-100 model.
In section 6.1 the text refers to figure C, I suppose this should be figure 1.

Section 6.2 describes an experiment where hyper-priors are used, and the parameters of these distributions, as well as other hyper-parameters such as the learning rates, are being optimised using Spearmint (Snoek et al., 2012). Figure 2 shows the performance of the  different points in the hyper-parameter space that have been evaluated (each trained network gives an accuracy-compressionrate point in the graph). The text claims that best results lie on a line, this seems a little opportunistic interpretation given the limited data. Moreover, it would be useful to add a small discussion on whether such a linear relationship would be expected or not. Currently the results of this experiment lack interpretation.

Section 6.3 describes results obtained for both CNN models and compares results to the recent results of (Han et al., 2015) and (Guo et al., 2016).
Comparable results are obtained in terms of compression rate and accuracy. 
The authors state that their current algorithm is too slow to be useful for larger models such as VGG-19, but they do briefly report some results obtained for this model (but do not compare to related work). It would be useful here to explain what slows the training down with respect to standard training without the weight clustering approach, and how the proposed algorithm scales in terms of the relevant quantities of the data and the model.

The contribution of this paper is mostly experimental, leveraging fairly standard ideas from empirical Bayesian learning to introduce weight clustering effects in CNN training.
This being said, it is an interesting result that such a relatively straightforward approach leads to results that are on par with state-of-the-art, but more ad-hoc, network compression techniques.
The paper could be improved by clearly describing the algorithm used for training, and how it scales to large networks and datasets.
Another point that would deserve further discussion is how the hyper-parameter search is performed ( not using test data I assume), and how the compared methods dealt with the search over hyper-parameters to determine the accuracy-compression tradeoff. Ideally, I think, methods should be evaluated across different points on this trade-off.

"
3,"The authors propose a method to compress neural networks by retraining them while putting a mixture of Gaussians prior on the weights with learned means and variances which then can be used to compress the neural network by first setting all weights to the mean of their infered mixture component (resulting in a possible loss of precision) and storing the network in a format which saves only the fixture index and exploits the sparseness of the weights that was enforced in training.

Quality:
Of course it is a serious drawback that the method doesn't seem to work on VGG which would render the method unusable for production (as it is right now, maybe this can be improved). I guess AlexNet takes too long to process, too, otherwise this might be a very valuable addition.
In Figure 2 I am noticing two things: On the left, there is a large number of points with improved accuracy which is not the case for LeNet5-Caffe. Is there any intuition for why that's the case? Additionally regarding the spearmint optimization: Do they authors have found any clues about which hyperparameter settings worked well? This might be helpful for other people trying to apply this method.
I really like Figure 7 in it's latest version.

Clarity:
Especially section 2 on MDL is written very well and gives a nice theoretic introduction. Sections 4, 5 and 6 are very short but seem to contain most relevant information. It might be helpful to have at least some more details about the used models in the paper (maybe the number of layers and the number of parameters).
In 6.1 the authors claim ""Even though most variances seem to be reasonable small there are some that are large"". From figure 1 this is very hard to assess, especially as the vertical histogram essentially shows only the zero component. It might be helpful to have either a log histogram or separate histograms for each componenent. What are the large points in Figure 2 as opposed to the smaller ones? They seem to have a very good compression/accuracy loss ratio, is that it?
Some other points are listed below

originality: While there has been some work on compressing neural networks by using a reduced number of bits to store the parameters and exploiting sparsity structure, I like the idea to directly learn the quantization by means of a gaussian mixture prior in retraining which seems to be more principled than other approaches

significance: The method achievs state-of-the-art performance on the two shown examples on MNIST, however these networks are far from the deep networks used in state-of-the-art models. This obviously is a drawback for the practical usability of the methods and therefor it's significance. If the method could be made to work on more state-of-the-art networks like VGG or ResNet, I would consider this a contribution of high significance.

Minor issues:

page 1: There seems to be a space in front of the first author's name
page 3: ""in this scenario, pi_0 may be fixed..."". Missing backslash in TeX?
page 6: 6.2: two wrong blanks in ""the number of components_, \tau_.""
page 6, 6.3: ""in experiences with VGG"": In experiments?
page 12: ""Figure C"": Figure 7?
"
3,"This paper proposes to use an empirical Bayesian approach to learn the parameters of a neural network, and their priors.
A mixture model prior over the weights leads to a clustering effect in the weight posterior distributions (which are approximated with delta peaks). 
This clustering effect can exploited for parameter quantisation and compression of the network parameters.
The authors show that this leads to compression rates and predictive accuracy comparable to related approaches. 

Earlier work [Han et al. 2015] is based on a three-stage process of pruning small magnitude weights, clustering the remaining ones, and updating the cluster centres to optimise performance. The current work provides a more principled approach that does not have such an ad-hoc multi-stage structure, but a single iterative optimisation process.

A first experiment, described in section 6.1 shows that an empirical Bayes approach, without the use of hyper priors, already leads to a pronounced clustering effect and to setting many weights to zero. 
In particular a compression rate of 64.2 is obtained on the LeNet300-100 model.
In section 6.1 the text refers to figure C, I suppose this should be figure 1.

Section 6.2 describes an experiment where hyper-priors are used, and the parameters of these distributions, as well as other hyper-parameters such as the learning rates, are being optimised using Spearmint (Snoek et al., 2012). Figure 2 shows the performance of the  different points in the hyper-parameter space that have been evaluated (each trained network gives an accuracy-compressionrate point in the graph). The text claims that best results lie on a line, this seems a little opportunistic interpretation given the limited data. Moreover, it would be useful to add a small discussion on whether such a linear relationship would be expected or not. Currently the results of this experiment lack interpretation.

Section 6.3 describes results obtained for both CNN models and compares results to the recent results of (Han et al., 2015) and (Guo et al., 2016).
Comparable results are obtained in terms of compression rate and accuracy. 
The authors state that their current algorithm is too slow to be useful for larger models such as VGG-19, but they do briefly report some results obtained for this model (but do not compare to related work). It would be useful here to explain what slows the training down with respect to standard training without the weight clustering approach, and how the proposed algorithm scales in terms of the relevant quantities of the data and the model.

The contribution of this paper is mostly experimental, leveraging fairly standard ideas from empirical Bayesian learning to introduce weight clustering effects in CNN training.
This being said, it is an interesting result that such a relatively straightforward approach leads to results that are on par with state-of-the-art, but more ad-hoc, network compression techniques.
The paper could be improved by clearly describing the algorithm used for training, and how it scales to large networks and datasets.
Another point that would deserve further discussion is how the hyper-parameter search is performed ( not using test data I assume), and how the compared methods dealt with the search over hyper-parameters to determine the accuracy-compression tradeoff. Ideally, I think, methods should be evaluated across different points on this trade-off.

"
3,"The authors propose a method to compress neural networks by retraining them while putting a mixture of Gaussians prior on the weights with learned means and variances which then can be used to compress the neural network by first setting all weights to the mean of their infered mixture component (resulting in a possible loss of precision) and storing the network in a format which saves only the fixture index and exploits the sparseness of the weights that was enforced in training.

Quality:
Of course it is a serious drawback that the method doesn't seem to work on VGG which would render the method unusable for production (as it is right now, maybe this can be improved). I guess AlexNet takes too long to process, too, otherwise this might be a very valuable addition.
In Figure 2 I am noticing two things: On the left, there is a large number of points with improved accuracy which is not the case for LeNet5-Caffe. Is there any intuition for why that's the case? Additionally regarding the spearmint optimization: Do they authors have found any clues about which hyperparameter settings worked well? This might be helpful for other people trying to apply this method.
I really like Figure 7 in it's latest version.

Clarity:
Especially section 2 on MDL is written very well and gives a nice theoretic introduction. Sections 4, 5 and 6 are very short but seem to contain most relevant information. It might be helpful to have at least some more details about the used models in the paper (maybe the number of layers and the number of parameters).
In 6.1 the authors claim ""Even though most variances seem to be reasonable small there are some that are large"". From figure 1 this is very hard to assess, especially as the vertical histogram essentially shows only the zero component. It might be helpful to have either a log histogram or separate histograms for each componenent. What are the large points in Figure 2 as opposed to the smaller ones? They seem to have a very good compression/accuracy loss ratio, is that it?
Some other points are listed below

originality: While there has been some work on compressing neural networks by using a reduced number of bits to store the parameters and exploiting sparsity structure, I like the idea to directly learn the quantization by means of a gaussian mixture prior in retraining which seems to be more principled than other approaches

significance: The method achievs state-of-the-art performance on the two shown examples on MNIST, however these networks are far from the deep networks used in state-of-the-art models. This obviously is a drawback for the practical usability of the methods and therefor it's significance. If the method could be made to work on more state-of-the-art networks like VGG or ResNet, I would consider this a contribution of high significance.

Minor issues:

page 1: There seems to be a space in front of the first author's name
page 3: ""in this scenario, pi_0 may be fixed..."". Missing backslash in TeX?
page 6: 6.2: two wrong blanks in ""the number of components_, \tau_.""
page 6, 6.3: ""in experiences with VGG"": In experiments?
page 12: ""Figure C"": Figure 7?
"
5,"This paper investigates a set of tasks that augment the basic bAbI problems. In particular, some of the people and objects in the scenarios are replaced with unknown variables. Some of these variables must be known to solve the question, thus the agent must learn to query for the values of these variables. Interestingly, one can now measure both the performance of the agent in correctly answering the question, and its efficiency in asking for the values of the correct unknown variables (and not variables that are unnecessary to answer the question). This inferring of unknown variables goes beyond what is required for the vanilla version of the bAbI tasks, which are now more or less solved.

The paper is well-written, and the contributions are clear. Due to the very limited vocabulary and structure of the bAbI problems in general, I think these tasks (and variants on them) should be viewed more as basic reasoning tasks than natural language understanding. Im not convinced by the claim of the paper that this really tests the interaction capabilities of agents  while the task is phrased as a kind of interaction, I think its more aptly described by simply inferring important unknown variables, which (while important) is more related to reasoning. Im not sure whether the connection of this ability to interaction is more a superficial one.

That being said, it is certainly true that conversational agents will need basic reasoning abilities to converse meaningfully with humans. I sympathise with the general goal of the bAbI tasks, which is to test these reasoning abilities in synthetic environments, that are just complicated enough (but not more) to drive the construction of interesting models. I am convinced by the authors that their extension to these tasks are interesting and worthy of future investigation, and thus I recommend the acceptance of the paper.
"
5,"This paper investigates a set of tasks that augment the basic bAbI problems. In particular, some of the people and objects in the scenarios are replaced with unknown variables. Some of these variables must be known to solve the question, thus the agent must learn to query for the values of these variables. Interestingly, one can now measure both the performance of the agent in correctly answering the question, and its efficiency in asking for the values of the correct unknown variables (and not variables that are unnecessary to answer the question). This inferring of unknown variables goes beyond what is required for the vanilla version of the bAbI tasks, which are now more or less solved.

The paper is well-written, and the contributions are clear. Due to the very limited vocabulary and structure of the bAbI problems in general, I think these tasks (and variants on them) should be viewed more as basic reasoning tasks than natural language understanding. Im not convinced by the claim of the paper that this really tests the interaction capabilities of agents  while the task is phrased as a kind of interaction, I think its more aptly described by simply inferring important unknown variables, which (while important) is more related to reasoning. Im not sure whether the connection of this ability to interaction is more a superficial one.

That being said, it is certainly true that conversational agents will need basic reasoning abilities to converse meaningfully with humans. I sympathise with the general goal of the bAbI tasks, which is to test these reasoning abilities in synthetic environments, that are just complicated enough (but not more) to drive the construction of interesting models. I am convinced by the authors that their extension to these tasks are interesting and worthy of future investigation, and thus I recommend the acceptance of the paper.
"
5,The problem addressed here is practically important (supervised learning with n<
5,The problem addressed here is practically important (supervised learning with n<
5,"UPDATE: The authors addressed all my concerns in the new version of the paper, so I raised my score and now recommend acceptance.
--------------
This paper combines the recent progress in variational autoencoder and autoregressive density modeling in the proposed PixelVAE model. The paper shows that it can match the NLL performance of a PixelCNN with a PixelVAE that has a much shallower PixelCNN decoder.
I think the idea of capturing the global structure with a VAE and modeling the local structure with a PixelCNN decoder makes a lot of sense and can prevent the blurry reconstruction/samples of VAE. I specially like the hierarchical image generation experiments.

I have the following suggestions/concerns about the paper:

1) Is there any experiment showing that using the PixelCNN as the decoder of VAE will result in better disentangling of high-level factors of variations in the hidden code? For example, the authors can train a PixelVAE and VAE on MNIST with 2D hidden code and visualize the 2D hidden code for test images and color code each hidden code based on the digit and show that the digits have a better separation in the PixelVAE representation. A semi-supervised classification comparison between VAE and the PixelVAE will also significantly improve the quality of the paper.

2) A similar idea is also presented in a concurrent ICLR submission ""Variational Lossy Autoencoder"". It would be interesting to have a discussion included in the paper and compare these works.

3) The answer to the pre-review questions made the architecture details of the paper much more clear, but I still ask the authors to include the exact architecture details of all the experiments in the paper and/or open source the code. The clarity of the presentation is not satisfying and the experiments are difficult to reproduce.

4) As pointed out in my pre-review question, it would be great to include two sets of MNIST samples maybe in an appendix section. One with PixelCNN and the other with PixelVAE with the same pixelcnn depth to illustrate the hidden code in PixelVAE actually captures the global structure.

I will gladly raise the score if the authors address my concerns."
5,"UPDATE: The authors addressed all my concerns in the new version of the paper, so I raised my score and now recommend acceptance.
--------------
This paper combines the recent progress in variational autoencoder and autoregressive density modeling in the proposed PixelVAE model. The paper shows that it can match the NLL performance of a PixelCNN with a PixelVAE that has a much shallower PixelCNN decoder.
I think the idea of capturing the global structure with a VAE and modeling the local structure with a PixelCNN decoder makes a lot of sense and can prevent the blurry reconstruction/samples of VAE. I specially like the hierarchical image generation experiments.

I have the following suggestions/concerns about the paper:

1) Is there any experiment showing that using the PixelCNN as the decoder of VAE will result in better disentangling of high-level factors of variations in the hidden code? For example, the authors can train a PixelVAE and VAE on MNIST with 2D hidden code and visualize the 2D hidden code for test images and color code each hidden code based on the digit and show that the digits have a better separation in the PixelVAE representation. A semi-supervised classification comparison between VAE and the PixelVAE will also significantly improve the quality of the paper.

2) A similar idea is also presented in a concurrent ICLR submission ""Variational Lossy Autoencoder"". It would be interesting to have a discussion included in the paper and compare these works.

3) The answer to the pre-review questions made the architecture details of the paper much more clear, but I still ask the authors to include the exact architecture details of all the experiments in the paper and/or open source the code. The clarity of the presentation is not satisfying and the experiments are difficult to reproduce.

4) As pointed out in my pre-review question, it would be great to include two sets of MNIST samples maybe in an appendix section. One with PixelCNN and the other with PixelVAE with the same pixelcnn depth to illustrate the hidden code in PixelVAE actually captures the global structure.

I will gladly raise the score if the authors address my concerns."
4,"I don't have much to add to my pre-review questions. The main thing I'd like to see that would strengthen my review further is a larger scale evaluation, more discussion of the hyperparameters, etc. Where test error are reported for snapshot ensembles it would be useful to report statistics about the performance of individual ensemble members for comparison (mean and standard deviation, maybe best single member's error rate)."
5,"The work presented in this paper proposes a method to get an ensemble of neural networks at no extra training cost (i.e., at the cost of training a single network), by saving snapshots of the network during training. Network is trained using a cyclic (cosine) learning rate schedule; the snapshots are obtained when the learning rate is at the lowest points of the cycles. Using these snapshot ensembles, they show gains in performance over a single network on the image classification task on a variety of datasets.


Positives:

1. The work should be easy to adopt and re-produce, given the simple techinque and the experimental details in the paper.
2. Well written paper, with clear description of the method and thorough experiments.


Suggestions for improvement / other comments:

1. While it is fair to compare against other techniques assuming a fixed computational budget, for a clear perspective, thorough comaprisons with ""true ensembles"" (i.e., ensembles of networks trained independently) should be provided.
Specificially, Table 4 should be augmented with results from ""true ensembles"".

2. Comparison with true ensembles is only provided for DenseNet-40 on CIFAR100 in Figure 4. The proposed snapshot ensemble achieves approximately 66% of the improvement of ""true ensemble"" over the single baseline model. This is not reflected accurately in the authors' claim in the abstract: ""[snapshot ensembles] **almost match[es]** the results of far more expensive independently trained [true ensembles].""

3. As mentioned before: to understand the diversity of snapshot ensembles, it would help to the diversity against different ensembling technique, e.g. (1) ""true ensembles"", (2) ensembles from dropout as described by Gal et. al, 2016 (Dropout as a Bayesian Approximation)."
4,"I don't have much to add to my pre-review questions. The main thing I'd like to see that would strengthen my review further is a larger scale evaluation, more discussion of the hyperparameters, etc. Where test error are reported for snapshot ensembles it would be useful to report statistics about the performance of individual ensemble members for comparison (mean and standard deviation, maybe best single member's error rate)."
5,"The work presented in this paper proposes a method to get an ensemble of neural networks at no extra training cost (i.e., at the cost of training a single network), by saving snapshots of the network during training. Network is trained using a cyclic (cosine) learning rate schedule; the snapshots are obtained when the learning rate is at the lowest points of the cycles. Using these snapshot ensembles, they show gains in performance over a single network on the image classification task on a variety of datasets.


Positives:

1. The work should be easy to adopt and re-produce, given the simple techinque and the experimental details in the paper.
2. Well written paper, with clear description of the method and thorough experiments.


Suggestions for improvement / other comments:

1. While it is fair to compare against other techniques assuming a fixed computational budget, for a clear perspective, thorough comaprisons with ""true ensembles"" (i.e., ensembles of networks trained independently) should be provided.
Specificially, Table 4 should be augmented with results from ""true ensembles"".

2. Comparison with true ensembles is only provided for DenseNet-40 on CIFAR100 in Figure 4. The proposed snapshot ensemble achieves approximately 66% of the improvement of ""true ensemble"" over the single baseline model. This is not reflected accurately in the authors' claim in the abstract: ""[snapshot ensembles] **almost match[es]** the results of far more expensive independently trained [true ensembles].""

3. As mentioned before: to understand the diversity of snapshot ensembles, it would help to the diversity against different ensembling technique, e.g. (1) ""true ensembles"", (2) ensembles from dropout as described by Gal et. al, 2016 (Dropout as a Bayesian Approximation)."
3,"This paper basically applies A3C to 3D spatial navigation tasks. 

- This is not the first time A3C has been applied to 3D navigation. In fact the original paper reported these experiments. Although the experimental results are great, I am not sure if this paper has any additional insights to warrant itself as a conference paper. It might make more sense as a workshop paper

-  Are the graphs in Fig 5 constructed using a single hyper-parameter sweep? I think the authors should report results with many random initializations to make the comparisons more robust

- Overall the two main ideas in this paper -- A3C and curriculums -- are not really novel but the authors do make use of them in a real system. "
3,"This paper basically applies A3C to 3D spatial navigation tasks. 

- This is not the first time A3C has been applied to 3D navigation. In fact the original paper reported these experiments. Although the experimental results are great, I am not sure if this paper has any additional insights to warrant itself as a conference paper. It might make more sense as a workshop paper

-  Are the graphs in Fig 5 constructed using a single hyper-parameter sweep? I think the authors should report results with many random initializations to make the comparisons more robust

- Overall the two main ideas in this paper -- A3C and curriculums -- are not really novel but the authors do make use of them in a real system. "
3,"The paper presents a method for predicting video sequences in the lines of Mathieu et al. The contribution is the separation of the predictor into two different networks, picking up motion and content, respectively.

The paper is very interesting, but the novelty is low compared to the referenced work. As also pointed out by AnonReviewer1, there is a similarity with two-stream networks (and also a whole body of work building on this seminal paper). Separating motion and content has also been proposed for other applications, e.g. pose estimation.

Details :

The paper can be clearly understood if the basic frameworks (like GANs) are known, but the presentation is not general and good enough for a broad public.

Example : Losses (7) to (9) are well known from the Matthieu et al. paper. However, to make the paper self-contained, they should be properly explained, and it should be mentioned that they are ""additional"" losses. The main target is the GAN loss. The adversarial part of the paper is not properly enough introduced. I do agree, that adversarial training is now well enough known in the community, but it should still be properly introduced. This also involves the explanation that L_Disc is the loss for a second network, the discriminator and explaining the role of both etc.

Equation (1) : c is not explained (are these motion vectors)? c is also overloaded with the feature dimension c'.

The residual nature of the layer should be made more apparent in equation (3).

There are several typos, absence of articles and prepositions (""of"" etc.). The paper should be reread carefully.
"
3,"You are training VGG size networks on quite small datasets, and you seem to use the same architecture for all datasets. I didn't find any information on pre-training. Didn't you have any issues with overfitting?"
3,"The paper presents a method for predicting video sequences in the lines of Mathieu et al. The contribution is the separation of the predictor into two different networks, picking up motion and content, respectively.

The paper is very interesting, but the novelty is low compared to the referenced work. As also pointed out by AnonReviewer1, there is a similarity with two-stream networks (and also a whole body of work building on this seminal paper). Separating motion and content has also been proposed for other applications, e.g. pose estimation.

Details :

The paper can be clearly understood if the basic frameworks (like GANs) are known, but the presentation is not general and good enough for a broad public.

Example : Losses (7) to (9) are well known from the Matthieu et al. paper. However, to make the paper self-contained, they should be properly explained, and it should be mentioned that they are ""additional"" losses. The main target is the GAN loss. The adversarial part of the paper is not properly enough introduced. I do agree, that adversarial training is now well enough known in the community, but it should still be properly introduced. This also involves the explanation that L_Disc is the loss for a second network, the discriminator and explaining the role of both etc.

Equation (1) : c is not explained (are these motion vectors)? c is also overloaded with the feature dimension c'.

The residual nature of the layer should be made more apparent in equation (3).

There are several typos, absence of articles and prepositions (""of"" etc.). The paper should be reread carefully.
"
3,"You are training VGG size networks on quite small datasets, and you seem to use the same architecture for all datasets. I didn't find any information on pre-training. Didn't you have any issues with overfitting?"
5,"In supervised learning, a significant advance occurred when the framework of semi-supervised learning was  adopted, which used the weaker approach of unsupervised learning to infer some property, such as a distance measure or a smoothness regularizer, which could then be used with a small number of labeled examples. The approach rested on the assumption of smoothness on the manifold, typically. 

This paper attempts to stretch this analogy to reinforcement learning, although the analogy is somewhat incoherent. Labels are not equivalent to reward functions, and positive or negative rewards do not mean the same as positive and negative labels. Still, the paper makes a worthwhile attempt to explore this notion of semi-supervised RL, which is clearly an important area that deserves more attention. The authors use the term ""labeled MDP"" to mean the typical MDP framework where the reward function is unknown. They use the confusing term ""unlabeled MDP"" to mean the situation where the reward is unknown, which is technically not an MDP (but a controlled Markov process). 

In the classical RL transfer learning setup, the agent is attempting to transfer learning from a source ""labeled"" MDP to a target ""labeled"" MDP (where both reward functions are known, but the learned policy is known only in the source MDP). In the semi-supervised RL setting, the target is an ""unlabeled"" CMP, and the source is both a ""labeled"" MDP and an ""unlabeled"" CMP. The basic approach is to use inverse RL to infer the unknown ""labels"" and then attempt to construct transfer. A further restriction is made to linearly solvable MDPs for technical reasons. Experiments are reported using three relatively complex domains using the Mujoco physics simulator. 

The work is interesting, but in the opinion of this reviewer, the work fails to provide a simple sufficiently general notion of semi-supervised RL that will be of sufficiently wide interest to the RL community. That remains to be done by a future paper, but in the interim, the work here is sufficiently interesting and the problem is certainly a worthwhile one to study. "
5,"In supervised learning, a significant advance occurred when the framework of semi-supervised learning was  adopted, which used the weaker approach of unsupervised learning to infer some property, such as a distance measure or a smoothness regularizer, which could then be used with a small number of labeled examples. The approach rested on the assumption of smoothness on the manifold, typically. 

This paper attempts to stretch this analogy to reinforcement learning, although the analogy is somewhat incoherent. Labels are not equivalent to reward functions, and positive or negative rewards do not mean the same as positive and negative labels. Still, the paper makes a worthwhile attempt to explore this notion of semi-supervised RL, which is clearly an important area that deserves more attention. The authors use the term ""labeled MDP"" to mean the typical MDP framework where the reward function is unknown. They use the confusing term ""unlabeled MDP"" to mean the situation where the reward is unknown, which is technically not an MDP (but a controlled Markov process). 

In the classical RL transfer learning setup, the agent is attempting to transfer learning from a source ""labeled"" MDP to a target ""labeled"" MDP (where both reward functions are known, but the learned policy is known only in the source MDP). In the semi-supervised RL setting, the target is an ""unlabeled"" CMP, and the source is both a ""labeled"" MDP and an ""unlabeled"" CMP. The basic approach is to use inverse RL to infer the unknown ""labels"" and then attempt to construct transfer. A further restriction is made to linearly solvable MDPs for technical reasons. Experiments are reported using three relatively complex domains using the Mujoco physics simulator. 

The work is interesting, but in the opinion of this reviewer, the work fails to provide a simple sufficiently general notion of semi-supervised RL that will be of sufficiently wide interest to the RL community. That remains to be done by a future paper, but in the interim, the work here is sufficiently interesting and the problem is certainly a worthwhile one to study. "
5,"This paper proposes a compare-aggregate framework that performs word-level matching followed by aggregation with convolutional neural networks. It compares six different comparison functions and evaluates them on four datasets. Extensive experimental results have been reported and compared against various published baselines.

The paper is well written overall.

A few detailed comments:
* page 4, line5: including a some -> including some
* What's the benefit of the preprocessing and attention step? Can you provide the results without it?
* Figure 2 is hard to read, esp. when on printed hard copy. Please enhance the quality.
"
5,"This paper proposes a compare-aggregate framework that performs word-level matching followed by aggregation with convolutional neural networks. It compares six different comparison functions and evaluates them on four datasets. Extensive experimental results have been reported and compared against various published baselines.

The paper is well written overall.

A few detailed comments:
* page 4, line5: including a some -> including some
* What's the benefit of the preprocessing and attention step? Can you provide the results without it?
* Figure 2 is hard to read, esp. when on printed hard copy. Please enhance the quality.
"
2,"The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)

In general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. 

Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?

Figure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads ""log p(topic proportions)"" which is a bit confusing.

Section 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal?

None of the numbers include error bars. Are the results statistically significant?


Minor comments:

Last term in equation (3) is not ""error""; reconstruction accuracy or negative reconstruction error perhaps?

The idea of using an inference network is much older, cf. Helmholtz machine. 
"
3,"This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called amortized inference can be much faster than normal inference where inference must be run iteratively for every document. Some comments:
Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?
The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?
The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof: "
2,"The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)

In general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. 

Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?

Figure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads ""log p(topic proportions)"" which is a bit confusing.

Section 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal?

None of the numbers include error bars. Are the results statistically significant?


Minor comments:

Last term in equation (3) is not ""error""; reconstruction accuracy or negative reconstruction error perhaps?

The idea of using an inference network is much older, cf. Helmholtz machine. 
"
3,"This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called amortized inference can be much faster than normal inference where inference must be run iteratively for every document. Some comments:
Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?
The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?
The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof: "
2,"The paper presents a novel look at binary auto-encoders, formulating the objective function as a min-max reconstruction error over a training set given the observed intermediate representations. The author shows that this formulation leads to a bi-convex problem that can be solved by alternating minimisation methods; this part is non-trivial and is the main contribution of the paper. Proof-of-concept experiments are performed, showing improvements for 1-hidden layer auto-encoders with respect to a vanilla approach. 

The experimental section is fairly weak because the literature on auto-encoders is huge and many variants were shown to perform better than straightforward approaches without being more complicated (e.g., denoising auto-encoders). Yet, the paper presents an analysis that leads to a new learning algorithm for an old problem, and is likely worth discussing. "
2,"The paper presents a novel look at binary auto-encoders, formulating the objective function as a min-max reconstruction error over a training set given the observed intermediate representations. The author shows that this formulation leads to a bi-convex problem that can be solved by alternating minimisation methods; this part is non-trivial and is the main contribution of the paper. Proof-of-concept experiments are performed, showing improvements for 1-hidden layer auto-encoders with respect to a vanilla approach. 

The experimental section is fairly weak because the literature on auto-encoders is huge and many variants were shown to perform better than straightforward approaches without being more complicated (e.g., denoising auto-encoders). Yet, the paper presents an analysis that leads to a new learning algorithm for an old problem, and is likely worth discussing. "
3,"This paper studies in depth the idea of quantizing down convolutional layers to 3 bits, with a different positive and negative per-layer scale. It goes on to provide an exhaustive analysis of performance (essentially no loss) on real benchmarks (this paper is remarkably MNIST-free).

The relevance of this paper is that it likely provides a lower bound on quantization approaches that don't sacrifice any performance, and hence can plausibly become the approach of choice for resource-constrained inference, and might suggest new hardware designs to take advantage of the proposed structure.

Furthermore, the paper provides power measurements, which is really the main metric that anyone working seriously in that space cares about. (Nit: I don't see measurements for the full-precision baseline).

I would have loved to see a SOTA result on ImageNet and a result on a strong LSTM baseline to be fully convinced.
I would have also liked to see discussion of the wall time to result using this training procedure."
3,"This paper studies in depth the idea of quantizing down convolutional layers to 3 bits, with a different positive and negative per-layer scale. It goes on to provide an exhaustive analysis of performance (essentially no loss) on real benchmarks (this paper is remarkably MNIST-free).

The relevance of this paper is that it likely provides a lower bound on quantization approaches that don't sacrifice any performance, and hence can plausibly become the approach of choice for resource-constrained inference, and might suggest new hardware designs to take advantage of the proposed structure.

Furthermore, the paper provides power measurements, which is really the main metric that anyone working seriously in that space cares about. (Nit: I don't see measurements for the full-precision baseline).

I would have loved to see a SOTA result on ImageNet and a result on a strong LSTM baseline to be fully convinced.
I would have also liked to see discussion of the wall time to result using this training procedure."
3,"Summary: 
The paper proposes a model training strategy to achieve higher accuracy. The issue is train a too large model and you going to over-fit and your model will capture noise. Prune models or make it too small then it will miss important connections and under-fit. Thus, the proposed method involves various training steps: first they train a dense network, then prune it making it sparse then train a sparse network and finally they add connections back and train the model as dense again (DSD). The DSD method is generic method that can be used in CNN/RNN/LSTM. The reasons why models have better accuracy after DSD are: escape of saddle point, sparsity makes model more robust to noise and symmetry break allowing richer representations.

Pro:
The main point that this paper wants to show is that a model has the capacity to achieve higher accuracy, because it was shown that it is possible to compress a model without losing accuracy. And lossless compression means that theres significant redundancy in the models that were trained using current training methods. This is an important observation that large models can get better accuracies as better training schemes are used. 

Cons & Questions:
The issue is that the accuracy is slightly increased (2 or 3%) for most models. And the question is what is the price paid for this improvement? Resource and performance concerns arises because training a large model is computationally expensive (hours or even days using high performance GPUs).

Second question, can I keep adding Dense, Sparse and Dense training iterations to get higher and higher accuracy improvement? Are there limitations to this DSDSD approach?


"
5,"This paper presents a training strategy for deep networks.  First, the network is trained in a standard fashion.  Second, small magnitude weights are clamped to 0; the rest of the weights continue to be trained.  Finally, all the weights are again jointly trained.  Experiments on a variety of image, text, and speech datasets demonstrate the approach can obtain high-quality results.

The proposed idea is novel and interesting.  In a sense it is close to Dropout, though as noted in the paper the deterministic weight clamping method is different.

The main advantage of the proposed method is its simplicity.  Three hyper-parameters are needed: the number of weights to clamp to 0, and the numbers of epochs of training used in the first dense phase and the sparse phase.  Given these, it can be plugged in to training a range of networks, as shown in the experiments.

The concern I have is regarding the current empirical evaluation.  As noted in the question phase, it seems the baseline methods are not trained for as many epochs as the proposed method.  Standard tricks, such as dropping the learning rate upon ""convergence"" and continuing to learn, can be employed.  The response seems to indicate that these approaches can be effective.  I think a more thorough empirical analysis of performance over epochs, learning rates, etc. would strengthen the paper.  An exploration regarding the sparsity hyper-parameter would also be interesting.
"
3,"Summary: 
The paper proposes a model training strategy to achieve higher accuracy. The issue is train a too large model and you going to over-fit and your model will capture noise. Prune models or make it too small then it will miss important connections and under-fit. Thus, the proposed method involves various training steps: first they train a dense network, then prune it making it sparse then train a sparse network and finally they add connections back and train the model as dense again (DSD). The DSD method is generic method that can be used in CNN/RNN/LSTM. The reasons why models have better accuracy after DSD are: escape of saddle point, sparsity makes model more robust to noise and symmetry break allowing richer representations.

Pro:
The main point that this paper wants to show is that a model has the capacity to achieve higher accuracy, because it was shown that it is possible to compress a model without losing accuracy. And lossless compression means that theres significant redundancy in the models that were trained using current training methods. This is an important observation that large models can get better accuracies as better training schemes are used. 

Cons & Questions:
The issue is that the accuracy is slightly increased (2 or 3%) for most models. And the question is what is the price paid for this improvement? Resource and performance concerns arises because training a large model is computationally expensive (hours or even days using high performance GPUs).

Second question, can I keep adding Dense, Sparse and Dense training iterations to get higher and higher accuracy improvement? Are there limitations to this DSDSD approach?


"
5,"This paper presents a training strategy for deep networks.  First, the network is trained in a standard fashion.  Second, small magnitude weights are clamped to 0; the rest of the weights continue to be trained.  Finally, all the weights are again jointly trained.  Experiments on a variety of image, text, and speech datasets demonstrate the approach can obtain high-quality results.

The proposed idea is novel and interesting.  In a sense it is close to Dropout, though as noted in the paper the deterministic weight clamping method is different.

The main advantage of the proposed method is its simplicity.  Three hyper-parameters are needed: the number of weights to clamp to 0, and the numbers of epochs of training used in the first dense phase and the sparse phase.  Given these, it can be plugged in to training a range of networks, as shown in the experiments.

The concern I have is regarding the current empirical evaluation.  As noted in the question phase, it seems the baseline methods are not trained for as many epochs as the proposed method.  Standard tricks, such as dropping the learning rate upon ""convergence"" and continuing to learn, can be employed.  The response seems to indicate that these approaches can be effective.  I think a more thorough empirical analysis of performance over epochs, learning rates, etc. would strengthen the paper.  An exploration regarding the sparsity hyper-parameter would also be interesting.
"
5,"Summary
===
This paper proposes the Neural Physics Engine (NPE), a network architecture
which simulates object interactions. While NPE decides to explicitly represent
objects (rather than video frames), it incorporates knowledge of physics
almost exclusively through training data. It is tested in a toy domain with
bouncing 2d balls.

The proposed architecture processes each object in a scene one at a time.
Pairs of objects are embedded in a common space where the effect of the
objects on each other can be represented. These embeddings are summed
and combined with the focus object's state to predict the focus object's
change in velocity. Alternative baselines are presented which either
forego the pairwise embedding for a single object embedding or
encode a focus object's neighbors in a sequence of LSTM states.

NPE outperforms the baselines dramatically, showing the importance of
architecture choices in learning to do this object based simulation.
The model is tested in multiple ways. Ability to predict object trajectory
over long time spans is measured. Generalization to different numbers of objects
is measured. Generalization to slightly altered environments (difference
shaped walls) is measured. Finally, the NPE is also trained to predict
object mass using only interactions with other objects, where it also
outperforms baselines.


Comments
===

* I have one more clarifying question. Are the inputs to the blue box in
figure 3 (b)/(c) the concatenation of the summed embeddings and state vector
of object 3? Or is the input to the blue module some other combination of the
two vectors?


* Section 2.1 begins with ""First, because physics does not
change across inertial frames, it suffices to separately predict the future state of each object conditioned
on the past states of itself and the other objects in its neighborhood, similar to Fragkiadaki
et al. (2015).""

I think this is an argument to forego the visual representation used by previous
work in favor of an object only representation. This would be more clear if there
were contrast with a visual representation.


* As addressed in the paper, this approach is novel, though less so after taking
into consideration the concurrent work of Battaglia et. al. in NIPS 2016 titled
""Interaction Networks for Learning about Objects, Relations and Physics.""
This work offers a different network architecture and set of experiments, as
well as great presentation, but the use of an object based representation
for learning to predict physical behavior is shared.


Overall Evaluation
===

This paper was a pleasure to read and provided many experiments that offered
clear and interesting conclusions. It offers a novel approach (though
less so compared to the concurrent work of Battaglia et. al. 2016) which
represents a significant step forward in the current investigation of
intuitive physics."
4,"- summary

The paper proposes a differntiable Neural Physics Engine (NPE). The NPE consists of an encoder and a decoder function. The NPE takes as input the state of pairs of objects (within a neighbourhood of a focus object) at two previous time-steps in a scene. The encoder function summarizes the interaction of each pair of objects. The decoder then outputs the change in velocity of the focus object at the next time step. The NPE is evaluated on various environments containing bouncing balls.

- novelty

The differentiable NPE is a novel concept. However, concurrently Battaglia et al. (NIPS 2016) proposes a very similar model. Just as this work, Battaglia et al. (NIPS 2016) consider a model which consists of a encoder function (relation-centric) which encodes the interaction among a focus object and other objects in the scene and a decoder (relation-centric) function which considers the cumulative (encoded) effect of object interactions on the focus object and predicts effect of the interactions.  Aspects like only considering objects interactions within a neighbourhood (versus the complete object interaction graph in Battaglia et al.) based on euclideian distance  are novel to this work. However, the advantages (if any) of NPE versus the model of Battaglia et al. are not clear. Moreover, it is not clear how this neighbourhood thresholding scene would preform in case of n-ball systems, where gravitational forces of massive objects can be felt over large distances.

- citations 

This work includes all relevant citations.

- clarity

The article is well written and easy to understand.

- experiments 

Battaglia et al. evaluates on wider variety senerios compared to this work (e.g. n-bodies under gravitation, falling strings). Such experiments demonstrate the ability of the models to generalize. However, this work does include more in-depth experiments in case of bouncing balls compared to Battaglia et al. (e.g. mass estimation and varying world configurations with obstacles in the bouncing balls senerio). 

Moreover, an extensive comparison to Fragkiadaki et al. (2015) (in the bouncing balls senerios) is missing. The authors (referring to answer to question 4) do point out to comaprable numbers in both works, but the experimental settings are different.  Comparison in a billiard table senerio like that Fragkiadaki et al. (2015) where a initial force is applied to a ball, would have been enlightening. 

The authors only evaluate the error in velocity in the bouncing balls senerios. We understand that this model predicts only the velocity (refer to answer of question 2). Error analysis also with respect to ground truth ball position would be more enlightening. As small errors in velocity can quickly lead to entirely different scene configuration.

- conclusion / recommendation

The main issue with this work is the unclear novelty with respect to work of Battaglia et al. at NIPS'16. A quantitative and qualitative comparison with Battaglia et al. is lacking.  But the authors state that their work was developed independently.

Differentiable physics engines like NPE or that of Battaglia et al. (NIPS 2016) requires generation of an extensive amount of synthetic data to learn about the physics of a certain senerio. Moreover, extensive retraining is required to adapt to new sceneries (e.g. bouncing balls to n-body systems). Any practical advantage versus generating new code for a physics engine is not clear. Other ""bottom-up"" approaches like that of  Fragkiadaki et al. (2015) couple vision along with learning dynamics. However, they require very few input parameters (position, mass, current velocity, world configuration), as approximate parameter estimation can be done from the visual component.  Such approaches could be potentially more useful of a robot in ""common-sense"" everyday tasks (e.g. manipulation). Thus, overall potential applications of a differentiable physics engine like NPE is unclear."
5,"Summary
===
This paper proposes the Neural Physics Engine (NPE), a network architecture
which simulates object interactions. While NPE decides to explicitly represent
objects (rather than video frames), it incorporates knowledge of physics
almost exclusively through training data. It is tested in a toy domain with
bouncing 2d balls.

The proposed architecture processes each object in a scene one at a time.
Pairs of objects are embedded in a common space where the effect of the
objects on each other can be represented. These embeddings are summed
and combined with the focus object's state to predict the focus object's
change in velocity. Alternative baselines are presented which either
forego the pairwise embedding for a single object embedding or
encode a focus object's neighbors in a sequence of LSTM states.

NPE outperforms the baselines dramatically, showing the importance of
architecture choices in learning to do this object based simulation.
The model is tested in multiple ways. Ability to predict object trajectory
over long time spans is measured. Generalization to different numbers of objects
is measured. Generalization to slightly altered environments (difference
shaped walls) is measured. Finally, the NPE is also trained to predict
object mass using only interactions with other objects, where it also
outperforms baselines.


Comments
===

* I have one more clarifying question. Are the inputs to the blue box in
figure 3 (b)/(c) the concatenation of the summed embeddings and state vector
of object 3? Or is the input to the blue module some other combination of the
two vectors?


* Section 2.1 begins with ""First, because physics does not
change across inertial frames, it suffices to separately predict the future state of each object conditioned
on the past states of itself and the other objects in its neighborhood, similar to Fragkiadaki
et al. (2015).""

I think this is an argument to forego the visual representation used by previous
work in favor of an object only representation. This would be more clear if there
were contrast with a visual representation.


* As addressed in the paper, this approach is novel, though less so after taking
into consideration the concurrent work of Battaglia et. al. in NIPS 2016 titled
""Interaction Networks for Learning about Objects, Relations and Physics.""
This work offers a different network architecture and set of experiments, as
well as great presentation, but the use of an object based representation
for learning to predict physical behavior is shared.


Overall Evaluation
===

This paper was a pleasure to read and provided many experiments that offered
clear and interesting conclusions. It offers a novel approach (though
less so compared to the concurrent work of Battaglia et. al. 2016) which
represents a significant step forward in the current investigation of
intuitive physics."
4,"- summary

The paper proposes a differntiable Neural Physics Engine (NPE). The NPE consists of an encoder and a decoder function. The NPE takes as input the state of pairs of objects (within a neighbourhood of a focus object) at two previous time-steps in a scene. The encoder function summarizes the interaction of each pair of objects. The decoder then outputs the change in velocity of the focus object at the next time step. The NPE is evaluated on various environments containing bouncing balls.

- novelty

The differentiable NPE is a novel concept. However, concurrently Battaglia et al. (NIPS 2016) proposes a very similar model. Just as this work, Battaglia et al. (NIPS 2016) consider a model which consists of a encoder function (relation-centric) which encodes the interaction among a focus object and other objects in the scene and a decoder (relation-centric) function which considers the cumulative (encoded) effect of object interactions on the focus object and predicts effect of the interactions.  Aspects like only considering objects interactions within a neighbourhood (versus the complete object interaction graph in Battaglia et al.) based on euclideian distance  are novel to this work. However, the advantages (if any) of NPE versus the model of Battaglia et al. are not clear. Moreover, it is not clear how this neighbourhood thresholding scene would preform in case of n-ball systems, where gravitational forces of massive objects can be felt over large distances.

- citations 

This work includes all relevant citations.

- clarity

The article is well written and easy to understand.

- experiments 

Battaglia et al. evaluates on wider variety senerios compared to this work (e.g. n-bodies under gravitation, falling strings). Such experiments demonstrate the ability of the models to generalize. However, this work does include more in-depth experiments in case of bouncing balls compared to Battaglia et al. (e.g. mass estimation and varying world configurations with obstacles in the bouncing balls senerio). 

Moreover, an extensive comparison to Fragkiadaki et al. (2015) (in the bouncing balls senerios) is missing. The authors (referring to answer to question 4) do point out to comaprable numbers in both works, but the experimental settings are different.  Comparison in a billiard table senerio like that Fragkiadaki et al. (2015) where a initial force is applied to a ball, would have been enlightening. 

The authors only evaluate the error in velocity in the bouncing balls senerios. We understand that this model predicts only the velocity (refer to answer of question 2). Error analysis also with respect to ground truth ball position would be more enlightening. As small errors in velocity can quickly lead to entirely different scene configuration.

- conclusion / recommendation

The main issue with this work is the unclear novelty with respect to work of Battaglia et al. at NIPS'16. A quantitative and qualitative comparison with Battaglia et al. is lacking.  But the authors state that their work was developed independently.

Differentiable physics engines like NPE or that of Battaglia et al. (NIPS 2016) requires generation of an extensive amount of synthetic data to learn about the physics of a certain senerio. Moreover, extensive retraining is required to adapt to new sceneries (e.g. bouncing balls to n-body systems). Any practical advantage versus generating new code for a physics engine is not clear. Other ""bottom-up"" approaches like that of  Fragkiadaki et al. (2015) couple vision along with learning dynamics. However, they require very few input parameters (position, mass, current velocity, world configuration), as approximate parameter estimation can be done from the visual component.  Such approaches could be potentially more useful of a robot in ""common-sense"" everyday tasks (e.g. manipulation). Thus, overall potential applications of a differentiable physics engine like NPE is unclear."
5,"This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough that the authors apply the module to several neural network architectures and show improvements in performance.

Using k-nearest neighbors for memory access is not completely new. This has been recently explored in Rae et al., 2016 and Chandar et al., 2016. K-nearest neighbors based memory for one-shot learning has also been explored in [R1]. This paper provides experimental evidence that such an approach can be applied to a variety of architectures.

Authors have addressed all my pre-review questions and I am ok with their response.

Are the authors willing to release the source code to reproduce the results? At least for omniglot experiments and synthetic task experiments?

References:

[R1] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, Demis Hassabis: Model-Free Episodic Control. CoRR abs/1606.04460 (2016)
"
3,"The paper proposes a new memory module to be used as an addition to existing neural network models.

Pros:
* Clearly written and original idea.
* Useful memory module, shows nice improvements.
* Tested on some big tasks.

Cons:
* No comparisons to other memory modules such as associative LSTMs etc.
"
5,"This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough that the authors apply the module to several neural network architectures and show improvements in performance.

Using k-nearest neighbors for memory access is not completely new. This has been recently explored in Rae et al., 2016 and Chandar et al., 2016. K-nearest neighbors based memory for one-shot learning has also been explored in [R1]. This paper provides experimental evidence that such an approach can be applied to a variety of architectures.

Authors have addressed all my pre-review questions and I am ok with their response.

Are the authors willing to release the source code to reproduce the results? At least for omniglot experiments and synthetic task experiments?

References:

[R1] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, Demis Hassabis: Model-Free Episodic Control. CoRR abs/1606.04460 (2016)
"
3,"The paper proposes a new memory module to be used as an addition to existing neural network models.

Pros:
* Clearly written and original idea.
* Useful memory module, shows nice improvements.
* Tested on some big tasks.

Cons:
* No comparisons to other memory modules such as associative LSTMs etc.
"
5,"I think the problem here is well motivated, the approach is insightful and intuitive, and the results are convincing of the approach (although lacking in variety of applications). I like the fact that the authors use POS and NER in terms of an intermediate signal for the decision. Also they compare against a sufficient range of baselines to show the effectiveness of the proposed model.

I am also convinced by the authors' answers to my question, I think there is sufficient evidence provided in the results to show the effectiveness of the inductive bias introduced by the fine-grained gating model."
5,"I think the problem here is well motivated, the approach is insightful and intuitive, and the results are convincing of the approach (although lacking in variety of applications). I like the fact that the authors use POS and NER in terms of an intermediate signal for the decision. Also they compare against a sufficient range of baselines to show the effectiveness of the proposed model.

I am also convinced by the authors' answers to my question, I think there is sufficient evidence provided in the results to show the effectiveness of the inductive bias introduced by the fine-grained gating model."
3,"This is a good paper with an interesting probabilistic motivation for weighted bag of words models.
The (hopefully soon) added comparison to Wang and Manning will make it stronger. 
Though it is sad that for sufficiently large datasets, NB-SVM still works better.

In the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f.

Minor comments:

""The capturing the similarities"" -- typo in line 2 of intro.
""Recently, (Wieting et al.,2016) learned"" -- use citet instead of parenthesized citation
 "
4,"This paper proposes a simple way to reweight the word embedding in the simple composition function for sentence representation. This paper also shows the connection between this new weighting scheme and some previous work.

Here are some comments on technical details:

- The word ""discourse"" is confusing. I am not sure whether the words ""discourse"" in ""discourse vector c_s"" and the one in ""most frequent discourse"" have the same meaning.
- Is there any justification about $c_0$ related to syntac?
- Not sure what thie line means: ""In fact the new model was discovered by our detecting the common component c0 in existing embeddings."" in section ""Computing the sentence embedding""
- Is there any explanation about the results on sentiment in Table 2?"
3,"This is a good paper with an interesting probabilistic motivation for weighted bag of words models.
The (hopefully soon) added comparison to Wang and Manning will make it stronger. 
Though it is sad that for sufficiently large datasets, NB-SVM still works better.

In the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f.

Minor comments:

""The capturing the similarities"" -- typo in line 2 of intro.
""Recently, (Wieting et al.,2016) learned"" -- use citet instead of parenthesized citation
 "
4,"This paper proposes a simple way to reweight the word embedding in the simple composition function for sentence representation. This paper also shows the connection between this new weighting scheme and some previous work.

Here are some comments on technical details:

- The word ""discourse"" is confusing. I am not sure whether the words ""discourse"" in ""discourse vector c_s"" and the one in ""most frequent discourse"" have the same meaning.
- Is there any justification about $c_0$ related to syntac?
- Not sure what thie line means: ""In fact the new model was discovered by our detecting the common component c0 in existing embeddings."" in section ""Computing the sentence embedding""
- Is there any explanation about the results on sentiment in Table 2?"
2,"This paper performs a very important service: exploring in a clear and systematic way the performance and trainability characteristics of a set of neural network architectures -- in particular, the basic RNN motifs that have recently been popular.   

Pros:

* This paper addresses an important question I and many others would have liked to know the answer to but didn't have the computational resources to thoroughly attack it.   This is a nice use of Google's resources to help the community. 

* The work appears to have been done carefully so that the results can be believed.

* The basic answer arrived at (that, in the ""typical training environment"" LSTMs are reliable but basically GRUs are the answer) seems fairly decisive and practically useful.   Of course the real answer is more complicated than my little summary here, but the subtleties are discussed nicely in the paper.

* The insistence on a strong distinction between capacity and trainability helps nicely clear up a misconception about the reasons why gated architectures work.  In sum, they're much more easily trainable but somewhat lower capacity than vanilla RNNs, and in hard tasks, the benefits of better trainability far outweigh the costs of mildly lower capacity.

* The point about the near-equivalence of capacity at equal numbers of parameters is very useful.   

* The paper makes it clear the importance of HP tuning, something that has sometimes gotten lost in the vast flow of papers about new architectures.

* The idea of quantifying the fraction of infeasible parameters (e.g. those that diverge) is nice, because it's a practical problem that everyone working with these networks has but often isn't addressed. 

* The paper text is very clearly written.

Cons:

* The work on the UGRNNs and the +RNNs seems a bit preliminary.  I don't think that the authors have clearly shown that the +RNN should be ""recommended"" with the same generality as the GRU.   I'd at the least want some better statistics on the significance of differences between +RNN and GRU performances quantifying the results in Figure 4 (8-layer panel).   In a way the high standards for declaring an architecture useful that are set in the paper make the UGRNNs and +RNN contributions seem less important.   I don't really mind having them in the paper though.   I guess the point of this paper is not really to be novel in the first place -- which is totally fine with me, though I don't know what the ICLR area chairs will think. 

* The paper gives short shrift to the details of the HP algorithm itself.  They do say: 

     ""Our setting of the tuners internal parameters was such that it uses Batched GP Bandits with an expected improvement acquisition function and a   
     Matern 5/2 Kernel with feature scaling and automatic relevance determination performed by optimizing over kernel HPs""  

and give some good references, but I expect that actually trying to replicate this involves a lot of missing details.   

* I found some of the figures a bit hard to read at first, esp. Fig 4, mostly due to the panels being small, having a lot of details, and bad choices for visual cleanliness.   

* The neuroscience reference (""4.7 bits per synapse"") seems a little bit of a throw-away to me, because the connection between these results and the experimental neuroscience is very tenuous, or at any rate, not well explained.  I guess it's just in the discussion, but it seems gratuitous.   Maybe it should couched in slightly less strong terms (nothing is really strongly shown to be ""in agreement"" here between computational architectures and neuroscience, but perhaps they could say something like -- ""We wonder if it is anything other than coincidence that our 5 bits result is numerically similar to the 4.7 bits measurement from neuroscience."")



"
2,"This paper performs a very important service: exploring in a clear and systematic way the performance and trainability characteristics of a set of neural network architectures -- in particular, the basic RNN motifs that have recently been popular.   

Pros:

* This paper addresses an important question I and many others would have liked to know the answer to but didn't have the computational resources to thoroughly attack it.   This is a nice use of Google's resources to help the community. 

* The work appears to have been done carefully so that the results can be believed.

* The basic answer arrived at (that, in the ""typical training environment"" LSTMs are reliable but basically GRUs are the answer) seems fairly decisive and practically useful.   Of course the real answer is more complicated than my little summary here, but the subtleties are discussed nicely in the paper.

* The insistence on a strong distinction between capacity and trainability helps nicely clear up a misconception about the reasons why gated architectures work.  In sum, they're much more easily trainable but somewhat lower capacity than vanilla RNNs, and in hard tasks, the benefits of better trainability far outweigh the costs of mildly lower capacity.

* The point about the near-equivalence of capacity at equal numbers of parameters is very useful.   

* The paper makes it clear the importance of HP tuning, something that has sometimes gotten lost in the vast flow of papers about new architectures.

* The idea of quantifying the fraction of infeasible parameters (e.g. those that diverge) is nice, because it's a practical problem that everyone working with these networks has but often isn't addressed. 

* The paper text is very clearly written.

Cons:

* The work on the UGRNNs and the +RNNs seems a bit preliminary.  I don't think that the authors have clearly shown that the +RNN should be ""recommended"" with the same generality as the GRU.   I'd at the least want some better statistics on the significance of differences between +RNN and GRU performances quantifying the results in Figure 4 (8-layer panel).   In a way the high standards for declaring an architecture useful that are set in the paper make the UGRNNs and +RNN contributions seem less important.   I don't really mind having them in the paper though.   I guess the point of this paper is not really to be novel in the first place -- which is totally fine with me, though I don't know what the ICLR area chairs will think. 

* The paper gives short shrift to the details of the HP algorithm itself.  They do say: 

     ""Our setting of the tuners internal parameters was such that it uses Batched GP Bandits with an expected improvement acquisition function and a   
     Matern 5/2 Kernel with feature scaling and automatic relevance determination performed by optimizing over kernel HPs""  

and give some good references, but I expect that actually trying to replicate this involves a lot of missing details.   

* I found some of the figures a bit hard to read at first, esp. Fig 4, mostly due to the panels being small, having a lot of details, and bad choices for visual cleanliness.   

* The neuroscience reference (""4.7 bits per synapse"") seems a little bit of a throw-away to me, because the connection between these results and the experimental neuroscience is very tenuous, or at any rate, not well explained.  I guess it's just in the discussion, but it seems gratuitous.   Maybe it should couched in slightly less strong terms (nothing is really strongly shown to be ""in agreement"" here between computational architectures and neuroscience, but perhaps they could say something like -- ""We wonder if it is anything other than coincidence that our 5 bits result is numerically similar to the 4.7 bits measurement from neuroscience."")



"
5,"The paper presents a novel strategy to deal with dynamic computation graphs. They arise, when the computation is dynamically influenced by the input data, such as in LSTMs. The authors propose an `unrolling' strategy over the operations done at every step, which allows a new kind of batching of inputs.

The presented idea is novel and the results clearly indicate the potential of the approach. For the sake of clarity of the presentation I would drop parts of Section 3 (""A combinator library for neural networks"") which presents technical details that are in general interesting, but do not help the understanding of the core idea of the paper. The presented experimental results on the ""Stanford Sentiment Treebank"" are in my opinion not supporting the claim of the paper, which is towards speed, than a little bit confusing. It is important to point out that even though the presented ensemble ""[...] variant sets a new state-of-the-art on both subtasks"" [p. 8], this is not due to the framework, not even due to the model (comp. lines 4 and 2 of Tab. 2), but probably, and this can only be speculated about, due to the ensemble averaging. I would appreciate a clearer argumentation in this respect.

Update on Jan. 17th:
after the authors update for their newest revision, I increase my rating to 8 due to the again improved, now very clear argumentation."
5,"The paper describes a novel technique to improve the efficiency of computation graphs in deep learning frameworks. An impressive speedup can be observed in their implementation within TensorFlow. The content is presented with sufficient clarity, although some more graphical illustrations could be useful. This work is relevant in order to achieve highest performance in neural network training.


Pros:

- significant speed improvements through dynamic batching
- source code provided


Cons:

- the effect on a large real-world (ASR, SMT) would allow the reader to put the improvements better into context
- presentation/vizualisation can be improved "
5,"The paper presents a novel strategy to deal with dynamic computation graphs. They arise, when the computation is dynamically influenced by the input data, such as in LSTMs. The authors propose an `unrolling' strategy over the operations done at every step, which allows a new kind of batching of inputs.

The presented idea is novel and the results clearly indicate the potential of the approach. For the sake of clarity of the presentation I would drop parts of Section 3 (""A combinator library for neural networks"") which presents technical details that are in general interesting, but do not help the understanding of the core idea of the paper. The presented experimental results on the ""Stanford Sentiment Treebank"" are in my opinion not supporting the claim of the paper, which is towards speed, than a little bit confusing. It is important to point out that even though the presented ensemble ""[...] variant sets a new state-of-the-art on both subtasks"" [p. 8], this is not due to the framework, not even due to the model (comp. lines 4 and 2 of Tab. 2), but probably, and this can only be speculated about, due to the ensemble averaging. I would appreciate a clearer argumentation in this respect.

Update on Jan. 17th:
after the authors update for their newest revision, I increase my rating to 8 due to the again improved, now very clear argumentation."
5,"The paper describes a novel technique to improve the efficiency of computation graphs in deep learning frameworks. An impressive speedup can be observed in their implementation within TensorFlow. The content is presented with sufficient clarity, although some more graphical illustrations could be useful. This work is relevant in order to achieve highest performance in neural network training.


Pros:

- significant speed improvements through dynamic batching
- source code provided


Cons:

- the effect on a large real-world (ASR, SMT) would allow the reader to put the improvements better into context
- presentation/vizualisation can be improved "
5,"The submission explores several alternatives to provide the generator function in generative adversarial training with additional gradient information. The exposition starts by describing a general formulation about how this additional gradient information (termed K(p_gen) could be added to the generative adversarial training objective function (Equation 1). Next, the authors prove that the shape of the optimal discriminator does indeed depend on the added gradient information (Proposition 3.1), which is unsurprising. Finally, the authors propose three particular alternatives to construct K(p_gen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function (which resembles the EBGAN objective of Zhao et al, 2016).

The exposition moves then to an experimental evaluation of the method, which sets K(p_gen) to be the approximate entropy of the generator distribution. At this point, my intuition is that the objective function under study is the vanilla GAN objective, plus a regularization term that encourages diversity (high entropy) in the generator distribution. The hope of the authors is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution.

The experimental evaluation proceeds by 1) showing the contour plots of the obtained generator distribution for a 2D problem, 2) studying the generation diversity in MNIST digits, and 3) showing some samples for CIFAR-10 and CelebA. The 2D problem results are convincing, since one can clearly observe that the discriminator scores translate into unnormalized values of the density function. The MNIST results offer good intuition also: the more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, and the less prototypical digits are assigned smaller scores. The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison.

To this end, I would recommend the authors to clarify three aspects. First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution. But, how does this regularization reshape the generator function? It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible. Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the art? Third, what are the *shortcomings* of this method versus vanilla GAN? Too much computational overhead? What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript?

Overall, a clearly written paper. I vote for acceptance.

As an open question to the authors: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?

"
5,"The submission explores several alternatives to provide the generator function in generative adversarial training with additional gradient information. The exposition starts by describing a general formulation about how this additional gradient information (termed K(p_gen) could be added to the generative adversarial training objective function (Equation 1). Next, the authors prove that the shape of the optimal discriminator does indeed depend on the added gradient information (Proposition 3.1), which is unsurprising. Finally, the authors propose three particular alternatives to construct K(p_gen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function (which resembles the EBGAN objective of Zhao et al, 2016).

The exposition moves then to an experimental evaluation of the method, which sets K(p_gen) to be the approximate entropy of the generator distribution. At this point, my intuition is that the objective function under study is the vanilla GAN objective, plus a regularization term that encourages diversity (high entropy) in the generator distribution. The hope of the authors is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution.

The experimental evaluation proceeds by 1) showing the contour plots of the obtained generator distribution for a 2D problem, 2) studying the generation diversity in MNIST digits, and 3) showing some samples for CIFAR-10 and CelebA. The 2D problem results are convincing, since one can clearly observe that the discriminator scores translate into unnormalized values of the density function. The MNIST results offer good intuition also: the more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, and the less prototypical digits are assigned smaller scores. The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison.

To this end, I would recommend the authors to clarify three aspects. First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution. But, how does this regularization reshape the generator function? It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible. Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the art? Third, what are the *shortcomings* of this method versus vanilla GAN? Too much computational overhead? What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript?

Overall, a clearly written paper. I vote for acceptance.

As an open question to the authors: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?

"
3,"Authors propose a strategy for pruning weights with the eventual goal of reducing GFLOP computations. The pruning strategy is well motivated using the taylor expansion of the neural network function with respect to the feature activations. The obtained strategy removes feature maps that have both a small activation and a small gradient (eqn 7). 

(A) Ideally the gradient of the output with respect to the activation functions should be 0 at the optimal, but as a result of stochastic gradient evaluations this would practically never be zero. Small variance in the gradient across mini-batches indicates that irrespective of input data the specific network parameter is unlikely to change - intuitively these are parameters that are closer to convergence. Parameters/weights that are close to convergence and also result in a small activation are intuitively good candidates for pruning. This is essentially what eqn 7 conveys and is likely to be reason why just removing weights that result in small activations is not as good of a pruning strategy (as shown by results in the paper). There are two kind of differences in weights that are removed by activation v/s taylor expansion:
1. Weights with high-activations but very low gradients will be removed by taylor expansion, but not by activation alone. 
2. Weights with low-activation but high gradients will be removed by activation criterion, but not by taylor expansion. 
It will be interesting to analyze which of (1) or (2) contribute more to the differences in weights that are removed by the taylor expansion v/s activation criterion. Intuitively it seems that weight that satisfy (1) are important because they are converged and contribute significantly to network's activation. It is possible that a modified criterion - eqn (7) + \lambda feature activation, (where \lambda needs to be found by cross-validation) may lead to even better results at the cost of more parameter tuning. 
  
(B) Another interesting comparison is with the with the optimal damage framework - where the first order gradients are assumed to be zero and pruning is performed using the second-order information (also discussed by authors in the appendix). Critically, only the diagonal of the Hessian is computed. There is no comparison with optimal damage as authors claim it is memory and computation inefficient. Back of envelope calculations suggest that this would result only in 50% increase in memory and computation during pruning, but no loss in efficiency during testing. Therefore from a standpoint of deployment, I don't think this missing comparison is justified. 

(C) The eventual goal of the authors is to reduce GFLOPs. Some recent papers have proposed using lower precision computation for this. A comparison in GFLOPs with lower precision v/s pruning would be a great. While both these approaches are complementary and it is expected that combining both of them can lead to superior performance than either of the two - it is unclear when we are operating in the low-precision regime how much pruning can be performed. Any analysis on this tradeoff would be great (but not necessary).

(D) On finetuning, authors report results of AlexNet and VGG on two different datasets - Flowers and Birds respectively. Why is this the case? It would be great to see the results of both the networks on both the datasets. 

(E) Authors report there is only a small drop in performance after pruning. Suppose the network was originally trained with N iterations, and then M finetuning iterations were performed during pruning. This means that pruned networks were trained for N + M iterations. The correct comparison in accuracies would be if we the original network was also trained for N + M iterations. In figure 4, does the performance at 100% parameters reports accuracy after N+M iterations or after N iterations? 

Overall I think the paper is technically and empirically sound, it proposes a new strategy for pruning:
(1) Based on taylor expansion
(2) Feature normalization to reduce parameter tuning efforts. 
(3) Iterative finetuning. 
However, I would like to see some comparisons mentioned in my comments above. If those comparisons are made I would change my ratings to an accept. 







"
5,"This paper presents a novel way of pruning filters from convolutional neural networks with a strong theoretical justification. The proposed methods is derived from the first order Taylor expansion of the loss change while pruning a particular unit. This leads to simple weighting of the unit activation with its gradient w.r.t. loss function and performs better than simply using the activation magnitude as the heuristic for pruning. This intuitively makes sense, as we would like to remove not only the filters with low activation, but also filters where the incorrect activation value would not have small influence on the target loss.

Authors thoroughly investigate multiple baselines, including an oracle which sets an upper bound on the target performance even though it is computationally expensive. The devised method seems to be quite elegant and authors show that it generalizes well on multiple tasks and is computationally more than feasible as it is easy to combine with traditional fine tuning procedure. Also, the work clearly shows the trade-offs of increased speed and decreased performance, which is useful for practical applications.

It would be also useful to compare against different baselines, e.g. [1]. However this method seems to be more useful as it does not involve training of a new network (and thus is probably much faster).

Suggestion - maybe it can be extended in the future towards also removing only parts of the filters(e.g. for the 3D convolution)? This may be more complicated as it would need to change the implementation of convolution operator, but can lead to further speedup.

[1] "
3,"Authors propose a strategy for pruning weights with the eventual goal of reducing GFLOP computations. The pruning strategy is well motivated using the taylor expansion of the neural network function with respect to the feature activations. The obtained strategy removes feature maps that have both a small activation and a small gradient (eqn 7). 

(A) Ideally the gradient of the output with respect to the activation functions should be 0 at the optimal, but as a result of stochastic gradient evaluations this would practically never be zero. Small variance in the gradient across mini-batches indicates that irrespective of input data the specific network parameter is unlikely to change - intuitively these are parameters that are closer to convergence. Parameters/weights that are close to convergence and also result in a small activation are intuitively good candidates for pruning. This is essentially what eqn 7 conveys and is likely to be reason why just removing weights that result in small activations is not as good of a pruning strategy (as shown by results in the paper). There are two kind of differences in weights that are removed by activation v/s taylor expansion:
1. Weights with high-activations but very low gradients will be removed by taylor expansion, but not by activation alone. 
2. Weights with low-activation but high gradients will be removed by activation criterion, but not by taylor expansion. 
It will be interesting to analyze which of (1) or (2) contribute more to the differences in weights that are removed by the taylor expansion v/s activation criterion. Intuitively it seems that weight that satisfy (1) are important because they are converged and contribute significantly to network's activation. It is possible that a modified criterion - eqn (7) + \lambda feature activation, (where \lambda needs to be found by cross-validation) may lead to even better results at the cost of more parameter tuning. 
  
(B) Another interesting comparison is with the with the optimal damage framework - where the first order gradients are assumed to be zero and pruning is performed using the second-order information (also discussed by authors in the appendix). Critically, only the diagonal of the Hessian is computed. There is no comparison with optimal damage as authors claim it is memory and computation inefficient. Back of envelope calculations suggest that this would result only in 50% increase in memory and computation during pruning, but no loss in efficiency during testing. Therefore from a standpoint of deployment, I don't think this missing comparison is justified. 

(C) The eventual goal of the authors is to reduce GFLOPs. Some recent papers have proposed using lower precision computation for this. A comparison in GFLOPs with lower precision v/s pruning would be a great. While both these approaches are complementary and it is expected that combining both of them can lead to superior performance than either of the two - it is unclear when we are operating in the low-precision regime how much pruning can be performed. Any analysis on this tradeoff would be great (but not necessary).

(D) On finetuning, authors report results of AlexNet and VGG on two different datasets - Flowers and Birds respectively. Why is this the case? It would be great to see the results of both the networks on both the datasets. 

(E) Authors report there is only a small drop in performance after pruning. Suppose the network was originally trained with N iterations, and then M finetuning iterations were performed during pruning. This means that pruned networks were trained for N + M iterations. The correct comparison in accuracies would be if we the original network was also trained for N + M iterations. In figure 4, does the performance at 100% parameters reports accuracy after N+M iterations or after N iterations? 

Overall I think the paper is technically and empirically sound, it proposes a new strategy for pruning:
(1) Based on taylor expansion
(2) Feature normalization to reduce parameter tuning efforts. 
(3) Iterative finetuning. 
However, I would like to see some comparisons mentioned in my comments above. If those comparisons are made I would change my ratings to an accept. 







"
5,"This paper presents a novel way of pruning filters from convolutional neural networks with a strong theoretical justification. The proposed methods is derived from the first order Taylor expansion of the loss change while pruning a particular unit. This leads to simple weighting of the unit activation with its gradient w.r.t. loss function and performs better than simply using the activation magnitude as the heuristic for pruning. This intuitively makes sense, as we would like to remove not only the filters with low activation, but also filters where the incorrect activation value would not have small influence on the target loss.

Authors thoroughly investigate multiple baselines, including an oracle which sets an upper bound on the target performance even though it is computationally expensive. The devised method seems to be quite elegant and authors show that it generalizes well on multiple tasks and is computationally more than feasible as it is easy to combine with traditional fine tuning procedure. Also, the work clearly shows the trade-offs of increased speed and decreased performance, which is useful for practical applications.

It would be also useful to compare against different baselines, e.g. [1]. However this method seems to be more useful as it does not involve training of a new network (and thus is probably much faster).

Suggestion - maybe it can be extended in the future towards also removing only parts of the filters(e.g. for the 3D convolution)? This may be more complicated as it would need to change the implementation of convolution operator, but can lead to further speedup.

[1] "
4,"The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al. that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps. 
The paper claims that this 
a) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method
b) improves performance on texture inpainting tasks compared to the Gatys et al. method
c) improves results in season transfer when combined with the style transfer method by Gatys et al. 
Furthermore the paper shows that
d) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved.

I agree with claim a). However, the generated textures still have some issues such as greyish regions so the problem is not solved. Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower. For example, in comparison, the concurrent work by Liu et al. ("
5,"This paper proposes a modification of the parametric texture synthesis model of Gatys et al. to take into account long-range correlations of textures. To this end the authors add the Gram matrices between spatially shifted feature vectors to the synthesis loss. Some of the synthesised textures are visually superior to the original Gatys et al. method, in particular on textures with very structured long-range correlations (such as bricks).

The paper is well written, the method and intuitions are clearly exposed and the authors perform quite a wide range of synthesis experiments on different textures.

My only concern, which is true for all methods including Gatys et al., is the variability of the samples. Clearly the global minimum of the proposed objective is the original image itself. This issue is partially circumvented by performing inpainting experiments, by which the synthesised paths needs to stay coherent with the borders (as the authors did). There are no additional insights into this problem in this paper, which would have been a plus.

All in all, this work is a simple and nice modification of Gatys at al. which is worth publishing but does not constitute a major breakthrough.


"
4,"The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al. that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps. 
The paper claims that this 
a) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method
b) improves performance on texture inpainting tasks compared to the Gatys et al. method
c) improves results in season transfer when combined with the style transfer method by Gatys et al. 
Furthermore the paper shows that
d) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved.

I agree with claim a). However, the generated textures still have some issues such as greyish regions so the problem is not solved. Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower. For example, in comparison, the concurrent work by Liu et al. ("
5,"This paper proposes a modification of the parametric texture synthesis model of Gatys et al. to take into account long-range correlations of textures. To this end the authors add the Gram matrices between spatially shifted feature vectors to the synthesis loss. Some of the synthesised textures are visually superior to the original Gatys et al. method, in particular on textures with very structured long-range correlations (such as bricks).

The paper is well written, the method and intuitions are clearly exposed and the authors perform quite a wide range of synthesis experiments on different textures.

My only concern, which is true for all methods including Gatys et al., is the variability of the samples. Clearly the global minimum of the proposed objective is the original image itself. This issue is partially circumvented by performing inpainting experiments, by which the synthesised paths needs to stay coherent with the borders (as the authors did). There are no additional insights into this problem in this paper, which would have been a plus.

All in all, this work is a simple and nice modification of Gatys at al. which is worth publishing but does not constitute a major breakthrough.


"
5,"Summary: The paper proposes a novel deep neural network architecture for the task of question answering on the SQuAD dataset. The model consists of two main components -- coattention encoder and dynamic pointer decoder. The encoder produces attention over the question as well as over the document in parallel and thus learns co-dependent representations of the question and the document. The decoder predicts the starting and the end token of the answer iteratively, with the motivation that multiple iterations will help the model escape local maxima and thus will reduce the errors made by the model. The proposed model achieved state-of-art result on SQuAD dataset at the time of writing the paper. The paper reports some analyses of the results such as performance across question types, document, question, answer lengths, etc. The paper also performs some ablation studies such as performing only single round of iteration on decoder, etc.

Strengths:

1. The paper is well-motivated with two main motivations -- co-attending to the document and the question, and iteratively producing the answer.

2. The proposed model architecture is novel and the design choices made seem reasonable.

3. The experiments show that the proposed model outperforms the existing model (at the time of writing the paper) on the SQuAD dataset by significant margin.

4. The analyses of the results and the ablation studies performed (as per someone's request) provide insights into the various modelling design choices made.

Weaknesses/Questions/Suggestions:

1. In order to gain insights into how much each additional iteration in the decoder help, I would like to see the following -- for every iteration report the mean F1 for questions that converged in that iteration along with the number of questions that converged in that iteration.

2. Example of Question 3 in figure 5 is an interesting example where the model is unable to decide between multiple local maxima despite several iterations. Could authors please report how often this happens?

3. In order to estimate how much modelling of attention in the encoder helps, it would be good if authors could report the performance of the model when attention is not modeled at all in the encoder (neither over question, nor over document).

4. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.

5. In Wang and Jiang (2016), the attention is predicted over question for each word in the document. But in table 2, when performing ablation study to make the proposed model similar to Wang and Jiang, C^D is set to C^Q. But isnt C^Q attention over document for each word in the question? So, how is this similar to Wang and Jiangs attention? I think QA^D will be similar to Wang and Jiang's attention since QA^D is attention over question for each word in the document. Please clarify.

6. In section 2.1, n and m are swapped when explaining the Document and Question encoding matrix. Please fix it.

Review Summary: The paper presents a novel and interesting model for the task of question answering on SQuAD dataset and shows that the model outperforms existing models. However, to gain more insights into the functioning of the model, I would like see more analyses of the results and one more ablation study (see weaknesses section above).
"
5,"Summary: The paper proposes a novel deep neural network architecture for the task of question answering on the SQuAD dataset. The model consists of two main components -- coattention encoder and dynamic pointer decoder. The encoder produces attention over the question as well as over the document in parallel and thus learns co-dependent representations of the question and the document. The decoder predicts the starting and the end token of the answer iteratively, with the motivation that multiple iterations will help the model escape local maxima and thus will reduce the errors made by the model. The proposed model achieved state-of-art result on SQuAD dataset at the time of writing the paper. The paper reports some analyses of the results such as performance across question types, document, question, answer lengths, etc. The paper also performs some ablation studies such as performing only single round of iteration on decoder, etc.

Strengths:

1. The paper is well-motivated with two main motivations -- co-attending to the document and the question, and iteratively producing the answer.

2. The proposed model architecture is novel and the design choices made seem reasonable.

3. The experiments show that the proposed model outperforms the existing model (at the time of writing the paper) on the SQuAD dataset by significant margin.

4. The analyses of the results and the ablation studies performed (as per someone's request) provide insights into the various modelling design choices made.

Weaknesses/Questions/Suggestions:

1. In order to gain insights into how much each additional iteration in the decoder help, I would like to see the following -- for every iteration report the mean F1 for questions that converged in that iteration along with the number of questions that converged in that iteration.

2. Example of Question 3 in figure 5 is an interesting example where the model is unable to decide between multiple local maxima despite several iterations. Could authors please report how often this happens?

3. In order to estimate how much modelling of attention in the encoder helps, it would be good if authors could report the performance of the model when attention is not modeled at all in the encoder (neither over question, nor over document).

4. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.

5. In Wang and Jiang (2016), the attention is predicted over question for each word in the document. But in table 2, when performing ablation study to make the proposed model similar to Wang and Jiang, C^D is set to C^Q. But isnt C^Q attention over document for each word in the question? So, how is this similar to Wang and Jiangs attention? I think QA^D will be similar to Wang and Jiang's attention since QA^D is attention over question for each word in the document. Please clarify.

6. In section 2.1, n and m are swapped when explaining the Document and Question encoding matrix. Please fix it.

Review Summary: The paper presents a novel and interesting model for the task of question answering on SQuAD dataset and shows that the model outperforms existing models. However, to gain more insights into the functioning of the model, I would like see more analyses of the results and one more ablation study (see weaknesses section above).
"
4,"The paper proposed a novel SampleRNN to directly model waveform signals and achieved better performance both in terms of objective test NLL and subjective A/B tests. 

As mentioned in the discussions, the current status of the paper lack plenty of details in describing their model. Hopefully, this will be addressed in the final version.

The authors attempted to compare with wavenet model, but they didn't manage to get a model better than the baseline LSTM-RNN, which makes all the comparisons to wavenets less convincing. Hence, instead of wasting time and space comparing to wavenet, detailing the proposed model would be better. "
5,"The paper introduces SampleRNN, a hierarchical recurrent neural network model of raw audio. The model is trained end-to-end and evaluated using log-likelihood and by human judgement of unconditional samples, on three different datasets covering speech and music. This evaluation shows the proposed model to compare favourably to the baselines.

It is shown that the subsequence length used for truncated BPTT affects performance significantly, but interestingly, a subsequence length of 512 samples (~32 ms) is sufficient to get good results, even though the features of the data that are modelled span much longer timescales. This is an interesting and somewhat unintuitive result that I think warrants a bit more discussion.

The authors have attempted to reimplement WaveNet, an alternative model of raw audio that is fully convolutional. They were unable to reproduce the exact model architecture from the original paper, but have attempted to build an instance of the model with a receptive field of about 250ms that could be trained in a reasonable time using their computational resources, which is commendable.

The architecture of the Wavenet model is described in detail, but it found it challenging to find the same details for the proposed SampleRNN architecture (e.g. which value of ""r"" is used for the different tiers, how many units per layer, ...). I think a comparison in terms of computational cost, training time and number of parameters would also be very informative.

Surprisingly, Table 1 shows a vanilla RNN (LSTM) substantially outperforming this model in terms of likelihood, which is quite suspicious as LSTMs tend to have effective receptive fields of a few hundred timesteps at best. One would expect the much larger receptive field of the Wavenet model to be reflected in the likelihood scores to some extent. Similarly, Figure 3 shows the vanilla RNN outperforming the Wavenet reimplementation in human evaluation on the Blizzard dataset. This raises questions about the implementation of the latter. Some discussion about this result and whether the authors expected it or not would be very welcome.

Table 1 and Figure 4 also show the 2-tier SampleRNN outperforming the 3-tier model in terms of likelihood and human rating respectively, which is very counterintuitive as one would expect longer-range temporal correlations to be even more relevant for music than for speech. This is not discussed at all, I think it would be useful to comment on why this could be happening.

Overall, this an interesting attempt to tackle modelling very long sequences with long-range temporal correlations and the results are quite convincing, even if the same can't always be said of the comparison with the baselines. It would be interesting to see how the model performs for conditional generation, seeing as it can be more easily be objectively compared to models like Wavenet in that domain.



Other remarks:

- upsampling the output of the models is done with r separate linear projections. This choice of upsampling method is not motivated. Why not just use linear interpolation or nearest neighbour upsampling? What is the advantage of learning this operation? Don't the r linear projections end up learning largely the same thing, give or take some noise?

- The third paragraph of Section 2.1.1 indicates that 8-bit linear PCM was used. This is in contrast to Wavenet, for which an 8-bit mu-law encoding was used, and this supposedly improves the audio fidelity of the samples. Did you try this as well?

- Section 2.1 mentions the discretisation of the input and the use of a softmax to model this discretised input, without any reference to prior work that made the same observation. A reference is given in 2.1.1, but it should probably be moved up a bit to avoid giving the impression that this is a novel observation.
"
3,"Pros:
The authors are presenting an RNN-based alternative to wavenet, for generating audio a sample at a time.
RNNs are a natural candidate for this task so this is an interesting alternative. Furthermore the authors claim to make significant improvement in the quality of the produces samples.
Another novelty here is that they use a quantitative likelihood-based measure to assess them model, in addition to the AB human comparisons used in the wavenet work.

Cons:
The paper is lacking equations that detail the model. This can be remedied in the camera-ready version.
The paper is lacking detailed explanations of the modeling choices:
- It's not clear why an MLP is used in the bottom layer instead of (another) RNN.
- It's not clear why r linear projections are used for up-sampling, instead of feeding the same state to all r samples, or use a more powerful type of transformation. 
As the authors admit, their wavenet implementation is probably not as good as the original one, which makes the comparisons questionable. 

Despite the cons and given that more modeling details are provided, I think this paper will be a valuable contribution. 
"
4,"The paper proposed a novel SampleRNN to directly model waveform signals and achieved better performance both in terms of objective test NLL and subjective A/B tests. 

As mentioned in the discussions, the current status of the paper lack plenty of details in describing their model. Hopefully, this will be addressed in the final version.

The authors attempted to compare with wavenet model, but they didn't manage to get a model better than the baseline LSTM-RNN, which makes all the comparisons to wavenets less convincing. Hence, instead of wasting time and space comparing to wavenet, detailing the proposed model would be better. "
5,"The paper introduces SampleRNN, a hierarchical recurrent neural network model of raw audio. The model is trained end-to-end and evaluated using log-likelihood and by human judgement of unconditional samples, on three different datasets covering speech and music. This evaluation shows the proposed model to compare favourably to the baselines.

It is shown that the subsequence length used for truncated BPTT affects performance significantly, but interestingly, a subsequence length of 512 samples (~32 ms) is sufficient to get good results, even though the features of the data that are modelled span much longer timescales. This is an interesting and somewhat unintuitive result that I think warrants a bit more discussion.

The authors have attempted to reimplement WaveNet, an alternative model of raw audio that is fully convolutional. They were unable to reproduce the exact model architecture from the original paper, but have attempted to build an instance of the model with a receptive field of about 250ms that could be trained in a reasonable time using their computational resources, which is commendable.

The architecture of the Wavenet model is described in detail, but it found it challenging to find the same details for the proposed SampleRNN architecture (e.g. which value of ""r"" is used for the different tiers, how many units per layer, ...). I think a comparison in terms of computational cost, training time and number of parameters would also be very informative.

Surprisingly, Table 1 shows a vanilla RNN (LSTM) substantially outperforming this model in terms of likelihood, which is quite suspicious as LSTMs tend to have effective receptive fields of a few hundred timesteps at best. One would expect the much larger receptive field of the Wavenet model to be reflected in the likelihood scores to some extent. Similarly, Figure 3 shows the vanilla RNN outperforming the Wavenet reimplementation in human evaluation on the Blizzard dataset. This raises questions about the implementation of the latter. Some discussion about this result and whether the authors expected it or not would be very welcome.

Table 1 and Figure 4 also show the 2-tier SampleRNN outperforming the 3-tier model in terms of likelihood and human rating respectively, which is very counterintuitive as one would expect longer-range temporal correlations to be even more relevant for music than for speech. This is not discussed at all, I think it would be useful to comment on why this could be happening.

Overall, this an interesting attempt to tackle modelling very long sequences with long-range temporal correlations and the results are quite convincing, even if the same can't always be said of the comparison with the baselines. It would be interesting to see how the model performs for conditional generation, seeing as it can be more easily be objectively compared to models like Wavenet in that domain.



Other remarks:

- upsampling the output of the models is done with r separate linear projections. This choice of upsampling method is not motivated. Why not just use linear interpolation or nearest neighbour upsampling? What is the advantage of learning this operation? Don't the r linear projections end up learning largely the same thing, give or take some noise?

- The third paragraph of Section 2.1.1 indicates that 8-bit linear PCM was used. This is in contrast to Wavenet, for which an 8-bit mu-law encoding was used, and this supposedly improves the audio fidelity of the samples. Did you try this as well?

- Section 2.1 mentions the discretisation of the input and the use of a softmax to model this discretised input, without any reference to prior work that made the same observation. A reference is given in 2.1.1, but it should probably be moved up a bit to avoid giving the impression that this is a novel observation.
"
3,"Pros:
The authors are presenting an RNN-based alternative to wavenet, for generating audio a sample at a time.
RNNs are a natural candidate for this task so this is an interesting alternative. Furthermore the authors claim to make significant improvement in the quality of the produces samples.
Another novelty here is that they use a quantitative likelihood-based measure to assess them model, in addition to the AB human comparisons used in the wavenet work.

Cons:
The paper is lacking equations that detail the model. This can be remedied in the camera-ready version.
The paper is lacking detailed explanations of the modeling choices:
- It's not clear why an MLP is used in the bottom layer instead of (another) RNN.
- It's not clear why r linear projections are used for up-sampling, instead of feeding the same state to all r samples, or use a more powerful type of transformation. 
As the authors admit, their wavenet implementation is probably not as good as the original one, which makes the comparisons questionable. 

Despite the cons and given that more modeling details are provided, I think this paper will be a valuable contribution. 
"
4,"This paper introduces an approach to reinforcement learning and control wherein, rather than training a single controller to perform a task, a metacontroller with access to a base-level controller and a number of accessory  experts  is utilized. The job of the metacontroller is to decide how many times to call the controller and the experts, and which expert to invoke at which iteration. (The controller is a bit special in that in addition to being provided the current state, it is given a summary of the history of previous calls to itself and previous experts.) The sequence of controls and expert advice is embedded into a fixed-size vector through an LSTM. The method is tested on an N-body  control task, where it is shown that there are benefits to multiple iterations ( pondering ) even for simple experts, and that the metacontroller can deliver accuracy and computational cost benefits over fixed-iteration controls.

The paper is in general well written, and reasonably easy to follow. As the authors note, the topic of metareasoning has been studied to some extent in AI, but its use as a differentiable and fully trainable component within an RL system appears new. At this stage, it is difficult to evaluate the impact of this kind of approach: the overall model architecture is intriguing and probably merits publication, but whether and how this will scale to other domains remains the subject of future work. The experimental validation is interesting and well carried out, but remains of limited scope. Moreover, given such a complex architecture, there should be a discussion of the training difficulties and convergence issues, if any.

Here are a few specific comments, questions and suggestions:

1) in Figure 1A, the meaning of the graphical language should be explained. For instance, there are arrows of different thickness and line style  do these mean different things? 

2) in Figure 3, the caption should better explain the contents of the figure. For example, what do the colours of the different lines refer to? Also, in the top row, there are dots and error bars that are given, but this is explained only in the  bottom row  part. This makes understanding this figure difficult.

3) in Figure 4, the shaded area represents a 95% confidence interval on the regression line; in addition, it would be helpful to give a standard error on the regression slope (to verify that it excludes zero, i.e. the slope is significant), as well as a fraction of explained variance (R^2). 

4) in Figure 5, the fraction of samples using the MLP expert does not appear to decrease monotonically with the increasing cost of the MLP expert (i.e. the bottom left part of the right plot, with a few red-shaded boxes). Why is that? Is there lots of variance in these fractions from experiment to experiment?

5) the supplementary materials are very helpful. Thank you for all these details.
"
4,"Thank you for an interesting read on an approach to choose computational models based on kind of examples given.

Pros
- As an idea, using a meta controller to decide the computational model and the number of steps to reach the conclusion is keeping in line with solving an important practical issue of increased computational times of a simple example. 

- The approach seems similar to an ensemble learning construct. But instead of random experts and a fixed computational complexity during testing time the architecture is designed to estimate hyper-parameters like number of ponder steps which gives this approach a distinct advantage.


Cons
- Even though the metacontroller is designed to choose the best amongst the given experts, its complete capability has not been explored yet. It would be interesting to see the architecture handle more than 2 experts."
4,"This paper introduces an approach to reinforcement learning and control wherein, rather than training a single controller to perform a task, a metacontroller with access to a base-level controller and a number of accessory  experts  is utilized. The job of the metacontroller is to decide how many times to call the controller and the experts, and which expert to invoke at which iteration. (The controller is a bit special in that in addition to being provided the current state, it is given a summary of the history of previous calls to itself and previous experts.) The sequence of controls and expert advice is embedded into a fixed-size vector through an LSTM. The method is tested on an N-body  control task, where it is shown that there are benefits to multiple iterations ( pondering ) even for simple experts, and that the metacontroller can deliver accuracy and computational cost benefits over fixed-iteration controls.

The paper is in general well written, and reasonably easy to follow. As the authors note, the topic of metareasoning has been studied to some extent in AI, but its use as a differentiable and fully trainable component within an RL system appears new. At this stage, it is difficult to evaluate the impact of this kind of approach: the overall model architecture is intriguing and probably merits publication, but whether and how this will scale to other domains remains the subject of future work. The experimental validation is interesting and well carried out, but remains of limited scope. Moreover, given such a complex architecture, there should be a discussion of the training difficulties and convergence issues, if any.

Here are a few specific comments, questions and suggestions:

1) in Figure 1A, the meaning of the graphical language should be explained. For instance, there are arrows of different thickness and line style  do these mean different things? 

2) in Figure 3, the caption should better explain the contents of the figure. For example, what do the colours of the different lines refer to? Also, in the top row, there are dots and error bars that are given, but this is explained only in the  bottom row  part. This makes understanding this figure difficult.

3) in Figure 4, the shaded area represents a 95% confidence interval on the regression line; in addition, it would be helpful to give a standard error on the regression slope (to verify that it excludes zero, i.e. the slope is significant), as well as a fraction of explained variance (R^2). 

4) in Figure 5, the fraction of samples using the MLP expert does not appear to decrease monotonically with the increasing cost of the MLP expert (i.e. the bottom left part of the right plot, with a few red-shaded boxes). Why is that? Is there lots of variance in these fractions from experiment to experiment?

5) the supplementary materials are very helpful. Thank you for all these details.
"
4,"Thank you for an interesting read on an approach to choose computational models based on kind of examples given.

Pros
- As an idea, using a meta controller to decide the computational model and the number of steps to reach the conclusion is keeping in line with solving an important practical issue of increased computational times of a simple example. 

- The approach seems similar to an ensemble learning construct. But instead of random experts and a fixed computational complexity during testing time the architecture is designed to estimate hyper-parameters like number of ponder steps which gives this approach a distinct advantage.


Cons
- Even though the metacontroller is designed to choose the best amongst the given experts, its complete capability has not been explored yet. It would be interesting to see the architecture handle more than 2 experts."
4,"This paper proposes an autoencoder approach to lossy image compression by minimizing the weighted sum of reconstruction error and code length. The architecture consists of a convolutional encoder and a sub-pixel convolutional decoder. Experiments compare PSNR, SSIM, and MS-SSIM performance against JPEG, JPEG-2000, and a recent RNN-based compression approach. A mean opinion score test was also conducted.

Pros:
+ The paper is clear and well-written.
+ The decoder architecture takes advantage of recent advances in convolutional approaches to image super-resolution.
+ The proposed approaches to quantization and rate estimation are sensible and well-justified.

Cons:
- The experimental baselines do not appear to be entirely complete.

The task of using autoencoders to perform compression is important and has a large practical impact. Though directly optimizing the rate-distortion tradeoff is not an entirely novel enterprise, there are enough differences (e.g. the quantization approach and sub-pixel convolutional decoder) to sufficiently distinguish this from earlier work. I am not an image compression expert but the approach and results both seem compelling. The main shortcoming is that the implementation of Toderici et al. 2016b appears to be incomplete, and there is no comparison to Balle et al. 2016. Overall, I feel that the fact that this architecture achieves competitive performance with JPEG-2000 while simultaneously setting the stage for future work that varies the encoder/decoder size and data domain means the community will find this work to be of significant interest.

I have no further specific comments at this time as they were answered sufficiently in the pre-review questions."
5,"The paper proposes a neural approach to learning an image compression-decompression scheme as an auto-encoder. While the idea is certainly interesting and well-motivated, in practice, it turns out to achieve effectively identical rates to JPEG-2000.

Now, as the authors argue, there is some value to the fact that this scheme was learned automatically rather than by expert design---which means it has benefits beyond the compression of natural images (e.g., it could be used to automatically learning a compression scheme for signals for which we don't have as much domain knowledge). However, I still believe that this makes the paper unsuitable for publication in its current form because of the following reasons---

1. Firstly, the fact that the learned encoder is competitive---and not clearly better---than JPEG 2000 means that the focus of the paper should more be about the aspects in which the encoder is similar to, and the aspects in which it differs, from JPEG 2000. Is it learning similar filters or completely different ones ? For what kinds of textures does it do better and for what kinds does it do worse (the paper could show the best and worst 10 patches at different bit-rates) ?

2. Secondly, I think it's crucial that the paper demonstrate that the benefits come from a better coding scheme (as opposed to just a better decoder), as suggested in my initial pre-review question. How would a decoder trained on JPEG-2000 codes (and perhaps also on encoded random projections) do worse or better ?

3. Finally, I think the fact that it does as well/worse than JPEG-2000 significantly diminishes the case for using a 'deep' auto-encoder. JPEG-2000 essentially uses a wavelet transform, which is a basis that past studies have shown could be recovered using a simple sparse dictionary algorithm like K-SVD. This is why I feel that the method needs to clearly outperform JPEG-2000, or show comparisons to (or atleast discuss) a well-crafted traditional/generative model-based baseline."
4,"This paper proposes an autoencoder approach to lossy image compression by minimizing the weighted sum of reconstruction error and code length. The architecture consists of a convolutional encoder and a sub-pixel convolutional decoder. Experiments compare PSNR, SSIM, and MS-SSIM performance against JPEG, JPEG-2000, and a recent RNN-based compression approach. A mean opinion score test was also conducted.

Pros:
+ The paper is clear and well-written.
+ The decoder architecture takes advantage of recent advances in convolutional approaches to image super-resolution.
+ The proposed approaches to quantization and rate estimation are sensible and well-justified.

Cons:
- The experimental baselines do not appear to be entirely complete.

The task of using autoencoders to perform compression is important and has a large practical impact. Though directly optimizing the rate-distortion tradeoff is not an entirely novel enterprise, there are enough differences (e.g. the quantization approach and sub-pixel convolutional decoder) to sufficiently distinguish this from earlier work. I am not an image compression expert but the approach and results both seem compelling. The main shortcoming is that the implementation of Toderici et al. 2016b appears to be incomplete, and there is no comparison to Balle et al. 2016. Overall, I feel that the fact that this architecture achieves competitive performance with JPEG-2000 while simultaneously setting the stage for future work that varies the encoder/decoder size and data domain means the community will find this work to be of significant interest.

I have no further specific comments at this time as they were answered sufficiently in the pre-review questions."
5,"The paper proposes a neural approach to learning an image compression-decompression scheme as an auto-encoder. While the idea is certainly interesting and well-motivated, in practice, it turns out to achieve effectively identical rates to JPEG-2000.

Now, as the authors argue, there is some value to the fact that this scheme was learned automatically rather than by expert design---which means it has benefits beyond the compression of natural images (e.g., it could be used to automatically learning a compression scheme for signals for which we don't have as much domain knowledge). However, I still believe that this makes the paper unsuitable for publication in its current form because of the following reasons---

1. Firstly, the fact that the learned encoder is competitive---and not clearly better---than JPEG 2000 means that the focus of the paper should more be about the aspects in which the encoder is similar to, and the aspects in which it differs, from JPEG 2000. Is it learning similar filters or completely different ones ? For what kinds of textures does it do better and for what kinds does it do worse (the paper could show the best and worst 10 patches at different bit-rates) ?

2. Secondly, I think it's crucial that the paper demonstrate that the benefits come from a better coding scheme (as opposed to just a better decoder), as suggested in my initial pre-review question. How would a decoder trained on JPEG-2000 codes (and perhaps also on encoded random projections) do worse or better ?

3. Finally, I think the fact that it does as well/worse than JPEG-2000 significantly diminishes the case for using a 'deep' auto-encoder. JPEG-2000 essentially uses a wavelet transform, which is a basis that past studies have shown could be recovered using a simple sparse dictionary algorithm like K-SVD. This is why I feel that the method needs to clearly outperform JPEG-2000, or show comparisons to (or atleast discuss) a well-crafted traditional/generative model-based baseline."
2,"This is a solid paper that proposes to endow attention mechanisms with structure (the attention posterior probabilities becoming structured latent variables). Experiments are shown with segmental atention (as in semi-Markov models) and syntactic attention (as in projective dependency parsing), both in a synthetic task (tree transduction) and real world tasks (neural machine translation and natural language inference). There is a small gain in using structured attention over simple attention in the latter tasks. A clear accept.

The paper is very clear, the approach is novel and interesting, and the experiments seem to give a good proof of concept. However, the use of structured attention in neural MT seems doesn't seem to be fully exploited here: segmental attention could be a way of approaching neural phrase-based MT, and syntactic attention offers a way of incorporating latent syntax in MT -- these seem very promising directions. In particular it would be interesting to try to add some (semi-)supervision on these attention mechanisms (e.g. posterior marginals computed by an external parser) to see if that helps learning the attention components of the network, or at least help initializing them. 

This seems to be the first interesting use of the backprop of forward-backward/inside-outside (Stoyanov et al. 2011). As stated in sec 3.3., for general probabilistic models the forward step over structured attention corresponds to the computation of first-order moments (posterior marginals) while the backprop step corresponds to second-order moments (gradients of marginals wrt log-potentials, i.e., Hessian of log-partition function). This extends the applicability of the proposed approach to arbitrary graphical models where these quantities can be computed efficiently. E.g. is there a generalized matrix-tree formula that allows to do backprop for non-projective syntax? On the negative side, I suspect the need for second-order statistics may bring some numerical instability in some problems, caused by the use of the signed log-space field. Was this seen in practice?

Minor comments/typos:
- last paragraph of sec 1: ""standard attention attention""
- third paragraph of sec 3.2: ""the on log-potentials""
- sec 4.1, Results: ""... as it has no information about the source ordering"" -- what do you mean here?"
2,"The authors propose to extend the standard attention mechanism, by extending it to consider a distribution over latent structures (e.g., alignments, syntactic parse trees, etc.). These latent variables are modeled as a graphical model with potentials derived from a neural network.

The paper is well-written and clear to understand. The proposed methods are evaluated on various problems, and in each case the structured attention models outperform baseline models (either one without attention, or using simple attention). For the two real-world tasks, the improvements obtained from the proposed approach are relatively small compared to the simple attention models, but the techniques are nonetheless interesting.

Main comments:
1. In the Japanese-English Machine Translation example, the relative difference in performance between the Sigmoid attention model, and the Structured attention model appears to be relatively small. In this case, Im curious if the authors analyzed the attention alignments to determine whether the structured models resulted in better alignments. In other words, if ground-truth alignments are available for the dataset, or if they can be human-annotated for some test examples, it would be interesting to measure the quality of the alignments in addition to the BLEU metric.
2. In the final experiment on natural language inference, I thought it was a bit surprising that using pretrained syntactic attention layers did not appear to improve model performance, but instead appear to degrade performance. I was curious if the authors have any hypotheses for why this is the case?

Minor comments:
1. Typographical error: Equation 1: p(z | x, q  p(z | x, q)
2. Section 3.3: Past work has demonstrated that the techniques necessary for this approach,     Past work has demonstrated the techniques necessary for this approach,  
"
2,"This is a solid paper that proposes to endow attention mechanisms with structure (the attention posterior probabilities becoming structured latent variables). Experiments are shown with segmental atention (as in semi-Markov models) and syntactic attention (as in projective dependency parsing), both in a synthetic task (tree transduction) and real world tasks (neural machine translation and natural language inference). There is a small gain in using structured attention over simple attention in the latter tasks. A clear accept.

The paper is very clear, the approach is novel and interesting, and the experiments seem to give a good proof of concept. However, the use of structured attention in neural MT seems doesn't seem to be fully exploited here: segmental attention could be a way of approaching neural phrase-based MT, and syntactic attention offers a way of incorporating latent syntax in MT -- these seem very promising directions. In particular it would be interesting to try to add some (semi-)supervision on these attention mechanisms (e.g. posterior marginals computed by an external parser) to see if that helps learning the attention components of the network, or at least help initializing them. 

This seems to be the first interesting use of the backprop of forward-backward/inside-outside (Stoyanov et al. 2011). As stated in sec 3.3., for general probabilistic models the forward step over structured attention corresponds to the computation of first-order moments (posterior marginals) while the backprop step corresponds to second-order moments (gradients of marginals wrt log-potentials, i.e., Hessian of log-partition function). This extends the applicability of the proposed approach to arbitrary graphical models where these quantities can be computed efficiently. E.g. is there a generalized matrix-tree formula that allows to do backprop for non-projective syntax? On the negative side, I suspect the need for second-order statistics may bring some numerical instability in some problems, caused by the use of the signed log-space field. Was this seen in practice?

Minor comments/typos:
- last paragraph of sec 1: ""standard attention attention""
- third paragraph of sec 3.2: ""the on log-potentials""
- sec 4.1, Results: ""... as it has no information about the source ordering"" -- what do you mean here?"
2,"The authors propose to extend the standard attention mechanism, by extending it to consider a distribution over latent structures (e.g., alignments, syntactic parse trees, etc.). These latent variables are modeled as a graphical model with potentials derived from a neural network.

The paper is well-written and clear to understand. The proposed methods are evaluated on various problems, and in each case the structured attention models outperform baseline models (either one without attention, or using simple attention). For the two real-world tasks, the improvements obtained from the proposed approach are relatively small compared to the simple attention models, but the techniques are nonetheless interesting.

Main comments:
1. In the Japanese-English Machine Translation example, the relative difference in performance between the Sigmoid attention model, and the Structured attention model appears to be relatively small. In this case, Im curious if the authors analyzed the attention alignments to determine whether the structured models resulted in better alignments. In other words, if ground-truth alignments are available for the dataset, or if they can be human-annotated for some test examples, it would be interesting to measure the quality of the alignments in addition to the BLEU metric.
2. In the final experiment on natural language inference, I thought it was a bit surprising that using pretrained syntactic attention layers did not appear to improve model performance, but instead appear to degrade performance. I was curious if the authors have any hypotheses for why this is the case?

Minor comments:
1. Typographical error: Equation 1: p(z | x, q  p(z | x, q)
2. Section 3.3: Past work has demonstrated that the techniques necessary for this approach,     Past work has demonstrated the techniques necessary for this approach,  
"
2,"The authors propose a conceptually simple method for regularisation of recurrent neural networks. The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability.

Overall, the paper is well written. The method is clearly represented up to issues raised by reviewers during the pre-review question phase. The related work is complete and probably the best currently available on the matter of regularising RNNs.

The experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem. All of the experiments focus on sequences over discrete values. An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case.

Overall, the paper bears great potential. However, I do see some points.

1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available.

I want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here.
Zoneout does not seem to improve that much in the other tasks.

2) Zoneout is not investigated well mathematically. E.g. an analysis of the of the form of gradients from unit K at time step T to unit K at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout. Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gals work is an obvious one. I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts. Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis.

3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away.


An extreme amount of tricks is being published currently for improved RNN training. How does zoneout stand out? It is a nice idea, and simple to implement. However, the paper under delivers: the experiments do not convince me (see 1) and 3)). There authors do not provide convincing theoretical insights either. (2)

Consequently, the paper reduces to a epsilon improvement, great text, mediocre experimental evaluation, little theoretical insight.
"
2,"The authors propose a conceptually simple method for regularisation of recurrent neural networks. The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability.

Overall, the paper is well written. The method is clearly represented up to issues raised by reviewers during the pre-review question phase. The related work is complete and probably the best currently available on the matter of regularising RNNs.

The experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem. All of the experiments focus on sequences over discrete values. An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case.

Overall, the paper bears great potential. However, I do see some points.

1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available.

I want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here.
Zoneout does not seem to improve that much in the other tasks.

2) Zoneout is not investigated well mathematically. E.g. an analysis of the of the form of gradients from unit K at time step T to unit K at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout. Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gals work is an obvious one. I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts. Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis.

3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away.


An extreme amount of tricks is being published currently for improved RNN training. How does zoneout stand out? It is a nice idea, and simple to implement. However, the paper under delivers: the experiments do not convince me (see 1) and 3)). There authors do not provide convincing theoretical insights either. (2)

Consequently, the paper reduces to a epsilon improvement, great text, mediocre experimental evaluation, little theoretical insight.
"
5,"Thank you for an interesting read.

I found this paper very interesting. Since I don't think (deterministic) approximate inference is separated from the modelling procedure (cf. exact inference), it is important to allow the users to select the inference method to suit their needs and constraints. I'm not an expert of PPL, but to my knowledge this is the first package that I've seen which put more focus on compositional inference. Leveraging tensorflow is also a plus, which allows flexible computation graph design as well as parallel computation using GPUs.

The only question I have is about the design of flexible objective functions to learn hyper-parameters (or in the paper those variables associated with delta q distributions). It seems hyper-parameter learning is also specified as inference, which makes sense if using MAP. However the authors also demonstrated other objective functions such as Renyi divergences, does that mean the user need to define a new class of inference method whenever they want to test an alternative loss function?"
5,"Thank you for an interesting read.

I found this paper very interesting. Since I don't think (deterministic) approximate inference is separated from the modelling procedure (cf. exact inference), it is important to allow the users to select the inference method to suit their needs and constraints. I'm not an expert of PPL, but to my knowledge this is the first package that I've seen which put more focus on compositional inference. Leveraging tensorflow is also a plus, which allows flexible computation graph design as well as parallel computation using GPUs.

The only question I have is about the design of flexible objective functions to learn hyper-parameters (or in the paper those variables associated with delta q distributions). It seems hyper-parameter learning is also specified as inference, which makes sense if using MAP. However the authors also demonstrated other objective functions such as Renyi divergences, does that mean the user need to define a new class of inference method whenever they want to test an alternative loss function?"
4,"The paper presents an interesting framework for image generation, which stitches the foreground and background to form an image. This is obviously a reasonable approach there is clearly a foreground object. However, real world images are often quite complicated, which may contain multiple layers of composition, instead of a simple foreground-background layer. How would the proposed method deal with such situations?

Overall, this is a reasonable work that approaches an important problem from a new angle. Yet, I think sizable efforts remain needed to make it a generic methodology. "
4,"The paper proposes a model for image generation where the back-ground is generated first and then the foreground is pasted in by generating first a foregound mask and corresponding appearance, curving the appearance image using the mask and transforming the mask using predicted affine transform to paste it on top of the image. Using AMTurkers the authors verify their generated images are selected 68% of the time as being more naturally looking than corresponding images from a DC-GAN model that does not use a figure-ground aware image generator.

The segmentations masks learn to depict objects in very constrained datasets (birds) only, thus the method appears limited for general shape datasets, as the authors also argue in the paper. Yet, the architectural contributions have potential merit.

It would be nice to see if multiple layers of foreground (occluding foregrounds) are ever generated with this layered model or it is just figure-ground aware."
5,"The authors propose a method that generates naturally looking images by first generating the background and then conditioned on the previous layer one or multiple foreground objects. Additionally they add a image transformer layer that allows the model to more easily model different appearances.

I would like to see some discussion about the choice of foreground+mask rather than just predicting foreground directly. For MNIST, for example the foreground seems completely irrelevant. For CUB and CIFAR of course the fg adds the texture and color while the masks ensures a crisp boundary. 
- Is the mask a binary mask or a alpha blending mask?
- I find the fact that the model learns to decompose images this nicely and learns to produce crisp foreground masks w/o too much spurious elements (though there are some in CIFAR) pretty fascinating.

The proposed evaluation metric makes sense and seems reasonable. However, AFAICT, theoretically it would be possible to get a high score even though the GAN produces images not recognizable to humans, but only to the classifier network that produces P_g. E.g. if the Generator encodes the class in some subtle way (though this shouldn't happen given the training with an adversarial network).

Fig 3 shows indeed nicely that the decomposition is much nicer when spatial transformers are used. However, it also seems to indicate that the foreground prediction and the foreground mask are largely redundant. For the final results the ""niceness"" of the decomposition appears to be largely irrelevant.

Furthermore, the transformation layer seems to have a small effect, judging from the transformed masked foreground objects. They are mainly scaled down.

- What is the 3rd & 6th column in Fig 9? It is not clear if the final composed images are really as bad as ""advertised"".

Regarding the eval experiment using AMT it is not clear why it is better to provide the users with L2 minimized NN matches rather than random pairs.

I assume that Tab 1 Adversarial Divergence for Real images was not actually evaluated? It would be interesting to see how close to 0 multiple differently initialized networks actually are. Also please mention how the confidences/std where generated, i.e. different training sets, initialisations, eval sets, and how many runs.
"
4,"The paper presents an interesting framework for image generation, which stitches the foreground and background to form an image. This is obviously a reasonable approach there is clearly a foreground object. However, real world images are often quite complicated, which may contain multiple layers of composition, instead of a simple foreground-background layer. How would the proposed method deal with such situations?

Overall, this is a reasonable work that approaches an important problem from a new angle. Yet, I think sizable efforts remain needed to make it a generic methodology. "
4,"The paper proposes a model for image generation where the back-ground is generated first and then the foreground is pasted in by generating first a foregound mask and corresponding appearance, curving the appearance image using the mask and transforming the mask using predicted affine transform to paste it on top of the image. Using AMTurkers the authors verify their generated images are selected 68% of the time as being more naturally looking than corresponding images from a DC-GAN model that does not use a figure-ground aware image generator.

The segmentations masks learn to depict objects in very constrained datasets (birds) only, thus the method appears limited for general shape datasets, as the authors also argue in the paper. Yet, the architectural contributions have potential merit.

It would be nice to see if multiple layers of foreground (occluding foregrounds) are ever generated with this layered model or it is just figure-ground aware."
5,"The authors propose a method that generates naturally looking images by first generating the background and then conditioned on the previous layer one or multiple foreground objects. Additionally they add a image transformer layer that allows the model to more easily model different appearances.

I would like to see some discussion about the choice of foreground+mask rather than just predicting foreground directly. For MNIST, for example the foreground seems completely irrelevant. For CUB and CIFAR of course the fg adds the texture and color while the masks ensures a crisp boundary. 
- Is the mask a binary mask or a alpha blending mask?
- I find the fact that the model learns to decompose images this nicely and learns to produce crisp foreground masks w/o too much spurious elements (though there are some in CIFAR) pretty fascinating.

The proposed evaluation metric makes sense and seems reasonable. However, AFAICT, theoretically it would be possible to get a high score even though the GAN produces images not recognizable to humans, but only to the classifier network that produces P_g. E.g. if the Generator encodes the class in some subtle way (though this shouldn't happen given the training with an adversarial network).

Fig 3 shows indeed nicely that the decomposition is much nicer when spatial transformers are used. However, it also seems to indicate that the foreground prediction and the foreground mask are largely redundant. For the final results the ""niceness"" of the decomposition appears to be largely irrelevant.

Furthermore, the transformation layer seems to have a small effect, judging from the transformed masked foreground objects. They are mainly scaled down.

- What is the 3rd & 6th column in Fig 9? It is not clear if the final composed images are really as bad as ""advertised"".

Regarding the eval experiment using AMT it is not clear why it is better to provide the users with L2 minimized NN matches rather than random pairs.

I assume that Tab 1 Adversarial Divergence for Real images was not actually evaluated? It would be interesting to see how close to 0 multiple differently initialized networks actually are. Also please mention how the confidences/std where generated, i.e. different training sets, initialisations, eval sets, and how many runs.
"
5,"This paper proposes a Variational Autoencoder model that can discard information found irrelevant, in order to learn interesting global representations of the data. This can be seen as a lossy compression algorithm, hence the name Variational Lossy Autoencoder. To achieve such model, the authors combine VAEs with neural autoregressive models resulting in a model that has both a latent variable structure and a powerful recurrence structure.

The authors first present an insightful Bits-Back interpretation of VAE to show when and how the latent code is ignored. As it was also mentioned in the literature, they say that the autoregressive part of the model ends up explaining all structure in the data, while the latent variables are not used. Then, they propose two complementary approaches to force the latent variables to be used by the decoder. The first one is to make sure the autoregressive decoder only uses small local receptive field so the model has to use the latent code to learn long-range dependency. The second is to parametrize the prior distribution over the latent code with an autoregressive model.

They also report new state-of-the-art results on binarized MNIST (both dynamical and statically binarization), OMNIGLOT and Caltech-101 Silhouettes.

Review:
The bits-Back interpretation of VAE is a nice contribution to the community. Having novel interpretations for a model helps to better understand it and sometimes, like in this paper, highlights how it can be improved.

Having a fine-grained control over the kind of information that gets included in the learned representation can be useful for a lot of applications. For instance, in image retrieval, such learned representation could be used to retrieve objects that have similar shape no matter what texture they have.

However, the authors say they propose two complementary classes of improvements to VAE, that is the lossy code via explicit information placement (Section 3.1) and learning the prior with autoregressive flow (Section 3.2). However, they never actually showed how a VAE without AF prior but that has a PixelCNN decoder performs. What would be the impact on the latent code is no AF prior is used?

Also, it is not clear if WindowAround(i) represents only a subset of x_{"
4,"This paper introduces the notion of a ""variational lossy autoencoder"", where a powerful autoregressive conditional distribution on the inputs x given the latent code z is crippled in a way that forces it to use z in a meaningful way. Its three main contributions are:

(1) It gives an interesting information-theoretical insight as to why VAE-type models don't tend to take advantage of their latent representation when the conditional distribution on x given z is powerful enough.

(2) It shows that this insight can be used to efficiently train VAEs with powerful autoregressive conditional distributions such that they make use of the latent code.

(3) It presents a powerful way to parametrize the prior in the form of an autoregressive flow transformation which is equivalent to using an inverse autoregressive flow transformation on the approximate posterior.

By itself, I think the information-theoretical explanation of why VAEs do not use their latent code when the conditional distribution on x given z is powerful enough constitutes an excellent addition to our understanding of VAE-related approaches.

However, the way this intuition is empirically evaluated is a bit weak. The ""crippling"" method used feels hand-crafted and very task-dependent, and the qualitative evaluation of the ""lossyness"" of the learned representation is carried out on three datasets (MNIST, OMNIGLOT and Caltech-101 Silhouettes) which feature black-and-white images with little-to-no texture. Figures 1a and 2a do show that reconstructions discard low-level information, as observed in the slight variations in strokes between the input and the reconstruction, but such an analysis would have been more compelling with more complex image datasets. Have the authors tried applying VLAE to such datasets?

I think the Caltech101 Silhouettes benchmark should be treated with caution, as no comparison is made against other competitive approaches like IAF VAE, PixelRNN and Conv DRAW. This means that VLAE significantly outperforms the state-of-the-art in only one of the four settings examined.

A question which is very relevant to this paper is ""Does a latent representation on top of an autoregressive model help improve the density modeling performance?"" The paper touches this question, but very briefly: the only setting in which VLAE is compared against recent autoregressive approaches shows that it wins against PixelRNN by a small margin.

The proposal to transform the latent code with an autoregressive flow which is equivalent to parametrizing the approximate posterior with an inverse autoregressive flow transformation is also interesting. There is, however, one important distinction to be made between the two approaches: in the former, the prior over the latent code can potentially be very complex whereas in the latter the prior is limited to be a simple, factorized distribution.

It is not clear to me that having a very powerful prior is necessarily a good thing from a representation learning point of view: oftentimes we are interested in learning a representation of the data distribution which is untangled and composed of roughly independent factors of variation. The degree to which this can be achieved using something as simple as a spherical gaussian prior is up for discussion, but finding a good balance between the ability of the prior to fit the data and its usefulness as a high-level representation certainly warrants some thought. I would be interested in hearing the authors' opinion on this.

Overall, the paper introduces interesting ideas despite the flaws outlined above, but weaknesses in the empirical evaluation prevent me from recommending its acceptance.

UPDATE: The rating has been revised to a 7 following the authors' reply."
5,"This paper motivates the combination of autoregressive models with Variational Auto-Encoders and how to control the amount the amount of information stored in the latent code. The authors provide state-of-the-art results on MNIST, OMNIGLOT and Caltech-101.
I find that the insights provided in the paper, e.g. with respect to the effect of having a more powerful decoder on learning the latent code, the bit-back coding, and the lossy decoding are well-written but are not novel.
The difference between an auto-regressive prior and the inverse auto-regressive posterior is new and interesting though.
The model presented combines the recent technique of PixelRNN/PixelCNN and Variational Auto-Encoders with Inverse Auto-Regressive Flows, which enables the authors to obtain state-of-the-art results on MNIST, OMNIGLOT and Caltech-101. Given the insights provided in the paper, the authors are also able to control the amount of information contained in the latent code to an extent.
This paper gather several insight on Variational Auto-Encoders scattered through several publications in a well-written way. From these, the authors are able to obtain state-of-the-art models on small complexity datasets. Larger scale experiments will be necessary."
5,"This paper proposes a Variational Autoencoder model that can discard information found irrelevant, in order to learn interesting global representations of the data. This can be seen as a lossy compression algorithm, hence the name Variational Lossy Autoencoder. To achieve such model, the authors combine VAEs with neural autoregressive models resulting in a model that has both a latent variable structure and a powerful recurrence structure.

The authors first present an insightful Bits-Back interpretation of VAE to show when and how the latent code is ignored. As it was also mentioned in the literature, they say that the autoregressive part of the model ends up explaining all structure in the data, while the latent variables are not used. Then, they propose two complementary approaches to force the latent variables to be used by the decoder. The first one is to make sure the autoregressive decoder only uses small local receptive field so the model has to use the latent code to learn long-range dependency. The second is to parametrize the prior distribution over the latent code with an autoregressive model.

They also report new state-of-the-art results on binarized MNIST (both dynamical and statically binarization), OMNIGLOT and Caltech-101 Silhouettes.

Review:
The bits-Back interpretation of VAE is a nice contribution to the community. Having novel interpretations for a model helps to better understand it and sometimes, like in this paper, highlights how it can be improved.

Having a fine-grained control over the kind of information that gets included in the learned representation can be useful for a lot of applications. For instance, in image retrieval, such learned representation could be used to retrieve objects that have similar shape no matter what texture they have.

However, the authors say they propose two complementary classes of improvements to VAE, that is the lossy code via explicit information placement (Section 3.1) and learning the prior with autoregressive flow (Section 3.2). However, they never actually showed how a VAE without AF prior but that has a PixelCNN decoder performs. What would be the impact on the latent code is no AF prior is used?

Also, it is not clear if WindowAround(i) represents only a subset of x_{"
4,"This paper introduces the notion of a ""variational lossy autoencoder"", where a powerful autoregressive conditional distribution on the inputs x given the latent code z is crippled in a way that forces it to use z in a meaningful way. Its three main contributions are:

(1) It gives an interesting information-theoretical insight as to why VAE-type models don't tend to take advantage of their latent representation when the conditional distribution on x given z is powerful enough.

(2) It shows that this insight can be used to efficiently train VAEs with powerful autoregressive conditional distributions such that they make use of the latent code.

(3) It presents a powerful way to parametrize the prior in the form of an autoregressive flow transformation which is equivalent to using an inverse autoregressive flow transformation on the approximate posterior.

By itself, I think the information-theoretical explanation of why VAEs do not use their latent code when the conditional distribution on x given z is powerful enough constitutes an excellent addition to our understanding of VAE-related approaches.

However, the way this intuition is empirically evaluated is a bit weak. The ""crippling"" method used feels hand-crafted and very task-dependent, and the qualitative evaluation of the ""lossyness"" of the learned representation is carried out on three datasets (MNIST, OMNIGLOT and Caltech-101 Silhouettes) which feature black-and-white images with little-to-no texture. Figures 1a and 2a do show that reconstructions discard low-level information, as observed in the slight variations in strokes between the input and the reconstruction, but such an analysis would have been more compelling with more complex image datasets. Have the authors tried applying VLAE to such datasets?

I think the Caltech101 Silhouettes benchmark should be treated with caution, as no comparison is made against other competitive approaches like IAF VAE, PixelRNN and Conv DRAW. This means that VLAE significantly outperforms the state-of-the-art in only one of the four settings examined.

A question which is very relevant to this paper is ""Does a latent representation on top of an autoregressive model help improve the density modeling performance?"" The paper touches this question, but very briefly: the only setting in which VLAE is compared against recent autoregressive approaches shows that it wins against PixelRNN by a small margin.

The proposal to transform the latent code with an autoregressive flow which is equivalent to parametrizing the approximate posterior with an inverse autoregressive flow transformation is also interesting. There is, however, one important distinction to be made between the two approaches: in the former, the prior over the latent code can potentially be very complex whereas in the latter the prior is limited to be a simple, factorized distribution.

It is not clear to me that having a very powerful prior is necessarily a good thing from a representation learning point of view: oftentimes we are interested in learning a representation of the data distribution which is untangled and composed of roughly independent factors of variation. The degree to which this can be achieved using something as simple as a spherical gaussian prior is up for discussion, but finding a good balance between the ability of the prior to fit the data and its usefulness as a high-level representation certainly warrants some thought. I would be interested in hearing the authors' opinion on this.

Overall, the paper introduces interesting ideas despite the flaws outlined above, but weaknesses in the empirical evaluation prevent me from recommending its acceptance.

UPDATE: The rating has been revised to a 7 following the authors' reply."
5,"This paper motivates the combination of autoregressive models with Variational Auto-Encoders and how to control the amount the amount of information stored in the latent code. The authors provide state-of-the-art results on MNIST, OMNIGLOT and Caltech-101.
I find that the insights provided in the paper, e.g. with respect to the effect of having a more powerful decoder on learning the latent code, the bit-back coding, and the lossy decoding are well-written but are not novel.
The difference between an auto-regressive prior and the inverse auto-regressive posterior is new and interesting though.
The model presented combines the recent technique of PixelRNN/PixelCNN and Variational Auto-Encoders with Inverse Auto-Regressive Flows, which enables the authors to obtain state-of-the-art results on MNIST, OMNIGLOT and Caltech-101. Given the insights provided in the paper, the authors are also able to control the amount of information contained in the latent code to an extent.
This paper gather several insight on Variational Auto-Encoders scattered through several publications in a well-written way. From these, the authors are able to obtain state-of-the-art models on small complexity datasets. Larger scale experiments will be necessary."
4,"I think the authors provide an interesting direction for understanding and maybe constructing recurrent models that are easier to interpret. Is not clear where such direction will lead but I think it could be an interesting starting point for future work, one that worth exploring. "
5,"This paper poses an interesting idea: removing chaotic behavior or RNNs.
While many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box, this paper does a good job on presenting why its method may work well.

Although, the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model), the reviewer is not fully convinced by the main claim of this paper, the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable. In the paper, the LSTM shows a very sensitive behaviour, even when a very small amount of noise is added to the input. However, it still performs surprisingly well with this chaotic behaviour. 

Measuring the model complexity is a very difficult task, therefore, many papers manage to use either same number of hidden units or choose approximately close model sizes. In this paper, the experiments were carried by using the same amount of parameters for both the LSTM and CFN. However, I think the CFN may have much more simpler computational graph. Taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor?

It is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher. Could you provide more statistics on this? For example, what is the average relaxation time of the whole hidden units at each layer?

Batch normalization and layer normalization can be helpful to make the training of RNNs become more stable. How would the behaviour of batch normalized or perhaps layer normalized LSTM look like? Also, it is often not trivial to make batch normalization or layer normalization to work on a new architecture. I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN.

The quality of the work is good, explanation is clear enough along with nice analyses and proofs. Overall, the performance is not any better than LSTMs, but it is still interesting when thinking of simplicity of this model. I am a bit concerned if this model might not work that well in more harder task, e.g., translation. Figure 4 of this paper is very interesting, where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones."
4,"I think the authors provide an interesting direction for understanding and maybe constructing recurrent models that are easier to interpret. Is not clear where such direction will lead but I think it could be an interesting starting point for future work, one that worth exploring. "
5,"This paper poses an interesting idea: removing chaotic behavior or RNNs.
While many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box, this paper does a good job on presenting why its method may work well.

Although, the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model), the reviewer is not fully convinced by the main claim of this paper, the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable. In the paper, the LSTM shows a very sensitive behaviour, even when a very small amount of noise is added to the input. However, it still performs surprisingly well with this chaotic behaviour. 

Measuring the model complexity is a very difficult task, therefore, many papers manage to use either same number of hidden units or choose approximately close model sizes. In this paper, the experiments were carried by using the same amount of parameters for both the LSTM and CFN. However, I think the CFN may have much more simpler computational graph. Taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor?

It is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher. Could you provide more statistics on this? For example, what is the average relaxation time of the whole hidden units at each layer?

Batch normalization and layer normalization can be helpful to make the training of RNNs become more stable. How would the behaviour of batch normalized or perhaps layer normalized LSTM look like? Also, it is often not trivial to make batch normalization or layer normalization to work on a new architecture. I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN.

The quality of the work is good, explanation is clear enough along with nice analyses and proofs. Overall, the performance is not any better than LSTMs, but it is still interesting when thinking of simplicity of this model. I am a bit concerned if this model might not work that well in more harder task, e.g., translation. Figure 4 of this paper is very interesting, where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones."
4,"This paper describes a method for greatly expanding network model size (in terms of number of stored parameters) in the context of a recurrent net, by applying a Mixture of Experts between recurrent net layers that is shared between all time steps.  By process features from all timesteps at the same time, the effective batch size to the MoE is increased by a factor of the number of steps in the model; thus even for sparsely assigned experts, each expert can be used on a large enough sub-batch of inputs to remain computationally efficient.  Another second technique that redistributes elements within a distributed model is also described, further increasing per-expert batch sizes.

Experiments are performed on language modeling and machine translation tasks, showing significant gains by increasing the number of experts, compared to both SoA as well as explicitly computationally-matched baseline systems.

An area that falls a bit short is in presenting plots or statistics on the real computational load and system behavior.  While two loss terms were employed to balance the use of experts, these are not explored in the experiments section.  It would have been nice to see the effects of these more, along with the effects of increasing effective batch sizes, e.g. measurements of the losses over the course of training, compared to the counts/histogram distributions of per-expert batch sizes.

Overall I think this is a well-described system that achieves good results, using a nifty placement for the MoE that can overcome what otherwise might be a disadvantage for sparse computation.



Small comment:
I like Fig 3, but it's not entirely clear whether datapoints coincide between left and right plots.  The H-H line has 3 points on left but 5 on the right?  Also would be nice if the colors matched between corresponding lines."
4,"This paper describes a method for greatly expanding network model size (in terms of number of stored parameters) in the context of a recurrent net, by applying a Mixture of Experts between recurrent net layers that is shared between all time steps.  By process features from all timesteps at the same time, the effective batch size to the MoE is increased by a factor of the number of steps in the model; thus even for sparsely assigned experts, each expert can be used on a large enough sub-batch of inputs to remain computationally efficient.  Another second technique that redistributes elements within a distributed model is also described, further increasing per-expert batch sizes.

Experiments are performed on language modeling and machine translation tasks, showing significant gains by increasing the number of experts, compared to both SoA as well as explicitly computationally-matched baseline systems.

An area that falls a bit short is in presenting plots or statistics on the real computational load and system behavior.  While two loss terms were employed to balance the use of experts, these are not explored in the experiments section.  It would have been nice to see the effects of these more, along with the effects of increasing effective batch sizes, e.g. measurements of the losses over the course of training, compared to the counts/histogram distributions of per-expert batch sizes.

Overall I think this is a well-described system that achieves good results, using a nifty placement for the MoE that can overcome what otherwise might be a disadvantage for sparse computation.



Small comment:
I like Fig 3, but it's not entirely clear whether datapoints coincide between left and right plots.  The H-H line has 3 points on left but 5 on the right?  Also would be nice if the colors matched between corresponding lines."
3,"This was an interesting paper. The algorithm seems clear, the problem well-recognized, and the results are both strong and plausible.

Approaches to hyperparameter optimization based on SMBO have struggled to make good use of convergence during training, and this paper presents a fresh look at a non-SMBO alternative (at least I thought it did, until one of the other reviewers pointed out how much overlap there is with the previously published successive halving algorithm - too bad!). Still, I'm excited to try it. I'm cautiously optimistic that this simple alternative to SMBO may be the first advance to model search for the skeptical practitioner since the case for random search > grid search ("
3,"This was an interesting paper. The algorithm seems clear, the problem well-recognized, and the results are both strong and plausible.

Approaches to hyperparameter optimization based on SMBO have struggled to make good use of convergence during training, and this paper presents a fresh look at a non-SMBO alternative (at least I thought it did, until one of the other reviewers pointed out how much overlap there is with the previously published successive halving algorithm - too bad!). Still, I'm excited to try it. I'm cautiously optimistic that this simple alternative to SMBO may be the first advance to model search for the skeptical practitioner since the case for random search > grid search ("
2,"The Neural Turing Machine and related external memory models have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures. In particular, the NTM, DNC and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position.

The NTM, which is the most relevant to this work, uses a differentiable version of a Turing machine tape. The controller outputs a kernel which softly shifts the head, allowing the machine to read and write sequences. Since this soft shift typically smears the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution.

The premise of this work is to notice that while the NTM emulates a differentiable version of a Turing tape, there is no particular reason that one is constrained to follow the topology of a Turing tape. Instead they propose memory stored at a set of points on a manifold and shift actions which form a Lie group. In this way, memory points can have have different relationships to one another, rather than being constrained to Z.

This is mathematically elegant and here they empirically test models with the shift group R^2 acting on R^2 and the rotation group acting on a sphere.

Overall, the paper is well communicated and a novel idea.

The primary limitation of this paper is its limited impact. While this approach is certainly mathematically elegant, even likely beneficial for some specific problems where the problem structure matches the group structure, it is not clear that this significantly contributes to building models capable of more general program learning. Instead, it is likely to make an already complex and slow model such as the NTM even slower. In general, it would seem memory topology is problem specific and should therefore be learned rather than specified.

The baseline used for comparison is a very simple model, which does not even having the sharpening (the NTM approach to solving the problem of head distributions becoming smeared). There is also no comparison with the successor to the NTM, the DNC, which provides a more general approach to linking memories based on prior memory accesses.

Minor issues:
Footnote on page 3 is misleading regarding the DNC. While the linkage matrix explicitly excludes the identity, the controller can keep the head in the same position by gating the following of the link matrix.
Figures on page 8 are difficult to follow.
"
3,"The paper proposes a new memory access scheme based on Lie group actions for NTMs.

Pros:
* Well written
* Novel addressing scheme as an extension to NTM.
* Seems to work slightly better than normal NTMs.
* Some interesting theory about the novel addressing scheme based on Lie groups.

Cons:
* In the results, the LANTM only seems to be slightly better than the normal NTM.
* The result tables are a bit confusing.
* No source code available.
* The difference to the properties of normal NTM doesn't become too clear. Esp it is said that LANTM are better than NTM because they are differentiable end-to-end and provide a robust relative indexing scheme but NTM are also differentiable end-to-end and also provide a robust indexing scheme.
* It is said that the head is discrete in NTM but actually it is in space R^n, i.e. it is already continuous. It doesn't become clear what is meant here.
* No tests on real-world tasks, only some toy tasks.
* No comparisons to some of the other NTM extensions such as D-NTM or Sparse Access Memory (SAM) ("
2,"I struggle to understand figure 2, despite the length of the caption. Perhaps labelling the images themselves a bit more clearly."
2,"The Neural Turing Machine and related external memory models have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures. In particular, the NTM, DNC and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position.

The NTM, which is the most relevant to this work, uses a differentiable version of a Turing machine tape. The controller outputs a kernel which softly shifts the head, allowing the machine to read and write sequences. Since this soft shift typically smears the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution.

The premise of this work is to notice that while the NTM emulates a differentiable version of a Turing tape, there is no particular reason that one is constrained to follow the topology of a Turing tape. Instead they propose memory stored at a set of points on a manifold and shift actions which form a Lie group. In this way, memory points can have have different relationships to one another, rather than being constrained to Z.

This is mathematically elegant and here they empirically test models with the shift group R^2 acting on R^2 and the rotation group acting on a sphere.

Overall, the paper is well communicated and a novel idea.

The primary limitation of this paper is its limited impact. While this approach is certainly mathematically elegant, even likely beneficial for some specific problems where the problem structure matches the group structure, it is not clear that this significantly contributes to building models capable of more general program learning. Instead, it is likely to make an already complex and slow model such as the NTM even slower. In general, it would seem memory topology is problem specific and should therefore be learned rather than specified.

The baseline used for comparison is a very simple model, which does not even having the sharpening (the NTM approach to solving the problem of head distributions becoming smeared). There is also no comparison with the successor to the NTM, the DNC, which provides a more general approach to linking memories based on prior memory accesses.

Minor issues:
Footnote on page 3 is misleading regarding the DNC. While the linkage matrix explicitly excludes the identity, the controller can keep the head in the same position by gating the following of the link matrix.
Figures on page 8 are difficult to follow.
"
3,"The paper proposes a new memory access scheme based on Lie group actions for NTMs.

Pros:
* Well written
* Novel addressing scheme as an extension to NTM.
* Seems to work slightly better than normal NTMs.
* Some interesting theory about the novel addressing scheme based on Lie groups.

Cons:
* In the results, the LANTM only seems to be slightly better than the normal NTM.
* The result tables are a bit confusing.
* No source code available.
* The difference to the properties of normal NTM doesn't become too clear. Esp it is said that LANTM are better than NTM because they are differentiable end-to-end and provide a robust relative indexing scheme but NTM are also differentiable end-to-end and also provide a robust indexing scheme.
* It is said that the head is discrete in NTM but actually it is in space R^n, i.e. it is already continuous. It doesn't become clear what is meant here.
* No tests on real-world tasks, only some toy tasks.
* No comparisons to some of the other NTM extensions such as D-NTM or Sparse Access Memory (SAM) ("
2,"I struggle to understand figure 2, despite the length of the caption. Perhaps labelling the images themselves a bit more clearly."
4,"This paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of the temporal transitions in
sequence data. Briefly (and slightly inaccurately) model starts with the LSTM structure but removes all but the diagonal elements to the transition
matrices. It also generalizes the connections from lower layers to upper layers to general convolutions in time (the standard LSTM can be though of as a convolution with a receptive field of 1 time-step). 

As discussed by the authors, the model is related to a number of other recent modifications of RNNs, in particular ByteNet and strongly-typed RNNs (T-RNN). In light of these existing models, the novelty of the QRNN is somewhat diminished, however in my opinion their is still sufficient novelty to justify publication.

The authors present a reasonably solid set of empirical results that support the claims of the paper. It does indeed seem that this particular modification of the LSTM warrants attention from others. 

While I feel that the contribution is somewhat incremental, I recommend acceptance. 
"
3,"
This paper points out that you can take an LSTM and make the gates only a function of the last few inputs  - h_t = f(x_t, x_{t-1}, ...x_{t-T}) - instead of the standard - h_t = f(x_t, h_{t-1}) -, and that if you do so the networks can run faster and work better. You're moving compute from a serial stream to a parallel stream and also making the serial stream more parallel. Unfortunately, this simple, effective and interesting concept is somewhat obscured by confusing language.

- I would encourage the authors to improve the explanation of the model. 
- Another improvement might be to explicitly go over some of the big Oh calculations, or give an example of exactly where the speed improvements are coming from. 
- Otherwise the experiments seem adequate and I enjoyed this paper.

This could be a high value contribution and become a standard neural network component if it can be replicated and if it turns out to work reliably in multiple settings.
"
4,"This paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of the temporal transitions in
sequence data. Briefly (and slightly inaccurately) model starts with the LSTM structure but removes all but the diagonal elements to the transition
matrices. It also generalizes the connections from lower layers to upper layers to general convolutions in time (the standard LSTM can be though of as a convolution with a receptive field of 1 time-step). 

As discussed by the authors, the model is related to a number of other recent modifications of RNNs, in particular ByteNet and strongly-typed RNNs (T-RNN). In light of these existing models, the novelty of the QRNN is somewhat diminished, however in my opinion their is still sufficient novelty to justify publication.

The authors present a reasonably solid set of empirical results that support the claims of the paper. It does indeed seem that this particular modification of the LSTM warrants attention from others. 

While I feel that the contribution is somewhat incremental, I recommend acceptance. 
"
3,"
This paper points out that you can take an LSTM and make the gates only a function of the last few inputs  - h_t = f(x_t, x_{t-1}, ...x_{t-T}) - instead of the standard - h_t = f(x_t, h_{t-1}) -, and that if you do so the networks can run faster and work better. You're moving compute from a serial stream to a parallel stream and also making the serial stream more parallel. Unfortunately, this simple, effective and interesting concept is somewhat obscured by confusing language.

- I would encourage the authors to improve the explanation of the model. 
- Another improvement might be to explicitly go over some of the big Oh calculations, or give an example of exactly where the speed improvements are coming from. 
- Otherwise the experiments seem adequate and I enjoyed this paper.

This could be a high value contribution and become a standard neural network component if it can be replicated and if it turns out to work reliably in multiple settings.
"
3,"[UPDATE]
After going through the response from the author and the revision, I increased my review score for two reasons.
1. I thank the reviewers for further investigating the difference between yours and the other work (Scheduled sampling, Unsupervised learning using LSTM) and providing some insights about it.
This paper at least shows empirically that 100%-Pred scheme is better for high-dimensional video and for long-term predictions.
It would be good if the authors briefly discuss this in the final revision (either in the appendix or in the main text).

2. The revised paper contains more comprehensive results than before.
The presented result and discussion in this paper will be quite useful to the research community as high-dimensional video prediction involves large-scale experiments that are computationally expensive.

- Summary
This paper presents a new RNN architecture for action-conditional future prediction. The proposed architecture combines actions into the recurrent connection of the LSTM core, which performs better than the previous state-of-the-art architecture [Oh et al.]. The paper also explores and compares different architectures such as frame-dependent/independent mode and observation/prediction-dependent architectures. The experimental result shows that the proposed architecture with fully prediction-dependent training scheme achieves the state-of-the-art performance on several complex visual domains. It is also shown that the proposed prediction architecture can be used to improve exploration in a 3D environment.

- Novelty
The novelty of the proposed architecture is not strong. The difference between [Oh et al.] and this work is that actions are combined into the LSTM in this paper, while actions are combined after LSTM in [Oh et al.]. The jumpy prediction was already introduced by [Srivastava et al.] in the deep learning area. 

- Experiment
The experiments are well-designed and thorough. Specifically, the paper evaluates different training schemes and compares different architectures using several rich domains (Atari, 3D worlds). Besides, the proposed method achieves the state-of-the-art results on many domains and presents an application for model-based exploration.

- Clarity
The paper is well-written and easy to follow.

- Overall 
Although the proposed architecture is not much novel, it achieves promising results on Atari games and 3D environments. In addition, the systematic evaluation of different architectures presented in the paper would be useful to the community.

[Reference]
Nitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov. Unsupervised Learning with LSTMs. ICML 2016."
3,"[UPDATE]
After going through the response from the author and the revision, I increased my review score for two reasons.
1. I thank the reviewers for further investigating the difference between yours and the other work (Scheduled sampling, Unsupervised learning using LSTM) and providing some insights about it.
This paper at least shows empirically that 100%-Pred scheme is better for high-dimensional video and for long-term predictions.
It would be good if the authors briefly discuss this in the final revision (either in the appendix or in the main text).

2. The revised paper contains more comprehensive results than before.
The presented result and discussion in this paper will be quite useful to the research community as high-dimensional video prediction involves large-scale experiments that are computationally expensive.

- Summary
This paper presents a new RNN architecture for action-conditional future prediction. The proposed architecture combines actions into the recurrent connection of the LSTM core, which performs better than the previous state-of-the-art architecture [Oh et al.]. The paper also explores and compares different architectures such as frame-dependent/independent mode and observation/prediction-dependent architectures. The experimental result shows that the proposed architecture with fully prediction-dependent training scheme achieves the state-of-the-art performance on several complex visual domains. It is also shown that the proposed prediction architecture can be used to improve exploration in a 3D environment.

- Novelty
The novelty of the proposed architecture is not strong. The difference between [Oh et al.] and this work is that actions are combined into the LSTM in this paper, while actions are combined after LSTM in [Oh et al.]. The jumpy prediction was already introduced by [Srivastava et al.] in the deep learning area. 

- Experiment
The experiments are well-designed and thorough. Specifically, the paper evaluates different training schemes and compares different architectures using several rich domains (Atari, 3D worlds). Besides, the proposed method achieves the state-of-the-art results on many domains and presents an application for model-based exploration.

- Clarity
The paper is well-written and easy to follow.

- Overall 
Although the proposed architecture is not much novel, it achieves promising results on Atari games and 3D environments. In addition, the systematic evaluation of different architectures presented in the paper would be useful to the community.

[Reference]
Nitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov. Unsupervised Learning with LSTMs. ICML 2016."
3,"This paper explores ensemble optimisation in the context of policy-gradient training. Ensemble training has been a low-hanging fruit for many years in the this space and this paper finally touches on this interesting subject. The paper is well written and accessible. In particular the questions posed in section 4 are well posed and interesting.

That said the paper does have some very weak points, most obviously that all of its results are for a very particular choice of domain+parameters. I eagerly look forward to the journal version where these experiments are repeated for all sorts of source domain/target domain/parameter combinations.

"
3,"The paper looks at the problem of transferring a policy learned in a simulator to  a target real-world system.  The proposed approach considers using an ensemble of simulated source domains, along with adversarial training, to learn a robust policy that is able to generalize to several target domains.

Overall, the paper tackles an interesting problem, and provides a reasonable solution.  The notion of adversarial training used here does not seem the same as other recent literature (e.g. on GANs).  It would be useful to add more details on a few components, as discussed in the question/response round.  I also encourage including the results with alternative policy gradient subroutines, even if they dont perform well (e.g. Reinforce), as well as results with and without the baseline on the value function. Such results are very useful to other researchers.
"
3,"This paper explores ensemble optimisation in the context of policy-gradient training. Ensemble training has been a low-hanging fruit for many years in the this space and this paper finally touches on this interesting subject. The paper is well written and accessible. In particular the questions posed in section 4 are well posed and interesting.

That said the paper does have some very weak points, most obviously that all of its results are for a very particular choice of domain+parameters. I eagerly look forward to the journal version where these experiments are repeated for all sorts of source domain/target domain/parameter combinations.

"
3,"The paper looks at the problem of transferring a policy learned in a simulator to  a target real-world system.  The proposed approach considers using an ensemble of simulated source domains, along with adversarial training, to learn a robust policy that is able to generalize to several target domains.

Overall, the paper tackles an interesting problem, and provides a reasonable solution.  The notion of adversarial training used here does not seem the same as other recent literature (e.g. on GANs).  It would be useful to add more details on a few components, as discussed in the question/response round.  I also encourage including the results with alternative policy gradient subroutines, even if they dont perform well (e.g. Reinforce), as well as results with and without the baseline on the value function. Such results are very useful to other researchers.
"
5,"Pros:
  Interesting training criterion.
Cons:
  Missing proper ASR technique based baselines.

Comments:
  The dataset is quite small.
  ROC curves for detection, and more measurements, e.g. EER would probably be helpful besides AP.
  More detailed analysis of the results would be necessary, e.g. precision of words seen during training compared to the detection
  performance of out-of-vocabulary words.
  It would be interesting to show scatter plots for embedding vs. orthographic distances.
"
5,"Pros:
  Interesting training criterion.
Cons:
  Missing proper ASR technique based baselines.

Comments:
  The dataset is quite small.
  ROC curves for detection, and more measurements, e.g. EER would probably be helpful besides AP.
  More detailed analysis of the results would be necessary, e.g. precision of words seen during training compared to the detection
  performance of out-of-vocabulary words.
  It would be interesting to show scatter plots for embedding vs. orthographic distances.
"
3,"The authors propose the use of statistics of softmax outputs to estimate the probability of error and probability of a test sample being out-of-domain. They contrast the performance of the proposed method with directly using the softmax output probabilities, and not their statistics, as a measure of confidence.

It would be great if the authors elaborate on the idea of ignoring the logit of the blank symbol.

It would be interesting to see the performance of the proposed methods in more confusable settings, ie., in cases where the out-of-domain examples are more similar to the in-domain examples. e.g., in the case of speech recognition this might correspond to using a different language's speech with an ASR system trained in a particular language. Here the acoustic characteristics of the speech signals from two different languages might be more similar as compared to the noisy and clean speech signals from the same language.

In section 4, the description of the auxiliary decoder setup might benefit from more detail.

There has been recent work on performance monitoring and accuracy prediction in the area of speech recognition, some of this work is listed below. 
1. Ogawa, Tetsuji, et al. ""Delta-M measure for accuracy prediction and its application to multi-stream based unsupervised adaptation."" Proceedings of ICASSP. 2015.

2. Hermansky, Hynek, et al. ""Towards machines that know when they do not know."" Proceedings of ICASSP, 2015.

3. Variani, Ehsan et al. ""Multi-stream recognition of noisy speech with performance monitoring."" INTERSPEECH. 2013."
3,"The authors propose the use of statistics of softmax outputs to estimate the probability of error and probability of a test sample being out-of-domain. They contrast the performance of the proposed method with directly using the softmax output probabilities, and not their statistics, as a measure of confidence.

It would be great if the authors elaborate on the idea of ignoring the logit of the blank symbol.

It would be interesting to see the performance of the proposed methods in more confusable settings, ie., in cases where the out-of-domain examples are more similar to the in-domain examples. e.g., in the case of speech recognition this might correspond to using a different language's speech with an ASR system trained in a particular language. Here the acoustic characteristics of the speech signals from two different languages might be more similar as compared to the noisy and clean speech signals from the same language.

In section 4, the description of the auxiliary decoder setup might benefit from more detail.

There has been recent work on performance monitoring and accuracy prediction in the area of speech recognition, some of this work is listed below. 
1. Ogawa, Tetsuji, et al. ""Delta-M measure for accuracy prediction and its application to multi-stream based unsupervised adaptation."" Proceedings of ICASSP. 2015.

2. Hermansky, Hynek, et al. ""Towards machines that know when they do not know."" Proceedings of ICASSP, 2015.

3. Variani, Ehsan et al. ""Multi-stream recognition of noisy speech with performance monitoring."" INTERSPEECH. 2013."
2,"This is an interesting and pleasant paper on superoptimization, that extends the  problem approached by the stochastic search STOKE to a learned stochastic search, where the STOKE proposals are the output of a neural network which takes some program embedding as an input. The authors then use REINFORCE to learn an MCMC scheme with the objective of minimizing the final program cost.

The writing is clear and results highlight the efficacy of the method.

comments / questions:
- Am I correct in understanding that of the entire stochastic computation graph, only the features->proposal part is learned. The rest is still effectively the stoke MCMC scheme? Does that imply that the 'uniform' model is effectively Stoke and is your baseline (this should probably be made explicit )

- Did the authors consider learning the features instead of using out of the box features (could be difficult given the relatively small amount of data - the feature extractor might not generalize).

- In a different context, 'Markov Chain Monte Carlo and Variational Inference:Bridging the Gap' by Salimans et al. suggests considering a MCMC scheme as a stochastic computation graph and optimizing using a variational (i.e. RL) criterion. The problem is different, it uses HMC instead of MCMC, but it might be worth citing as a similar approach to 'meta-optimized' MCMC algorithms.

"
2,"This is an interesting and pleasant paper on superoptimization, that extends the  problem approached by the stochastic search STOKE to a learned stochastic search, where the STOKE proposals are the output of a neural network which takes some program embedding as an input. The authors then use REINFORCE to learn an MCMC scheme with the objective of minimizing the final program cost.

The writing is clear and results highlight the efficacy of the method.

comments / questions:
- Am I correct in understanding that of the entire stochastic computation graph, only the features->proposal part is learned. The rest is still effectively the stoke MCMC scheme? Does that imply that the 'uniform' model is effectively Stoke and is your baseline (this should probably be made explicit )

- Did the authors consider learning the features instead of using out of the box features (could be difficult given the relatively small amount of data - the feature extractor might not generalize).

- In a different context, 'Markov Chain Monte Carlo and Variational Inference:Bridging the Gap' by Salimans et al. suggests considering a MCMC scheme as a stochastic computation graph and optimizing using a variational (i.e. RL) criterion. The problem is different, it uses HMC instead of MCMC, but it might be worth citing as a similar approach to 'meta-optimized' MCMC algorithms.

"
5,"The author proposed a simple but yet effective technique in order to regularized neural networks. The results obtained are quite good and the technique shows to be effective when it it applied even on state of the art topologies, that is welcome because some regularization techniques used to be applied in easy task or on a initial configuration which results are still far from the best known results. "
5,"This is a good paper with the appropriate experimentation. Regularization must be tested on deep and complex topologies, near to state of the art. Other papers test reg. with simple models where regularization helps but are not in the edge...

"
5,"The author proposed a simple but yet effective technique in order to regularized neural networks. The results obtained are quite good and the technique shows to be effective when it it applied even on state of the art topologies, that is welcome because some regularization techniques used to be applied in easy task or on a initial configuration which results are still far from the best known results. "
5,"This is a good paper with the appropriate experimentation. Regularization must be tested on deep and complex topologies, near to state of the art. Other papers test reg. with simple models where regularization helps but are not in the edge...

"
5,"This work introduces a novel method for training GANs by displacing simultaneous SGD, and unrolling the inner optimization in the minmax game as a computational graph. The paper is very clearly written, and explains the justification very well. The problem being attacked is very significant and important. The approach is novel, however, similar ideas have been tried to solve problems unrelated to GANs.

The first quantitative experiment is section 3.3.1, where the authors attempt to find the best z which can generate training examples. This is done by using L-BFGS on |G(z) - x|. The claim is that if we're able to find such a z, then the generator can generate this particular training example. It's demonstrated that 0-step GANs are not able to generate many training examples, while unrolled GANs do. However, I find this experiment unreasonable. Being able to find a certain z, which generates a certain sample does not guarantee that this particular mode is high probability. In fact, an identity function can potentially beat all the GAN models in the proposed metric. And due to Cantor's proof of equivalence between all powers of real spaces, this applies to smaller dimension of z as well. More realistically, it should be possible to generate *any* image from a generator by finding a very specific z. That a certain z exists which can generate a sample does not prove that the generator is not missing modes. It just proves that the generator is similar enough to an identity function to be able to generate any possible image. This metric is thus measuring something potentially tangential to diversity or mode-dropping. Another problem with this metric is that that showing that the optimization is not able to find a z for a specific training examples does not prove that such a z does not exist, only that it's harder to find. So, this comparison might just be showing that unrolled GANs have a smoother function than 0-step GANs, and thus easier to optimize for z.

The second quantitative experiment considers mean pairwise distance between generated samples, and between data samples. The first number is likely to be small in the case of a mode-dropping GAN. The authors argue that the two numbers being closer to each other is an indication of the generated samples being as diverse as the data. Once again, this metric is not convincing. 1. The distances are being measured in pixel-space. 2. A GAN model could be generating garbage, and yet still perform very well in this metric.

There are no other quantitative results in the paper. Even though the method is optimizing diversity, for a sanity check, scores for quality such as Inception scores or SSL performance would have been useful. Another metric that the authors can consider is training GAN using this approach on the tri-MNIST dataset (concatenation of 3 MNIST digits), which results in 1000 easily-identifiable modes. Then, demonstrate that the GAN is able to generate all the 1000 modes with equal probability. This is not a perfect metric either, but arguably much better than the metrics in this paper. This metric is used in this ICLR submission: "
5,"This work introduces a novel method for training GANs by displacing simultaneous SGD, and unrolling the inner optimization in the minmax game as a computational graph. The paper is very clearly written, and explains the justification very well. The problem being attacked is very significant and important. The approach is novel, however, similar ideas have been tried to solve problems unrelated to GANs.

The first quantitative experiment is section 3.3.1, where the authors attempt to find the best z which can generate training examples. This is done by using L-BFGS on |G(z) - x|. The claim is that if we're able to find such a z, then the generator can generate this particular training example. It's demonstrated that 0-step GANs are not able to generate many training examples, while unrolled GANs do. However, I find this experiment unreasonable. Being able to find a certain z, which generates a certain sample does not guarantee that this particular mode is high probability. In fact, an identity function can potentially beat all the GAN models in the proposed metric. And due to Cantor's proof of equivalence between all powers of real spaces, this applies to smaller dimension of z as well. More realistically, it should be possible to generate *any* image from a generator by finding a very specific z. That a certain z exists which can generate a sample does not prove that the generator is not missing modes. It just proves that the generator is similar enough to an identity function to be able to generate any possible image. This metric is thus measuring something potentially tangential to diversity or mode-dropping. Another problem with this metric is that that showing that the optimization is not able to find a z for a specific training examples does not prove that such a z does not exist, only that it's harder to find. So, this comparison might just be showing that unrolled GANs have a smoother function than 0-step GANs, and thus easier to optimize for z.

The second quantitative experiment considers mean pairwise distance between generated samples, and between data samples. The first number is likely to be small in the case of a mode-dropping GAN. The authors argue that the two numbers being closer to each other is an indication of the generated samples being as diverse as the data. Once again, this metric is not convincing. 1. The distances are being measured in pixel-space. 2. A GAN model could be generating garbage, and yet still perform very well in this metric.

There are no other quantitative results in the paper. Even though the method is optimizing diversity, for a sanity check, scores for quality such as Inception scores or SSL performance would have been useful. Another metric that the authors can consider is training GAN using this approach on the tri-MNIST dataset (concatenation of 3 MNIST digits), which results in 1000 easily-identifiable modes. Then, demonstrate that the GAN is able to generate all the 1000 modes with equal probability. This is not a perfect metric either, but arguably much better than the metrics in this paper. This metric is used in this ICLR submission: "
5,"This paper focusses on attention for neural language modeling and has two major contributions:

1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as well.
2. Authors report that very short attention span is sufficient for language models (which is not very surprising) and propose an n-gram RNN which exploits this fact.

The paper has novel models for neural language modeling and some interesting messages. Authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and CBT task.

I am convinced with authors responses for my pre-review questions.

Minor comment: Ba et al., Reed & de Freitas, and Gulcehre et al. should be added to the related work section as well.
"
5,"This paper focusses on attention for neural language modeling and has two major contributions:

1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as well.
2. Authors report that very short attention span is sufficient for language models (which is not very surprising) and propose an n-gram RNN which exploits this fact.

The paper has novel models for neural language modeling and some interesting messages. Authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and CBT task.

I am convinced with authors responses for my pre-review questions.

Minor comment: Ba et al., Reed & de Freitas, and Gulcehre et al. should be added to the related work section as well.
"
3,"This paper proposes a novel and interesting way to tackle the difficulties of performing inference atop HSMM. The idea of using an embedded bi-RNN to approximate the posterior is a reasonable and clever idea. 

That being said, I think two aspects may need further improvement:
(1) An explanation as to why a bi-RNN can provide more accurate approximations than other modeling choices (e.g. structured mean field that uses a sequential model to formulate the variational distribution) is needed. I think it would make the paper stronger if the authors can explain in an intuitive way why this modeling choice is better than some other natural choices (in addition to empirical verification).
(2) The real world datasets seem to be quite small (e.g. less than 100 sequences). Experimental results reported on larger datasets may also strengthen the paper."
3,"This paper proposes a novel and interesting way to tackle the difficulties of performing inference atop HSMM. The idea of using an embedded bi-RNN to approximate the posterior is a reasonable and clever idea. 

That being said, I think two aspects may need further improvement:
(1) An explanation as to why a bi-RNN can provide more accurate approximations than other modeling choices (e.g. structured mean field that uses a sequential model to formulate the variational distribution) is needed. I think it would make the paper stronger if the authors can explain in an intuitive way why this modeling choice is better than some other natural choices (in addition to empirical verification).
(2) The real world datasets seem to be quite small (e.g. less than 100 sequences). Experimental results reported on larger datasets may also strengthen the paper."
3,"This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed method is better able to learn meaningful representations of sequence data.

The proposed DVBF is well motivated, and for the most part the presentation is clear. The experiments show interesting results on illustrative toy examples. I think the contribution is interesting and potentially useful, so Id recommend acceptance.

The SVAE method of Johnson et al. (2016) deserves more discussion than the two sentences devoted to it, since the method seems pretty closely related. Like the DVBF, the SVAE imposes a Markovianity assumption, and it is able to handle similar kinds of problems. From what I understand, the most important algorithmic difference is that the SVAE q network predicts potentials, whereas the DVBF q network predicts innovations. What are the tradeoffs between the two?  Section 2.2 says they do the latter in the interest of solving control-related tasks, but Im not clear why this follows. 

Is there a reason SVAEs dont meet all the desiderata mentioned at the end of the Introduction?

Since the SVAE code is publicly available, one could probably compare against it in the experiments. 

Im a bit confused about the role of uncertainty about v. In principle, one could estimate the transition parameters by maximum likelihood (i.e. fitting a point estimate of v), but this isnt whats done. Instead, v is integrated out as part of the marginal likelihood, which I interpret as giving the flexibility to model different dynamics for different sequences. But if this is the case, then shouldnt the q distribution for v depend on the data, rather than being data-independent as in Eqn. (9)?
"
3,"This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed method is better able to learn meaningful representations of sequence data.

The proposed DVBF is well motivated, and for the most part the presentation is clear. The experiments show interesting results on illustrative toy examples. I think the contribution is interesting and potentially useful, so Id recommend acceptance.

The SVAE method of Johnson et al. (2016) deserves more discussion than the two sentences devoted to it, since the method seems pretty closely related. Like the DVBF, the SVAE imposes a Markovianity assumption, and it is able to handle similar kinds of problems. From what I understand, the most important algorithmic difference is that the SVAE q network predicts potentials, whereas the DVBF q network predicts innovations. What are the tradeoffs between the two?  Section 2.2 says they do the latter in the interest of solving control-related tasks, but Im not clear why this follows. 

Is there a reason SVAEs dont meet all the desiderata mentioned at the end of the Introduction?

Since the SVAE code is publicly available, one could probably compare against it in the experiments. 

Im a bit confused about the role of uncertainty about v. In principle, one could estimate the transition parameters by maximum likelihood (i.e. fitting a point estimate of v), but this isnt whats done. Instead, v is integrated out as part of the marginal likelihood, which I interpret as giving the flexibility to model different dynamics for different sequences. But if this is the case, then shouldnt the q distribution for v depend on the data, rather than being data-independent as in Eqn. (9)?
"
2,"The paper focuses on bilingual word representation learning with the following setting:

1. Bilingual representation is learnt in an offline manner i.e., we already have monolingual representations for the source and target language and we are learning a common mapping for these two representations.
2. There is no direct word to word alignments available between the source and target language.

This is a practically useful setting to consider and authors have done a good job of unifying the existing solutions for this problem by providing theoretical justifications. Even though the authors do not propose a new method for offline bilingual representation learning, the paper is significant for the following contributions:

1. Theory for offline bilingual representation learning.
2. Inverted softmax.
3. Using cognate words for languages that share similar scripts.
4. Showing that this method also works at sentence level (to some extent).

Authors have addressed all my pre-review questions and I am ok with their response. I have few more comments:

1. Header for table 3 which says word frequency is misleading. word frequency could mean that rare words occur in row-1 while I guess authors meant to say that rare words occur in row-5.
2. I see that authors have removed precision @5 and @10 from table-6. Is it because of the space constraints or the results have different trend? I would like to see these results in the appendix.
3. In table-6 what is the difference between row-3 and row-4? Is the only difference NN vs. inverted softmax? Or there are other differences? Please elaborate.
4. Another suggestion is to try running an additional experiment where one can use both expert dictionary and cognate dictionary. Comparing all 3 methods in this setting should give more valuable insights about the usefulness of cognate dictionary.
"
2,"The paper focuses on bilingual word representation learning with the following setting:

1. Bilingual representation is learnt in an offline manner i.e., we already have monolingual representations for the source and target language and we are learning a common mapping for these two representations.
2. There is no direct word to word alignments available between the source and target language.

This is a practically useful setting to consider and authors have done a good job of unifying the existing solutions for this problem by providing theoretical justifications. Even though the authors do not propose a new method for offline bilingual representation learning, the paper is significant for the following contributions:

1. Theory for offline bilingual representation learning.
2. Inverted softmax.
3. Using cognate words for languages that share similar scripts.
4. Showing that this method also works at sentence level (to some extent).

Authors have addressed all my pre-review questions and I am ok with their response. I have few more comments:

1. Header for table 3 which says word frequency is misleading. word frequency could mean that rare words occur in row-1 while I guess authors meant to say that rare words occur in row-5.
2. I see that authors have removed precision @5 and @10 from table-6. Is it because of the space constraints or the results have different trend? I would like to see these results in the appendix.
3. In table-6 what is the difference between row-3 and row-4? Is the only difference NN vs. inverted softmax? Or there are other differences? Please elaborate.
4. Another suggestion is to try running an additional experiment where one can use both expert dictionary and cognate dictionary. Comparing all 3 methods in this setting should give more valuable insights about the usefulness of cognate dictionary.
"
